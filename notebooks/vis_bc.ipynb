{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "\n",
    "from models.utils import get_model\n",
    "from models.config import TOKENS_RAW_CUTOFF\n",
    "from models.seq2seqattn import init_weights, EncRnn, DecRnn, Seq2SeqAttn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = get_model()\n",
    "# w2ind from w2v\n",
    "w2ind = {token: token_index for token_index, token in enumerate(w2v_model.wv.index2word)} \n",
    "# sorted vocab words\n",
    "assert w2v_model.vocabulary.sorted_vocab == True\n",
    "word_counts = {word: vocab_obj.count for word, vocab_obj in w2v_model.wv.vocab.items()}\n",
    "word_counts = sorted(word_counts.items(), key=lambda x:-x[1])\n",
    "words = [t[0] for t in word_counts]\n",
    "# sentence marker token inds\n",
    "sos_ind = w2ind['<sos>']\n",
    "eos_ind = w2ind['<eos>']\n",
    "# adjusted sequence length\n",
    "SEQ_LEN = 5 + 2 # sos, eos tokens\n",
    "# padding token for now\n",
    "TRG_PAD_IDX = w2ind[\".\"] # this is 0\n",
    "# vocab, embed dims\n",
    "VOCAB_SIZE, EMBED_DIM = w2v_model.wv.vectors.shape\n",
    "VOCAB_SIZE, EMBED_DIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "enc = EncRnn(hidden_size=64, num_layers=2, embed_size=EMBED_DIM)\n",
    "dec = DecRnn(hidden_size=64, num_layers=2, embed_size=EMBED_DIM, output_size=VOCAB_SIZE)\n",
    "model = Seq2SeqAttn(enc, dec, TRG_PAD_IDX, VOCAB_SIZE, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\n",
    "    '/scratch/rz1567/deep_rl/final_project_new/limitation-learning/src/pretrained_generators/model-epoch10.pt')\n",
    "                     )\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(words, input_state, next_state, model, eos_ind, max_len, device):\n",
    "    \n",
    "    model.eval()\n",
    "    src_tensor = input_state.unsqueeze(0).to(device)\n",
    "    src_len = torch.Tensor([int(max_len)])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        encoder_outputs, hidden = model.encoder(src_tensor, src_len)\n",
    "\n",
    "    mask = model.create_mask(src_tensor.transpose(1,0)).to(device)\n",
    "    # get first decoder input (<sos>)'s one hot\n",
    "    trg_indexes = [next_state[0]]\n",
    "    # create a array to store attetnion\n",
    "    attentions = torch.zeros(max_len, 1, len(input_state))\n",
    "    #print(attentions.shape)\n",
    "\n",
    "\n",
    "    for i in range(max_len):\n",
    "        trg_tensor = torch.LongTensor([trg_indexes[-1]]).to(device)\n",
    "        #print(trg_tensor.shape)\n",
    "        with torch.no_grad():\n",
    "            output, hidden, attention = model.decoder(trg_tensor, hidden, encoder_outputs, mask)\n",
    "        #print(F.softmax(output))\n",
    "        attentions[i] = attention\n",
    "        pred_token = output.argmax(1).item()\n",
    "        if pred_token == eos_ind: # end of sentence.\n",
    "            break\n",
    "        trg_indexes.append(pred_token)\n",
    "        \n",
    "    trg_tokens = [words[int(ind)] for ind in trg_indexes]\n",
    "    #  remove <sos>\n",
    "    return trg_tokens[1:], attentions[:len(trg_tokens)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = torch.load('../dat/processed/padded_vectorized_states_v3.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GAIL import get_cosine_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for idx, (index, vects) in enumerate(d.items()):\n",
    "        input_state, next_state = vects[0], vects[1]\n",
    "        \n",
    "        input_state = torch.cat((torch.LongTensor([sos_ind]), \n",
    "                                 input_state,\n",
    "                                 torch.LongTensor([eos_ind])), \n",
    "                                 dim=0).to(device)\n",
    "        \n",
    "        next_state = torch.cat((torch.LongTensor([sos_ind]), \n",
    "                                next_state, \n",
    "                                torch.LongTensor([eos_ind])), \n",
    "                               dim=0).to(device)\n",
    "        \n",
    "        trg = next_state.unsqueeze(0).to(device)\n",
    "        \n",
    "        seq_len_tensor = torch.Tensor([int(SEQ_LEN)])\n",
    "        \n",
    "        output = model(input_state.unsqueeze(0), seq_len_tensor, trg)\n",
    "\n",
    "        trg = trg.transpose(1,0)\n",
    "        output_dim = output.shape[-1]                \n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        trg = trg[1:].view(-1)\n",
    "\n",
    "        translation, attention = translate_sentence(words, input_state, next_state, model, eos_ind, SEQ_LEN, device)\n",
    "\n",
    "        # drop <sos>, <eos>\n",
    "        init_act = [words[int(ind)] for ind in input_state.cpu().detach().numpy()][1:-1]\n",
    "        expert_act = [words[int(ind)] for ind in next_state.cpu().detach().numpy()][1:-1]\n",
    "        # drop multiple instances of padded token\n",
    "        expert_act_unpadded = []\n",
    "        for tok in expert_act:\n",
    "            expert_act_unpadded.append(tok)\n",
    "            #if tok == words[int(TRG_PAD_IDX)]:\n",
    "            #    break\n",
    "        init_act_unpadded = []\n",
    "        for tok in init_act:\n",
    "            init_act_unpadded.append(tok)\n",
    "            #if tok == words[int(TRG_PAD_IDX)]:\n",
    "            #    break        \n",
    "        vectorized_expert_act = [w2v_model.wv[tok] for tok in expert_act_unpadded]\n",
    "        vectorized_pred_act = [w2v_model.wv[tok] for tok in translation]\n",
    "        cos_sim = get_cosine_sim(vectorized_expert_act, vectorized_pred_act, \n",
    "                                 type = None, \n",
    "                                 seq_len = SEQ_LEN-2,\n",
    "                                 dim = EMBED_DIM)\n",
    "        \n",
    "        print(f'state = {\" \".join(init_act_unpadded)}')\n",
    "        print(f'expert = {\" \".join(expert_act_unpadded)}')\n",
    "        print(f'model = {\" \".join(translation)}')\n",
    "        print(cos_sim)\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        if idx>10:\n",
    "            break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
