{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rough runthrough of GAIL for dialog generation, getting close..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque  \n",
    "#code for training \n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "from models import *\n",
    "from dialog_environment import *\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import torch\n",
    "from torch.distributions import Normal\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = DialogEnvironment()\n",
    "\n",
    "# Normally args but not here :-)\n",
    "seed = 0\n",
    "render = False\n",
    "gamma = 0.99\n",
    "lamda = .98\n",
    "\n",
    "train_discrim_flag = True\n",
    "learning_rate = 3e-4\n",
    "clip_param = .2\n",
    "discrim_update_num = 2\n",
    "actor_critic_update_num = 10\n",
    "l2_rate = 1e-3 # weight decay\n",
    "total_sample_size = 2048 # total num of state-actions to collect before learning\n",
    "batch_size = 256\n",
    "suspend_accu_exp = 1. # do not need to be this high typically, but seems likely it has to be for a simple env like mountain car cont.\n",
    "suspend_accu_gen = 1.\n",
    "max_iter_num = 1000\n",
    "\n",
    "\n",
    "actor = Actor(hidden_size=3,num_layers=3)\n",
    "critic = Critic(hidden_size=1,num_layers=3)\n",
    "discrim = Discriminator(input_size = 300, hidden_size=1,device='cuda',num_layers=3)\n",
    "actor.to(device), critic.to(device), discrim.to(device)\n",
    "actor_optim = optim.Adam(actor.parameters(), lr=learning_rate)\n",
    "critic_optim = optim.Adam(critic.parameters(), lr=learning_rate, \n",
    "                          weight_decay=l2_rate) \n",
    "discrim_optim = optim.Adam(discrim.parameters(), lr=learning_rate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def subsample(data, target, n=15):\n",
    "    return [x[::n] for x in data], [y[::n] for y in target]\n",
    "\n",
    "\n",
    "def get_action(mu, std):\n",
    "    action = torch.normal(mu, std)\n",
    "    action = action.data.numpy()\n",
    "    \n",
    "    # This should be normalized. \n",
    "    return action\n",
    "\n",
    "\n",
    "def get_entropy(mu, std):\n",
    "    dist = Normal(mu, std)\n",
    "    entropy = dist.entropy().mean()\n",
    "    return entropy\n",
    "\n",
    "def log_prob_density(x, mu, std):\n",
    "    log_prob_density = -(x - mu).pow(2) / (2 * std.pow(2)) \\\n",
    "                     - 0.5 * math.log(2 * math.pi)\n",
    "    return log_prob_density.sum(1, keepdim=True)\n",
    "\n",
    "def get_reward(discrim, state, action):\n",
    "    \"\"\"\n",
    "    The reward function according to irl. It's log D(s,a). \n",
    "    \n",
    "    Reward is higher the closer this is to 0, because the more similar it is to an expert action. :\n",
    "    Is quite close to imitation learning, but hope here is that with such a large number of expert demonstrations and entropy bonuses etc. it learns more than direct imitation. \n",
    "    \"\"\"\n",
    "\n",
    "    action = torch.Tensor(action).to(device)# turn state into a tensor if not already\n",
    "\n",
    "    with torch.no_grad():\n",
    "        return -math.log(discrim(state.resize(1,60,300),action.resize(1,60,300))[0].item())\n",
    "\n",
    "def save_checkpoint(state, filename):\n",
    "    return\n",
    "\n",
    "\n",
    "def train_discrim(discrim, memory, discrim_optim, discrim_update_num, clip_param):\n",
    "    \"\"\"\n",
    "    Training the discriminator. \n",
    "\n",
    "    Use binary cross entropy to classify whether \n",
    "    or not a sequence was predicted by the expert (real data) or actor. \n",
    "    \"\"\"\n",
    "    states = torch.stack([memory[i][0] for i in range(len(memory))])\n",
    "    actions = torch.stack([memory[i][1] for i in range(len(memory))])\n",
    "    rewards = [memory[i][2] for i in range(len(memory))]\n",
    "\n",
    "    masks = [memory[i][2] for i in range(len(memory))]\n",
    "    expert_actions = torch.stack([memory[i][4] for i in range(len(memory))])\n",
    "\n",
    "    criterion = torch.nn.BCELoss() # classify\n",
    "\n",
    "    for _ in range(discrim_update_num):\n",
    "\n",
    "        learner = discrim(states, actions) #pass (s,a) through discriminator\n",
    "\n",
    "        # TODO\n",
    "       # demonstrations = torch.Tensor([states, expert_actions]) # pass (s,a) of expert through discriminator\n",
    "        expert = discrim(states,expert_actions) #discrimator \"guesses\" whether or not these \n",
    "        # actions came from expert or learner\n",
    "        discrim_loss = criterion(learner, torch.ones((states.shape[0], 1)).to(device)) + \\\n",
    "                        criterion(expert, torch.zeros((states.shape[0], 1)).to(device))\n",
    "                # discrim loss: predict agent is all wrong, get as close to 0, and predict expert is 1, getting as close to 1 as possible. \n",
    "        discrim_optim.zero_grad() # gan loss, it tries to always get it right. \n",
    "        discrim_loss.backward()\n",
    "        discrim_optim.step()\n",
    "            # take these steps, do it however many times specified. \n",
    "        #return discrim(states,expert_actions) , discrim(states,actions)\n",
    "    expert_acc = ((discrim(states,expert_actions) < 0.5).float()).mean() #how often it realized the fake examples were fake\n",
    "    learner_acc = ((discrim(states,actions) > 0.5).float()).mean() #how often if predicted expert correctly. \n",
    "\n",
    "    return expert_acc, learner_acc # accuracy, it's the same kind, but because imbalanced better to look at separately. \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_actor_critic(actor, critic, memory, actor_optim, critic_optim, actor_critic_update_num, batch_size, clip_param):\n",
    "    \"\"\"\n",
    "    Take a PPO step or two to improve the actor critic model,  using GAE to estimate returns. \n",
    "    \n",
    "    In our case each trajectory it most one step, so the value function will have to do. \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    # tuple of a regular old RL problem, but now reward is what the discriminator says. \n",
    "    states = torch.stack([memory[i][0] for i in range(len(memory))])\n",
    "    actions = torch.stack([memory[i][1] for i in range(len(memory))])\n",
    "    rewards = [memory[i][2] for i in range(len(memory))]\n",
    "    masks = [memory[i][2] for i in range(len(memory))]\n",
    "    # compute value of what happened, see if what we can get us better. \n",
    "    old_values = critic(states)\n",
    "\n",
    "    #GAE aka estimate of Value + actual return roughtly \n",
    "    returns, advants = get_gae(rewards, masks, old_values, gamma, lamda)\n",
    "    \n",
    "    # pass states through actor, get corresponding actions\n",
    "    mu, std = actor(states)\n",
    "    # new mus and stds? \n",
    "    old_policy = log_prob_density(actions, mu, std) # sum of log probability\n",
    "    # of old actions\n",
    "\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    n = len(states)\n",
    "    arr = np.arange(n)\n",
    "\n",
    "    for _ in range(actor_critic_update_num):\n",
    "        np.random.shuffle(arr)\n",
    "\n",
    "        for i in range(n // batch_size): \n",
    "            batch_index = arr[batch_size * i : batch_size * (i + 1)]\n",
    "            #batch_index = torch.LongTensor(batch_index)\n",
    "            \n",
    "            inputs = states[batch_index]\n",
    "            actions_samples = actions[batch_index]\n",
    "            returns_samples = returns.unsqueeze(1)[batch_index].to(device)\n",
    "            advants_samples = advants.unsqueeze(1)[batch_index].to(device)\n",
    "            oldvalue_samples = old_values[batch_index].detach()\n",
    "        \n",
    "        \n",
    "            values = critic(inputs) #\n",
    "            clipped_values = oldvalue_samples + \\\n",
    "                             torch.clamp(values - oldvalue_samples,\n",
    "                                         -clip_param, \n",
    "                                         clip_param)\n",
    "            critic_loss1 = criterion(clipped_values, returns_samples)\n",
    "            critic_loss2 = criterion(values, returns_samples)\n",
    "            critic_loss = torch.max(critic_loss1, critic_loss2).mean()\n",
    "\n",
    "            loss, ratio, entropy = surrogate_loss(actor, advants_samples, inputs,\n",
    "                                         old_policy.detach(), actions_samples,\n",
    "                                         batch_index)\n",
    "            clipped_ratio = torch.clamp(ratio,\n",
    "                                        1.0 - clip_param,\n",
    "                                        1.0 + clip_param)\n",
    "            clipped_loss = clipped_ratio * advants_samples\n",
    "            actor_loss = -torch.min(loss, clipped_loss).mean()\n",
    "            #print(actor_loss,critic_loss,entropy)\n",
    "           # return actor_loss, critic_loss, entropy\n",
    "            loss = actor_loss + 0.5 * critic_loss - 0.001 * entropy #entropy bonus to promote exploration.\n",
    "\n",
    "            actor_optim.zero_grad()\n",
    "            loss.backward()\n",
    "            actor_optim.step()\n",
    "\n",
    "           # critic_optim.zero_grad()\n",
    "           # loss.backward() \n",
    "            critic_optim.step()\n",
    "\n",
    "def get_gae(rewards, masks, values, gamma, lamda):\n",
    "    \"\"\"\n",
    "    How much better a particular action is in a particular state. \n",
    "    \n",
    "    Uses reward of current action + value function of that state-action pair, discount factor gamma, and then lamda to compute. \n",
    "    \"\"\"\n",
    "    rewards = torch.Tensor(rewards)\n",
    "    masks = torch.Tensor(masks)\n",
    "    returns = torch.zeros_like(rewards)\n",
    "    advants = torch.zeros_like(rewards)\n",
    "    \n",
    "    running_returns = 0\n",
    "    previous_value = 0\n",
    "    running_advants = 0\n",
    "\n",
    "    for t in reversed(range(0, len(rewards))): #for LL, only ever one step :-)\n",
    "        running_returns = rewards[t] + (gamma * running_returns * masks[t])\n",
    "        returns[t] = running_returns\n",
    "\n",
    "        running_delta = rewards[t] + (gamma * previous_value * masks[t]) - \\\n",
    "                                        values.data[t]\n",
    "        previous_value = values.data[t]\n",
    "        \n",
    "        running_advants = running_delta + (gamma * lamda * \\\n",
    "                                            running_advants * masks[t])\n",
    "        advants[t] = running_advants\n",
    "\n",
    "    advants = (advants - advants.mean()) / advants.std()\n",
    "    return returns, advants\n",
    "\n",
    "def surrogate_loss(actor, advants, states, old_policy, actions, batch_index):\n",
    "    \"\"\"\n",
    "    The loss for PPO. Re-run through network, recomput policy from states\n",
    "    and see if this surrogate ratio is better. If it is, use as proximal policy update. It's very close to prior policy, but def better. \n",
    "    \n",
    "    Not sure this actually works though. Should not the new mu and stds be used to draw,\n",
    "    \n",
    "        When do we use get_action? Only once in main, I think it should be for all? \n",
    "    \"\"\"\n",
    "    mu, std = actor(states)\n",
    "    new_policy = log_prob_density(actions, mu, std)\n",
    "    old_policy = old_policy[batch_index]\n",
    "\n",
    "    ratio = torch.exp(new_policy - old_policy)\n",
    "    surrogate_loss = ratio * advants\n",
    "    entropy = get_entropy(mu, std)\n",
    "\n",
    "    return surrogate_loss, ratio, entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/nsk367/anaconda3/envs/irl/lib/python3.8/site-packages/torch/tensor.py:447: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:: 2048 episode score is 0.81\n",
      "Expert: 100.00% | Learner: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# Now what we came for...\n",
    "episodes = 0\n",
    "for iter in range(max_iter_num):\n",
    "    actor.eval(), critic.eval()\n",
    "    memory = deque()\n",
    "\n",
    "    steps = 0\n",
    "    scores = []\n",
    "\n",
    "    while steps < total_sample_size: \n",
    "        state, expert_action, raw_state, raw_expert_action = env.reset()\n",
    "        state = state.to(device)\n",
    "        expert_action = expert_action.to(device)\n",
    "        score = 0\n",
    "\n",
    "        \n",
    "        for _ in range(1000): \n",
    "            if render:\n",
    "                print(raw_state, raw_expert_action)\n",
    "            steps += 1\n",
    "\n",
    "\n",
    "            mu, std = actor(state.resize(1,60,300))\n",
    "            action = get_action(mu.cpu(), std.cpu())[0]\n",
    "            done= env.step(action)\n",
    "            irl_reward = get_reward(discrim, state, action)\n",
    "            if done:\n",
    "                mask = 0\n",
    "            else:\n",
    "                mask = 1\n",
    "\n",
    "            memory.append([state, torch.from_numpy(action).to(device), irl_reward, mask,expert_action])\n",
    "\n",
    "            score += irl_reward\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        episodes += 1\n",
    "        scores.append(score)\n",
    "\n",
    "    score_avg = np.mean(scores)\n",
    "    print('{}:: {} episode score is {:.2f}'.format(iter, episodes, score_avg))\n",
    "\n",
    "    actor.train(), critic.train(), discrim.train()\n",
    "    if train_discrim_flag:\n",
    "        expert_acc, learner_acc = train_discrim(discrim, memory, discrim_optim, discrim_update_num, clip_param)\n",
    "        print(\"Expert: %.2f%% | Learner: %.2f%%\" % (expert_acc * 100, learner_acc * 100))\n",
    "        if expert_acc > suspend_accu_exp and learner_acc > suspend_accu_gen:\n",
    "            train_discrim_flag = False\n",
    "    train_actor_critic(actor, critic, memory, actor_optim, critic_optim, actor_critic_update_num, batch_size, clip_param)\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"That was n't a fucking restraint .  \",)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 9.8520e-02,  2.5001e-01, -2.7018e-01, -2.3186e-01,  2.2378e-02,\n",
       "         4.5321e-02, -5.2444e-02, -5.8867e-02, -3.1937e-02,  3.0121e+00,\n",
       "        -2.3642e-02, -7.7292e-02,  3.0742e-01,  7.3579e-02, -3.3171e-01,\n",
       "        -1.6588e-01, -2.0228e-02,  1.0850e+00, -4.2189e-01, -2.4159e-01,\n",
       "         2.0511e-01,  2.5093e-02,  1.1497e-01, -2.4636e-02, -1.0416e-01,\n",
       "         1.9698e-01, -2.0124e-01, -3.6615e-02, -1.5073e-01, -2.5125e-01,\n",
       "        -1.6182e-01,  2.2131e-01, -4.1937e-02,  6.4382e-02,  6.6347e-02,\n",
       "        -6.9173e-03, -1.3059e-01, -3.7921e-02, -1.3478e-01, -9.5946e-02,\n",
       "         1.6818e-02,  2.6046e-01,  4.0114e-02, -2.0923e-01,  2.3095e-01,\n",
       "        -5.9719e-02, -3.8339e-01, -1.0134e-01, -1.0069e-01,  7.3018e-02,\n",
       "        -2.7729e-01,  8.2580e-02, -7.3222e-02,  1.9346e-01,  2.3859e-01,\n",
       "        -5.2506e-02,  1.3046e-01, -1.3450e-01, -9.6133e-02, -1.1073e-01,\n",
       "         9.8949e-02, -5.0032e-03, -1.2383e-01,  3.5765e-01, -1.1829e-01,\n",
       "        -2.2458e-01,  2.3951e-02,  2.4224e-01,  4.1263e-02,  1.1803e-01,\n",
       "        -2.7008e-03,  2.0971e-02,  3.1150e-01, -2.1006e-02, -1.8890e-01,\n",
       "        -1.1755e-01, -1.0380e-01, -2.0398e-01, -2.0374e-01,  3.4373e-01,\n",
       "         1.0465e-01,  1.2733e-01, -3.4323e-01, -2.1958e-01,  2.1851e-01,\n",
       "        -4.6476e-01, -1.3259e-01, -4.9580e-01,  2.9493e-01,  5.2023e-02,\n",
       "        -3.0517e-02,  4.0759e-03, -1.2702e-01,  2.9625e-01,  1.4591e-01,\n",
       "         5.1789e-02, -9.3879e-02, -8.5754e-02, -1.8455e-01,  4.4252e-02,\n",
       "         5.8965e-02,  1.8165e-01, -1.8175e-01, -7.8165e-02,  6.9561e-02,\n",
       "        -1.2850e+00, -5.4441e-02,  1.5005e-01, -1.6758e-01, -5.5905e-02,\n",
       "        -3.0110e-02,  1.8919e-01, -1.4799e-01, -1.1013e-01,  7.2332e-02,\n",
       "        -6.4289e-02, -1.6124e-01,  5.6683e-02,  1.7084e-01, -1.3390e-01,\n",
       "         6.7247e-02, -2.3344e-01,  2.3066e-02, -1.0787e-01, -8.6551e-02,\n",
       "         7.6817e-02, -1.3164e-02, -6.6599e-01,  8.0374e-02,  1.1884e-02,\n",
       "        -1.4544e-01,  5.7928e-02,  1.8992e-01,  3.8859e-02,  3.4437e-02,\n",
       "         1.7240e-01,  2.8254e-02, -2.9560e-01,  1.8364e-01, -8.5466e-03,\n",
       "        -1.3864e+00,  2.6142e-01,  2.3629e-01,  2.0470e-01, -6.2789e-02,\n",
       "        -1.6743e-01, -1.3996e-01, -4.3662e-02, -4.0376e-02, -1.8405e-01,\n",
       "        -1.3819e-01, -1.5173e-01,  8.2284e-03, -6.0196e-02,  6.4731e-02,\n",
       "        -8.5640e-02, -3.0603e-01, -2.0295e-01, -2.4928e-01, -3.7877e-02,\n",
       "        -2.1733e-01,  2.2598e-01, -1.2165e-01, -1.2929e-01, -1.7562e-01,\n",
       "        -4.2181e-02,  2.2738e-01,  3.3748e-03,  2.2381e-01, -1.0925e-01,\n",
       "         1.1977e-01, -7.0582e-02,  1.0818e-01, -1.5419e-01, -6.8567e-02,\n",
       "         1.1879e-01,  3.2163e-02, -1.2023e-01,  1.5811e-03, -3.1903e-02,\n",
       "        -1.9634e-01, -1.2366e-02, -1.0347e-01, -1.3484e-01, -2.3577e-01,\n",
       "        -4.4767e-03,  1.2205e-01,  4.3512e-02, -2.0441e-02,  1.4099e-01,\n",
       "         1.0910e-01, -1.9912e-01, -4.1783e-01,  2.0830e-01,  1.2752e-01,\n",
       "         1.0165e-01, -1.8279e-01, -1.2781e-01, -1.1247e-01,  3.2660e-01,\n",
       "        -1.8286e-01, -1.2189e-02, -9.8733e-02, -1.4702e-01,  1.9123e-01,\n",
       "         1.0175e-01,  6.6325e-02,  6.5839e-02, -6.9673e-03, -1.7495e-02,\n",
       "        -1.0122e-01, -1.2645e-01, -1.7692e-01, -7.4391e-02,  2.3176e-01,\n",
       "         2.0650e-01, -3.2261e-01, -1.6756e-02, -2.2781e-01, -2.6889e-01,\n",
       "         2.8889e-02,  1.1281e-01, -8.7673e-02,  1.3235e-01,  2.0797e-01,\n",
       "         2.4392e-01,  8.8125e-02,  1.8456e-01, -4.1869e-02,  1.8712e-01,\n",
       "        -1.2383e-01,  1.4974e-01, -2.2145e-03,  2.3455e-02, -7.0194e-02,\n",
       "         2.3735e-02,  1.6384e-01, -1.1675e-02, -1.4943e-01,  3.1397e-01,\n",
       "         1.3872e-01,  3.5348e-02,  6.2663e-03,  2.0222e-01,  3.2528e-01,\n",
       "        -3.8256e-01,  8.5346e-03, -4.1604e-01, -5.8313e-02,  2.1325e-01,\n",
       "         9.8621e-02, -2.6227e-01, -3.9877e-03,  6.2827e-04, -2.2198e-01,\n",
       "         1.3791e-01,  1.7525e-01,  8.2616e-02,  2.5852e-01, -4.7226e-02,\n",
       "         4.0242e-01,  4.9074e-02, -3.6251e-01,  9.9738e-02,  2.8844e-01,\n",
       "         1.3101e-01, -5.8852e-02,  1.3985e-01,  7.4787e-01,  2.1331e-01,\n",
       "         1.4665e-01, -7.4234e-02, -2.3464e-02, -1.3176e-01, -2.3619e-02,\n",
       "        -7.1066e-02, -1.6462e-01,  2.6197e-01,  3.7356e-01,  1.6785e-01,\n",
       "         1.5434e-01,  1.2311e-01,  6.8364e-02, -4.2458e-02, -8.3352e-02,\n",
       "        -1.4705e-01,  2.2365e-01,  5.1185e-02,  2.1317e-01, -3.2661e-01,\n",
       "        -2.4267e-01,  1.1959e-01,  1.3652e-02, -1.1243e-01, -8.9702e-02,\n",
       "        -1.9614e-01, -2.7097e-01, -6.2639e-02,  2.4424e-01,  1.7779e-01],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expert_action[:10][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(23.2905, device='cuda:0')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(expert_action[0]**2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23.290461562008186"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([expert_action[0][i].cpu().item()**2 for i in range(len(expert_action[0]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_action = expert_action.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity = 0\n",
    "for i in range(60):\n",
    "    similarity += np.dot(action[i],expert_action.numpy()[i])/(np.linalg.norm(action[i])*np.linalg.norm(expert_action[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " np.dot(action[i],expert_action.numpy()[i])/(np.linalg.norm(action[i])*np.linalg.norm(expert_action[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action[0].mean(), action[0].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_action[0].mean(), expert_action[0].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
