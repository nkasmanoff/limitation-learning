{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rough runthrough of GAIL for dialog generation, getting close..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque  \n",
    "#code for training \n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "from models import *\n",
    "from dialog_environment import *\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import torch\n",
    "from torch.distributions import Normal\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = DialogEnvironment()\n",
    "\n",
    "# Normally args but not here :-)\n",
    "seed = 0\n",
    "render = False\n",
    "gamma = 0.99\n",
    "lamda = .98\n",
    "\n",
    "train_discrim_flag = True\n",
    "learning_rate = 3e-4\n",
    "clip_param = .2\n",
    "discrim_update_num = 2\n",
    "actor_critic_update_num = 10\n",
    "l2_rate = 1e-3 # weight decay\n",
    "total_sample_size = 256 # total num of state-actions to collect before learning\n",
    "batch_size = 32\n",
    "suspend_accu_exp = 1. # do not need to be this high typically, but seems likely it has to be for a simple env like mountain car cont.\n",
    "suspend_accu_gen = 1.\n",
    "max_iter_num = 500\n",
    "\n",
    "\n",
    "actor = Actor(hidden_size=3,num_layers=3)\n",
    "critic = Critic(hidden_size=1,num_layers=3)\n",
    "discrim = Discriminator(input_size = 300, hidden_size=1,device='cuda',num_layers=3)\n",
    "actor.to(device), critic.to(device), discrim.to(device)\n",
    "actor_optim = optim.Adam(actor.parameters(), lr=learning_rate)\n",
    "critic_optim = optim.Adam(critic.parameters(), lr=learning_rate, \n",
    "                          weight_decay=l2_rate) \n",
    "discrim_optim = optim.Adam(discrim.parameters(), lr=learning_rate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def subsample(data, target, n=15):\n",
    "    return [x[::n] for x in data], [y[::n] for y in target]\n",
    "\n",
    "\n",
    "def get_action(mu, std):\n",
    "    action = torch.normal(mu, std)\n",
    "    action = action.data.numpy()\n",
    "    return action\n",
    "\n",
    "\n",
    "def get_entropy(mu, std):\n",
    "    dist = Normal(mu, std)\n",
    "    entropy = dist.entropy().mean()\n",
    "    return entropy\n",
    "\n",
    "def log_prob_density(x, mu, std):\n",
    "    log_prob_density = -(x - mu).pow(2) / (2 * std.pow(2)) \\\n",
    "                     - 0.5 * math.log(2 * math.pi)\n",
    "    return log_prob_density.sum(1, keepdim=True)\n",
    "\n",
    "def get_reward(discrim, state, action):\n",
    "    \"\"\"\n",
    "    The reward function according to irl. It's log D(s,a). \n",
    "    \n",
    "    Reward is higher the closer this is to 0, because the more similar it is to an expert action. :\n",
    "    Is quite close to imitation learning, but hope here is that with such a large number of expert demonstrations and entropy bonuses etc. it learns more than direct imitation. \n",
    "    \"\"\"\n",
    "\n",
    "    action = torch.Tensor(action).to(device)# turn state into a tensor if not already\n",
    "\n",
    "    with torch.no_grad():\n",
    "        return -math.log(discrim(state.resize(1,60,300),action.resize(1,60,300))[0].item())\n",
    "\n",
    "def save_checkpoint(state, filename):\n",
    "    return\n",
    "\n",
    "\n",
    "def train_discrim(discrim, memory, discrim_optim, discrim_update_num, clip_param):\n",
    "    \"\"\"\n",
    "    Training the discriminator. \n",
    "\n",
    "    Use binary cross entropy to classify whether \n",
    "    or not a sequence was predicted by the expert (real data) or actor. \n",
    "    \"\"\"\n",
    "    states = torch.stack([memory[i][0] for i in range(len(memory))])\n",
    "    actions = torch.stack([memory[i][1] for i in range(len(memory))])\n",
    "    rewards = [memory[i][2] for i in range(len(memory))]\n",
    "\n",
    "    masks = [memory[i][2] for i in range(len(memory))]\n",
    "    expert_actions = torch.stack([memory[i][4] for i in range(len(memory))])\n",
    "\n",
    "    criterion = torch.nn.BCELoss() # classify\n",
    "\n",
    "    for _ in range(discrim_update_num):\n",
    "\n",
    "        learner = discrim(states, actions) #pass (s,a) through discriminator\n",
    "\n",
    "        # TODO\n",
    "       # demonstrations = torch.Tensor([states, expert_actions]) # pass (s,a) of expert through discriminator\n",
    "        expert = discrim(states,expert_actions) #discrimator \"guesses\" whether or not these \n",
    "        # actions came from expert or learner\n",
    "        discrim_loss = criterion(learner, torch.ones((states.shape[0], 1)).to(device)) + \\\n",
    "                        criterion(expert, torch.zeros((states.shape[0], 1)).to(device))\n",
    "                # discrim loss: predict agent is all wrong, get as close to 0, and predict expert is 1, getting as close to 1 as possible. \n",
    "        discrim_optim.zero_grad() # gan loss, it tries to always get it right. \n",
    "        discrim_loss.backward()\n",
    "        discrim_optim.step()\n",
    "            # take these steps, do it however many times specified. \n",
    "        #return discrim(states,expert_actions) , discrim(states,actions)\n",
    "    expert_acc = ((discrim(states,expert_actions) < 0.5).float()).mean() #how often it realized the fake examples were fake\n",
    "    learner_acc = ((discrim(states,actions) > 0.5).float()).mean() #how often if predicted expert correctly. \n",
    "\n",
    "    return expert_acc, learner_acc # accuracy, it's the same kind, but because imbalanced better to look at separately. \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_actor_critic(actor, critic, memory, actor_optim, critic_optim, actor_critic_update_num, batch_size, clip_param):\n",
    "    \"\"\"\n",
    "    Take a PPO step or two to improve the actor critic model,  using GAE to estimate returns. \n",
    "    \n",
    "    In our case each trajectory it most one step, so the value function will have to do. \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    # tuple of a regular old RL problem, but now reward is what the discriminator says. \n",
    "    states = torch.stack([memory[i][0] for i in range(len(memory))])\n",
    "    actions = torch.stack([memory[i][1] for i in range(len(memory))])\n",
    "    rewards = [memory[i][2] for i in range(len(memory))]\n",
    "    masks = [memory[i][2] for i in range(len(memory))]\n",
    "    # compute value of what happened, see if what we can get us better. \n",
    "    old_values = critic(states)\n",
    "\n",
    "    #GAE aka estimate of Value + actual return roughtly \n",
    "    returns, advants = get_gae(rewards, masks, old_values, gamma, lamda)\n",
    "    \n",
    "    # pass states through actor, get corresponding actions\n",
    "    mu, std = actor(states)\n",
    "    # new mus and stds? \n",
    "    old_policy = log_prob_density(actions, mu, std) # sum of log probability\n",
    "    # of old actions\n",
    "\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    n = len(states)\n",
    "    arr = np.arange(n)\n",
    "\n",
    "    for _ in range(actor_critic_update_num):\n",
    "        np.random.shuffle(arr)\n",
    "\n",
    "        for i in range(n // batch_size): \n",
    "            batch_index = arr[batch_size * i : batch_size * (i + 1)]\n",
    "            #batch_index = torch.LongTensor(batch_index)\n",
    "            \n",
    "            inputs = states[batch_index]\n",
    "            actions_samples = actions[batch_index]\n",
    "            returns_samples = returns.unsqueeze(1)[batch_index].to(device)\n",
    "            advants_samples = advants.unsqueeze(1)[batch_index].to(device)\n",
    "            oldvalue_samples = old_values[batch_index].detach()\n",
    "        \n",
    "        \n",
    "            values = critic(inputs) #\n",
    "            clipped_values = oldvalue_samples + \\\n",
    "                             torch.clamp(values - oldvalue_samples,\n",
    "                                         -clip_param, \n",
    "                                         clip_param)\n",
    "            critic_loss1 = criterion(clipped_values, returns_samples)\n",
    "            critic_loss2 = criterion(values, returns_samples)\n",
    "            critic_loss = torch.max(critic_loss1, critic_loss2).mean()\n",
    "\n",
    "            loss, ratio, entropy = surrogate_loss(actor, advants_samples, inputs,\n",
    "                                         old_policy.detach(), actions_samples,\n",
    "                                         batch_index)\n",
    "            clipped_ratio = torch.clamp(ratio,\n",
    "                                        1.0 - clip_param,\n",
    "                                        1.0 + clip_param)\n",
    "            clipped_loss = clipped_ratio * advants_samples\n",
    "            actor_loss = -torch.min(loss, clipped_loss).mean()\n",
    "            #print(actor_loss,critic_loss,entropy)\n",
    "           # return actor_loss, critic_loss, entropy\n",
    "            loss = actor_loss + 0.5 * critic_loss - 0.001 * entropy #entropy bonus to promote exploration.\n",
    "\n",
    "            actor_optim.zero_grad()\n",
    "            loss.backward()\n",
    "            actor_optim.step()\n",
    "\n",
    "           # critic_optim.zero_grad()\n",
    "           # loss.backward() \n",
    "            critic_optim.step()\n",
    "\n",
    "def get_gae(rewards, masks, values, gamma, lamda):\n",
    "    \"\"\"\n",
    "    How much better a particular action is in a particular state. \n",
    "    \n",
    "    Uses reward of current action + value function of that state-action pair, discount factor gamma, and then lamda to compute. \n",
    "    \"\"\"\n",
    "    rewards = torch.Tensor(rewards)\n",
    "    masks = torch.Tensor(masks)\n",
    "    returns = torch.zeros_like(rewards)\n",
    "    advants = torch.zeros_like(rewards)\n",
    "    \n",
    "    running_returns = 0\n",
    "    previous_value = 0\n",
    "    running_advants = 0\n",
    "\n",
    "    for t in reversed(range(0, len(rewards))): #for LL, only ever one step :-)\n",
    "        running_returns = rewards[t] + (gamma * running_returns * masks[t])\n",
    "        returns[t] = running_returns\n",
    "\n",
    "        running_delta = rewards[t] + (gamma * previous_value * masks[t]) - \\\n",
    "                                        values.data[t]\n",
    "        previous_value = values.data[t]\n",
    "        \n",
    "        running_advants = running_delta + (gamma * lamda * \\\n",
    "                                            running_advants * masks[t])\n",
    "        advants[t] = running_advants\n",
    "\n",
    "    advants = (advants - advants.mean()) / advants.std()\n",
    "    return returns, advants\n",
    "\n",
    "def surrogate_loss(actor, advants, states, old_policy, actions, batch_index):\n",
    "    \"\"\"\n",
    "    The loss for PPO. Re-run through network, recomput policy from states\n",
    "    and see if this surrogate ratio is better. If it is, use as proximal policy update. It's very close to prior policy, but def better. \n",
    "    \n",
    "    Not sure this actually works though. Should not the new mu and stds be used to draw,\n",
    "    \n",
    "        When do we use get_action? Only once in main, I think it should be for all? \n",
    "    \"\"\"\n",
    "    mu, std = actor(states)\n",
    "    new_policy = log_prob_density(actions, mu, std)\n",
    "    old_policy = old_policy[batch_index]\n",
    "\n",
    "    ratio = torch.exp(new_policy - old_policy)\n",
    "    surrogate_loss = ratio * advants\n",
    "    entropy = get_entropy(mu, std)\n",
    "\n",
    "    return surrogate_loss, ratio, entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 0\n",
    "train_discrim_flag = True\n",
    "total_sample_size = 256\n",
    "max_iter_num = 500\n",
    "render=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:: 256 episode score is 0.77\n",
      "Expert: 100.00% | Learner: 0.00%\n",
      "1:: 512 episode score is 0.75\n",
      "Expert: 100.00% | Learner: 0.00%\n",
      "score_avg: 0\n",
      "2:: 768 episode score is 0.73\n",
      "Expert: 100.00% | Learner: 0.00%\n",
      "score_avg: 0\n",
      "3:: 1024 episode score is 0.71\n",
      "Expert: 100.00% | Learner: 0.00%\n",
      "score_avg: 0\n",
      "4:: 1280 episode score is 0.70\n",
      "Expert: 0.00% | Learner: 100.00%\n",
      "score_avg: 0\n",
      "5:: 1536 episode score is 0.69\n",
      "Expert: 0.00% | Learner: 100.00%\n",
      "score_avg: 0\n",
      "6:: 1792 episode score is 0.68\n",
      "Expert: 0.00% | Learner: 100.00%\n",
      "score_avg: 0\n",
      "7:: 2048 episode score is 0.67\n",
      "Expert: 0.00% | Learner: 100.00%\n",
      "score_avg: 0\n",
      "8:: 2304 episode score is 0.66\n",
      "Expert: 0.00% | Learner: 100.00%\n",
      "score_avg: 0\n",
      "9:: 2560 episode score is 0.66\n",
      "Expert: 0.00% | Learner: 100.00%\n",
      "score_avg: 0\n",
      "10:: 2816 episode score is 0.66\n",
      "Expert: 0.00% | Learner: 100.00%\n",
      "score_avg: 0\n",
      "11:: 3072 episode score is 0.67\n",
      "Expert: 0.00% | Learner: 100.00%\n",
      "score_avg: 0\n",
      "12:: 3328 episode score is 0.67\n",
      "Expert: 0.00% | Learner: 100.00%\n",
      "score_avg: 0\n",
      "13:: 3584 episode score is 0.67\n",
      "Expert: 0.00% | Learner: 100.00%\n",
      "score_avg: 0\n",
      "14:: 3840 episode score is 0.68\n",
      "Expert: 0.00% | Learner: 100.00%\n",
      "score_avg: 0\n",
      "15:: 4096 episode score is 0.68\n",
      "Expert: 0.00% | Learner: 100.00%\n",
      "score_avg: 0\n",
      "16:: 4352 episode score is 0.69\n",
      "Expert: 12.50% | Learner: 87.50%\n",
      "score_avg: 0\n",
      "17:: 4608 episode score is 0.69\n",
      "Expert: 100.00% | Learner: 0.00%\n",
      "score_avg: 0\n",
      "18:: 4864 episode score is 0.70\n",
      "Expert: 100.00% | Learner: 0.00%\n",
      "score_avg: 0\n",
      "19:: 5120 episode score is 0.70\n",
      "Expert: 100.00% | Learner: 0.00%\n",
      "score_avg: 0\n",
      "20:: 5376 episode score is 0.70\n",
      "Expert: 100.00% | Learner: 0.00%\n",
      "score_avg: 0\n",
      "21:: 5632 episode score is 0.70\n",
      "Expert: 100.00% | Learner: 0.00%\n",
      "score_avg: 0\n",
      "22:: 5888 episode score is 0.70\n",
      "Expert: 100.00% | Learner: 0.00%\n",
      "score_avg: 0\n",
      "23:: 6144 episode score is 0.70\n",
      "Expert: 100.00% | Learner: 0.00%\n",
      "score_avg: 0\n",
      "24:: 6400 episode score is 0.70\n",
      "Expert: 100.00% | Learner: 0.00%\n",
      "score_avg: 0\n",
      "25:: 6656 episode score is 0.70\n",
      "Expert: 100.00% | Learner: 0.00%\n",
      "score_avg: 0\n",
      "26:: 6912 episode score is 0.70\n",
      "Expert: 99.61% | Learner: 0.39%\n",
      "score_avg: 0\n",
      "27:: 7168 episode score is 0.70\n",
      "Expert: 87.89% | Learner: 11.72%\n",
      "score_avg: 0\n",
      "28:: 7424 episode score is 0.69\n",
      "Expert: 3.52% | Learner: 96.09%\n",
      "score_avg: 0\n",
      "29:: 7680 episode score is 0.69\n",
      "Expert: 0.39% | Learner: 99.61%\n",
      "score_avg: 0\n",
      "30:: 7936 episode score is 0.69\n",
      "Expert: 0.00% | Learner: 100.00%\n",
      "score_avg: 0\n",
      "31:: 8192 episode score is 0.69\n",
      "Expert: 0.00% | Learner: 100.00%\n",
      "score_avg: 0\n",
      "32:: 8448 episode score is 0.69\n",
      "Expert: 0.00% | Learner: 100.00%\n",
      "score_avg: 0\n",
      "33:: 8704 episode score is 0.69\n",
      "Expert: 0.00% | Learner: 100.00%\n",
      "score_avg: 0\n",
      "34:: 8960 episode score is 0.69\n",
      "Expert: 0.00% | Learner: 100.00%\n",
      "score_avg: 0\n",
      "35:: 9216 episode score is 0.69\n",
      "Expert: 0.39% | Learner: 99.61%\n",
      "score_avg: 0\n",
      "36:: 9472 episode score is 0.69\n",
      "Expert: 2.34% | Learner: 97.66%\n",
      "score_avg: 0\n",
      "37:: 9728 episode score is 0.69\n",
      "Expert: 20.70% | Learner: 78.91%\n",
      "score_avg: 0\n",
      "38:: 9984 episode score is 0.69\n",
      "Expert: 81.64% | Learner: 18.75%\n",
      "score_avg: 0\n",
      "39:: 10240 episode score is 0.69\n",
      "Expert: 90.23% | Learner: 9.77%\n",
      "score_avg: 0\n",
      "40:: 10496 episode score is 0.69\n",
      "Expert: 95.70% | Learner: 4.30%\n",
      "score_avg: 0\n",
      "41:: 10752 episode score is 0.69\n",
      "Expert: 96.88% | Learner: 3.12%\n",
      "score_avg: 0\n",
      "42:: 11008 episode score is 0.69\n",
      "Expert: 98.05% | Learner: 1.56%\n",
      "score_avg: 0\n",
      "43:: 11264 episode score is 0.69\n",
      "Expert: 97.27% | Learner: 2.73%\n",
      "score_avg: 0\n",
      "44:: 11520 episode score is 0.69\n",
      "Expert: 93.75% | Learner: 5.47%\n",
      "score_avg: 0\n",
      "45:: 11776 episode score is 0.69\n",
      "Expert: 90.23% | Learner: 10.16%\n",
      "score_avg: 0\n",
      "46:: 12032 episode score is 0.69\n",
      "Expert: 82.03% | Learner: 18.36%\n",
      "score_avg: 0\n",
      "47:: 12288 episode score is 0.69\n",
      "Expert: 64.84% | Learner: 36.33%\n",
      "score_avg: 0\n",
      "48:: 12544 episode score is 0.69\n",
      "Expert: 41.02% | Learner: 61.72%\n",
      "score_avg: 0\n",
      "49:: 12800 episode score is 0.69\n",
      "Expert: 19.53% | Learner: 79.30%\n",
      "score_avg: 0\n",
      "50:: 13056 episode score is 0.69\n",
      "Expert: 12.50% | Learner: 85.55%\n",
      "score_avg: 0\n",
      "51:: 13312 episode score is 0.69\n",
      "Expert: 7.81% | Learner: 92.58%\n",
      "score_avg: 0\n",
      "52:: 13568 episode score is 0.69\n",
      "Expert: 10.16% | Learner: 90.23%\n",
      "score_avg: 0\n",
      "53:: 13824 episode score is 0.69\n",
      "Expert: 19.92% | Learner: 78.12%\n",
      "score_avg: 0\n",
      "54:: 14080 episode score is 0.69\n",
      "Expert: 39.84% | Learner: 59.77%\n",
      "score_avg: 0\n",
      "55:: 14336 episode score is 0.69\n",
      "Expert: 60.94% | Learner: 38.28%\n",
      "score_avg: 0\n",
      "56:: 14592 episode score is 0.69\n",
      "Expert: 66.80% | Learner: 33.59%\n",
      "score_avg: 0\n",
      "57:: 14848 episode score is 0.69\n",
      "Expert: 70.70% | Learner: 30.08%\n",
      "score_avg: 0\n",
      "58:: 15104 episode score is 0.69\n",
      "Expert: 76.95% | Learner: 23.44%\n"
     ]
    }
   ],
   "source": [
    "# Now what we came for...\n",
    "\n",
    "for iter in range(max_iter_num):\n",
    "    actor.eval(), critic.eval()\n",
    "    memory = deque()\n",
    "\n",
    "    steps = 0\n",
    "    scores = []\n",
    "\n",
    "    while steps < total_sample_size: \n",
    "        state, expert_action, raw_state, raw_expert_action = env.reset()\n",
    "        state = state.to(device)\n",
    "        expert_action = expert_action.to(device)\n",
    "        score = 0\n",
    "            \n",
    "        #print(\"breakpt\")\n",
    "       # break\n",
    "        \n",
    "        \n",
    "        for _ in range(10000): \n",
    "            if render:\n",
    "                print(raw_state, raw_expert_action)\n",
    "            steps += 1\n",
    "\n",
    "            #TODO\n",
    "\n",
    "            mu, std = actor(state.resize(1,60,300))\n",
    "            action = get_action(mu.cpu(), std.cpu())[0]\n",
    "            done= env.step(action)\n",
    "            irl_reward = get_reward(discrim, state, action)\n",
    "            if done:\n",
    "                mask = 0\n",
    "            else:\n",
    "                mask = 1\n",
    "\n",
    "            memory.append([state, torch.from_numpy(action).to(device), irl_reward, mask,expert_action])\n",
    "\n",
    "            sys.exit\n",
    "\n",
    "            score += irl_reward\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        episodes += 1\n",
    "        scores.append(score)\n",
    "\n",
    "    score_avg = np.mean(scores)\n",
    "    print('{}:: {} episode score is {:.2f}'.format(iter, episodes, score_avg))\n",
    "\n",
    "    actor.train(), critic.train(), discrim.train()\n",
    "    if train_discrim_flag:\n",
    "        expert_acc, learner_acc = train_discrim(discrim, memory, discrim_optim, discrim_update_num, clip_param)\n",
    "        print(\"Expert: %.2f%% | Learner: %.2f%%\" % (expert_acc * 100, learner_acc * 100))\n",
    "        if expert_acc > suspend_accu_exp and learner_acc > suspend_accu_gen:\n",
    "            train_discrim_flag = False\n",
    "    train_actor_critic(actor, critic, memory, actor_optim, critic_optim, actor_critic_update_num, batch_size, clip_param)\n",
    "\n",
    "    if iter % 100:\n",
    "        score_avg = int(score_avg)\n",
    "\n",
    "        print(\"score_avg:\",score_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/scratch/nsk367/anaconda3/envs/irl/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m(2660)\u001b[0;36mmse_loss\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m   2658 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m   2659 \u001b[0;31m    \u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m-> 2660 \u001b[0;31m    \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m   2661 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m   2662 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> u\n",
      "> \u001b[0;32m/scratch/nsk367/anaconda3/envs/irl/lib/python3.8/site-packages/torch/nn/modules/loss.py\u001b[0m(446)\u001b[0;36mforward\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    444 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    445 \u001b[0;31m    \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 446 \u001b[0;31m        \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    447 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    448 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> u\n",
      "> \u001b[0;32m/scratch/nsk367/anaconda3/envs/irl/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m(727)\u001b[0;36m_call_impl\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    725 \u001b[0;31m            \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    726 \u001b[0;31m        \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 727 \u001b[0;31m            \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    728 \u001b[0;31m        for hook in itertools.chain(\n",
      "\u001b[0m\u001b[0;32m    729 \u001b[0;31m                \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> u\n",
      "> \u001b[0;32m<ipython-input-16-94f10cd625e4>\u001b[0m(49)\u001b[0;36mtrain_actor_critic\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     47 \u001b[0;31m                                         \u001b[0;34m-\u001b[0m\u001b[0mclip_param\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     48 \u001b[0;31m                                         clip_param)\n",
      "\u001b[0m\u001b[0;32m---> 49 \u001b[0;31m            \u001b[0mcritic_loss1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclipped_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturns_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     50 \u001b[0;31m            \u001b[0mcritic_loss2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturns_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     51 \u001b[0;31m            \u001b[0mcritic_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcritic_loss1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcritic_loss2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> clipped_values\n",
      "tensor([[0.1806],\n",
      "        [0.1833],\n",
      "        [0.1837],\n",
      "        [0.1817],\n",
      "        [0.1818],\n",
      "        [0.1790],\n",
      "        [0.1793],\n",
      "        [0.1830],\n",
      "        [0.1800],\n",
      "        [0.1792],\n",
      "        [0.1849],\n",
      "        [0.1835],\n",
      "        [0.1789],\n",
      "        [0.1830],\n",
      "        [0.1802],\n",
      "        [0.1833],\n",
      "        [0.1825],\n",
      "        [0.1787],\n",
      "        [0.1817],\n",
      "        [0.1836],\n",
      "        [0.1805],\n",
      "        [0.1803],\n",
      "        [0.1830],\n",
      "        [0.1804],\n",
      "        [0.1787],\n",
      "        [0.1795],\n",
      "        [0.1802],\n",
      "        [0.1816],\n",
      "        [0.1829],\n",
      "        [0.1847],\n",
      "        [0.1801],\n",
      "        [0.1781]], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ipdb> returns_samples\n",
      "tensor([[3.6525],\n",
      "        [3.6440],\n",
      "        [3.6391],\n",
      "        [3.6486],\n",
      "        [3.6414],\n",
      "        [3.6531],\n",
      "        [3.6493],\n",
      "        [3.6482],\n",
      "        [3.6483],\n",
      "        [3.6431],\n",
      "        [3.6487],\n",
      "        [3.6351],\n",
      "        [3.6399],\n",
      "        [3.6481],\n",
      "        [3.6462],\n",
      "        [3.6529],\n",
      "        [3.6460],\n",
      "        [3.6386],\n",
      "        [3.6451],\n",
      "        [3.6465],\n",
      "        [3.5230],\n",
      "        [3.6498],\n",
      "        [3.6441],\n",
      "        [3.6548],\n",
      "        [1.4102],\n",
      "        [3.6400],\n",
      "        [3.6431],\n",
      "        [3.6488],\n",
      "        [3.6405],\n",
      "        [3.6502],\n",
      "        [3.6443],\n",
      "        [3.6517]])\n",
      "ipdb> exit\n"
     ]
    }
   ],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, std = actor(state.resize(1,60,300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = get_action(mu, std)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
