{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rough runthrough of GAIL for dialog generation, getting close..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque  \n",
    "#code for training \n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "from models import *\n",
    "from dialog_environment import *\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import torch\n",
    "from torch.distributions import Normal\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = DialogEnvironment()\n",
    "\n",
    "# Normally args but not here :-)\n",
    "seed = 0\n",
    "render = False\n",
    "gamma = 0.99\n",
    "lamda = .98\n",
    "\n",
    "train_discrim_flag = True\n",
    "learning_rate = 3e-4\n",
    "clip_param = .2\n",
    "discrim_update_num = 2\n",
    "actor_critic_update_num = 10\n",
    "l2_rate = 1e-3 # weight decay\n",
    "total_sample_size = 256 # total num of state-actions to collect before learning\n",
    "batch_size = 32\n",
    "suspend_accu_exp = 1. # do not need to be this high typically, but seems likely it has to be for a simple env like mountain car cont.\n",
    "suspend_accu_gen = 1.\n",
    "max_iter_num = 500\n",
    "\n",
    "\n",
    "actor = Actor(hidden_size=3,num_layers=3)\n",
    "critic = Critic(hidden_size=1,num_layers=3)\n",
    "discrim = Discriminator(input_size = 300, hidden_size=1,device='cuda',num_layers=3)\n",
    "actor.to(device), critic.to(device), discrim.to(device)\n",
    "actor_optim = optim.Adam(actor.parameters(), lr=learning_rate)\n",
    "critic_optim = optim.Adam(critic.parameters(), lr=learning_rate, \n",
    "                          weight_decay=l2_rate) \n",
    "discrim_optim = optim.Adam(discrim.parameters(), lr=learning_rate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def subsample(data, target, n=15):\n",
    "    return [x[::n] for x in data], [y[::n] for y in target]\n",
    "\n",
    "\n",
    "def get_action(mu, std):\n",
    "    action = torch.normal(mu, std)\n",
    "    action = action.data.numpy()\n",
    "    return action\n",
    "\n",
    "\n",
    "def get_entropy(mu, std):\n",
    "    dist = Normal(mu, std)\n",
    "    entropy = dist.entropy().mean()\n",
    "    return entropy\n",
    "\n",
    "def log_prob_density(x, mu, std):\n",
    "    log_prob_density = -(x - mu).pow(2) / (2 * std.pow(2)) \\\n",
    "                     - 0.5 * math.log(2 * math.pi)\n",
    "    return log_prob_density.sum(1, keepdim=True)\n",
    "\n",
    "def get_reward(discrim, state, action):\n",
    "    \"\"\"\n",
    "    The reward function according to irl. It's log D(s,a). \n",
    "    \n",
    "    Reward is higher the closer this is to 0, because the more similar it is to an expert action. :\n",
    "    Is quite close to imitation learning, but hope here is that with such a large number of expert demonstrations and entropy bonuses etc. it learns more than direct imitation. \n",
    "    \"\"\"\n",
    "\n",
    "    action = torch.Tensor(action).to(device)# turn state into a tensor if not already\n",
    "\n",
    "    with torch.no_grad():\n",
    "        return -math.log(discrim(state.resize(1,60,300),action.resize(1,60,300))[0].item())\n",
    "\n",
    "def save_checkpoint(state, filename):\n",
    "    return\n",
    "\n",
    "\n",
    "def train_discrim(discrim, memory, discrim_optim, discrim_update_num, clip_param):\n",
    "    \"\"\"\n",
    "    Training the discriminator. \n",
    "\n",
    "    Use binary cross entropy to classify whether \n",
    "    or not a sequence was predicted by the expert (real data) or actor. \n",
    "    \"\"\"\n",
    "    states = torch.stack([memory[i][0] for i in range(len(memory))])\n",
    "    actions = torch.stack([memory[i][1] for i in range(len(memory))])\n",
    "    rewards = [memory[i][2] for i in range(len(memory))]\n",
    "\n",
    "    masks = [memory[i][2] for i in range(len(memory))]\n",
    "    expert_actions = torch.stack([memory[i][4] for i in range(len(memory))])\n",
    "\n",
    "    criterion = torch.nn.BCELoss() # classify\n",
    "\n",
    "    for _ in range(discrim_update_num):\n",
    "\n",
    "        learner = discrim(states, actions) #pass (s,a) through discriminator\n",
    "\n",
    "        # TODO\n",
    "       # demonstrations = torch.Tensor([states, expert_actions]) # pass (s,a) of expert through discriminator\n",
    "        expert = discrim(states,expert_actions) #discrimator \"guesses\" whether or not these \n",
    "        # actions came from expert or learner\n",
    "        discrim_loss = criterion(learner, torch.ones((states.shape[0], 1)).to(device)) + \\\n",
    "                        criterion(expert, torch.zeros((states.shape[0], 1)).to(device))\n",
    "                # discrim loss: predict agent is all wrong, get as close to 0, and predict expert is 1, getting as close to 1 as possible. \n",
    "        discrim_optim.zero_grad() # gan loss, it tries to always get it right. \n",
    "        discrim_loss.backward()\n",
    "        discrim_optim.step()\n",
    "            # take these steps, do it however many times specified. \n",
    "        #return discrim(states,expert_actions) , discrim(states,actions)\n",
    "    expert_acc = ((discrim(states,expert_actions) < 0.5).float()).mean() #how often it realized the fake examples were fake\n",
    "    learner_acc = ((discrim(states,actions) > 0.5).float()).mean() #how often if predicted expert correctly. \n",
    "\n",
    "    return expert_acc, learner_acc # accuracy, it's the same kind, but because imbalanced better to look at separately. \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_actor_critic(actor, critic, memory, actor_optim, critic_optim, actor_critic_update_num, batch_size, clip_param):\n",
    "    \"\"\"\n",
    "    Take a PPO step or two to improve the actor critic model,  using GAE to estimate returns. \n",
    "    \n",
    "    In our case each trajectory it most one step, so the value function will have to do. \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    # tuple of a regular old RL problem, but now reward is what the discriminator says. \n",
    "    states = torch.stack([memory[i][0] for i in range(len(memory))])\n",
    "    actions = torch.stack([memory[i][1] for i in range(len(memory))])\n",
    "    rewards = [memory[i][2] for i in range(len(memory))]\n",
    "    masks = [memory[i][2] for i in range(len(memory))]\n",
    "    # compute value of what happened, see if what we can get us better. \n",
    "    old_values = critic(states)\n",
    "\n",
    "    #GAE aka estimate of Value + actual return roughtly \n",
    "    returns, advants = get_gae(rewards, masks, old_values, gamma, lamda)\n",
    "    \n",
    "    # pass states through actor, get corresponding actions\n",
    "    mu, std = actor(states)\n",
    "    # new mus and stds? \n",
    "    old_policy = log_prob_density(actions, mu, std) # sum of log probability\n",
    "    # of old actions\n",
    "\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    n = len(states)\n",
    "    arr = np.arange(n)\n",
    "\n",
    "    for _ in range(actor_critic_update_num):\n",
    "        np.random.shuffle(arr)\n",
    "\n",
    "        for i in range(n // batch_size): \n",
    "            batch_index = arr[batch_size * i : batch_size * (i + 1)]\n",
    "            #batch_index = torch.LongTensor(batch_index)\n",
    "            \n",
    "            inputs = states[batch_index]\n",
    "            actions_samples = actions[batch_index]\n",
    "            returns_samples = returns.unsqueeze(1)[batch_index].to(device)\n",
    "            advants_samples = advants.unsqueeze(1)[batch_index].to(device)\n",
    "            oldvalue_samples = old_values[batch_index].detach()\n",
    "        \n",
    "        \n",
    "            values = critic(inputs) #\n",
    "            clipped_values = oldvalue_samples + \\\n",
    "                             torch.clamp(values - oldvalue_samples,\n",
    "                                         -clip_param, \n",
    "                                         clip_param)\n",
    "            critic_loss1 = criterion(clipped_values, returns_samples)\n",
    "            critic_loss2 = criterion(values, returns_samples)\n",
    "            critic_loss = torch.max(critic_loss1, critic_loss2).mean()\n",
    "\n",
    "            loss, ratio, entropy = surrogate_loss(actor, advants_samples, inputs,\n",
    "                                         old_policy.detach(), actions_samples,\n",
    "                                         batch_index)\n",
    "            clipped_ratio = torch.clamp(ratio,\n",
    "                                        1.0 - clip_param,\n",
    "                                        1.0 + clip_param)\n",
    "            clipped_loss = clipped_ratio * advants_samples\n",
    "            actor_loss = -torch.min(loss, clipped_loss).mean()\n",
    "            #print(actor_loss,critic_loss,entropy)\n",
    "           # return actor_loss, critic_loss, entropy\n",
    "            loss = actor_loss + 0.5 * critic_loss - 0.001 * entropy #entropy bonus to promote exploration.\n",
    "\n",
    "            actor_optim.zero_grad()\n",
    "            loss.backward()\n",
    "            actor_optim.step()\n",
    "\n",
    "           # critic_optim.zero_grad()\n",
    "           # loss.backward() \n",
    "            critic_optim.step()\n",
    "\n",
    "def get_gae(rewards, masks, values, gamma, lamda):\n",
    "    \"\"\"\n",
    "    How much better a particular action is in a particular state. \n",
    "    \n",
    "    Uses reward of current action + value function of that state-action pair, discount factor gamma, and then lamda to compute. \n",
    "    \"\"\"\n",
    "    rewards = torch.Tensor(rewards)\n",
    "    masks = torch.Tensor(masks)\n",
    "    returns = torch.zeros_like(rewards)\n",
    "    advants = torch.zeros_like(rewards)\n",
    "    \n",
    "    running_returns = 0\n",
    "    previous_value = 0\n",
    "    running_advants = 0\n",
    "\n",
    "    for t in reversed(range(0, len(rewards))): #for LL, only ever one step :-)\n",
    "        running_returns = rewards[t] + (gamma * running_returns * masks[t])\n",
    "        returns[t] = running_returns\n",
    "\n",
    "        running_delta = rewards[t] + (gamma * previous_value * masks[t]) - \\\n",
    "                                        values.data[t]\n",
    "        previous_value = values.data[t]\n",
    "        \n",
    "        running_advants = running_delta + (gamma * lamda * \\\n",
    "                                            running_advants * masks[t])\n",
    "        advants[t] = running_advants\n",
    "\n",
    "    advants = (advants - advants.mean()) / advants.std()\n",
    "    return returns, advants\n",
    "\n",
    "def surrogate_loss(actor, advants, states, old_policy, actions, batch_index):\n",
    "    \"\"\"\n",
    "    The loss for PPO. Re-run through network, recomput policy from states\n",
    "    and see if this surrogate ratio is better. If it is, use as proximal policy update. It's very close to prior policy, but def better. \n",
    "    \n",
    "    Not sure this actually works though. Should not the new mu and stds be used to draw,\n",
    "    \n",
    "        When do we use get_action? Only once in main, I think it should be for all? \n",
    "    \"\"\"\n",
    "    mu, std = actor(states)\n",
    "    new_policy = log_prob_density(actions, mu, std)\n",
    "    old_policy = old_policy[batch_index]\n",
    "\n",
    "    ratio = torch.exp(new_policy - old_policy)\n",
    "    surrogate_loss = ratio * advants\n",
    "    entropy = get_entropy(mu, std)\n",
    "\n",
    "    return surrogate_loss, ratio, entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 0\n",
    "train_discrim_flag = True\n",
    "total_sample_size = 256\n",
    "max_iter_num = 500\n",
    "render=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:: 256 episode score is 0.77\n",
      "Expert: 100.00% | Learner: 0.00%\n",
      "1:: 512 episode score is 0.75\n",
      "Expert: 100.00% | Learner: 0.00%\n",
      "score_avg: 0\n",
      "2:: 768 episode score is 0.73\n",
      "Expert: 100.00% | Learner: 0.00%\n",
      "score_avg: 0\n",
      "3:: 1024 episode score is 0.71\n",
      "Expert: 100.00% | Learner: 0.00%\n",
      "score_avg: 0\n",
      "4:: 1280 episode score is 0.70\n",
      "Expert: 0.00% | Learner: 100.00%\n",
      "score_avg: 0\n",
      "5:: 1536 episode score is 0.69\n",
      "Expert: 0.00% | Learner: 100.00%\n",
      "score_avg: 0\n",
      "6:: 1792 episode score is 0.68\n",
      "Expert: 0.00% | Learner: 100.00%\n",
      "score_avg: 0\n",
      "7:: 2048 episode score is 0.67\n",
      "Expert: 0.00% | Learner: 100.00%\n",
      "score_avg: 0\n",
      "8:: 2304 episode score is 0.66\n",
      "Expert: 0.00% | Learner: 100.00%\n",
      "score_avg: 0\n",
      "9:: 2560 episode score is 0.66\n",
      "Expert: 0.00% | Learner: 100.00%\n",
      "score_avg: 0\n",
      "10:: 2816 episode score is 0.66\n",
      "Expert: 0.00% | Learner: 100.00%\n",
      "score_avg: 0\n",
      "11:: 3072 episode score is 0.67\n",
      "Expert: 0.00% | Learner: 100.00%\n",
      "score_avg: 0\n",
      "12:: 3328 episode score is 0.67\n",
      "Expert: 0.00% | Learner: 100.00%\n",
      "score_avg: 0\n",
      "13:: 3584 episode score is 0.67\n",
      "Expert: 0.00% | Learner: 100.00%\n",
      "score_avg: 0\n",
      "14:: 3840 episode score is 0.68\n",
      "Expert: 0.00% | Learner: 100.00%\n",
      "score_avg: 0\n",
      "15:: 4096 episode score is 0.68\n",
      "Expert: 0.00% | Learner: 100.00%\n",
      "score_avg: 0\n",
      "16:: 4352 episode score is 0.69\n",
      "Expert: 12.50% | Learner: 87.50%\n",
      "score_avg: 0\n",
      "17:: 4608 episode score is 0.69\n",
      "Expert: 100.00% | Learner: 0.00%\n",
      "score_avg: 0\n",
      "18:: 4864 episode score is 0.70\n",
      "Expert: 100.00% | Learner: 0.00%\n",
      "score_avg: 0\n",
      "19:: 5120 episode score is 0.70\n",
      "Expert: 100.00% | Learner: 0.00%\n",
      "score_avg: 0\n",
      "20:: 5376 episode score is 0.70\n",
      "Expert: 100.00% | Learner: 0.00%\n",
      "score_avg: 0\n",
      "21:: 5632 episode score is 0.70\n",
      "Expert: 100.00% | Learner: 0.00%\n",
      "score_avg: 0\n",
      "22:: 5888 episode score is 0.70\n",
      "Expert: 100.00% | Learner: 0.00%\n",
      "score_avg: 0\n",
      "23:: 6144 episode score is 0.70\n",
      "Expert: 100.00% | Learner: 0.00%\n",
      "score_avg: 0\n",
      "24:: 6400 episode score is 0.70\n",
      "Expert: 100.00% | Learner: 0.00%\n",
      "score_avg: 0\n",
      "25:: 6656 episode score is 0.70\n",
      "Expert: 100.00% | Learner: 0.00%\n",
      "score_avg: 0\n",
      "26:: 6912 episode score is 0.70\n",
      "Expert: 99.61% | Learner: 0.39%\n",
      "score_avg: 0\n",
      "27:: 7168 episode score is 0.70\n",
      "Expert: 87.89% | Learner: 11.72%\n",
      "score_avg: 0\n",
      "28:: 7424 episode score is 0.69\n",
      "Expert: 3.52% | Learner: 96.09%\n",
      "score_avg: 0\n",
      "29:: 7680 episode score is 0.69\n",
      "Expert: 0.39% | Learner: 99.61%\n",
      "score_avg: 0\n",
      "30:: 7936 episode score is 0.69\n",
      "Expert: 0.00% | Learner: 100.00%\n",
      "score_avg: 0\n",
      "31:: 8192 episode score is 0.69\n",
      "Expert: 0.00% | Learner: 100.00%\n",
      "score_avg: 0\n",
      "32:: 8448 episode score is 0.69\n",
      "Expert: 0.00% | Learner: 100.00%\n",
      "score_avg: 0\n",
      "33:: 8704 episode score is 0.69\n",
      "Expert: 0.00% | Learner: 100.00%\n",
      "score_avg: 0\n",
      "34:: 8960 episode score is 0.69\n",
      "Expert: 0.00% | Learner: 100.00%\n",
      "score_avg: 0\n",
      "35:: 9216 episode score is 0.69\n",
      "Expert: 0.39% | Learner: 99.61%\n",
      "score_avg: 0\n",
      "36:: 9472 episode score is 0.69\n",
      "Expert: 2.34% | Learner: 97.66%\n",
      "score_avg: 0\n",
      "37:: 9728 episode score is 0.69\n",
      "Expert: 20.70% | Learner: 78.91%\n",
      "score_avg: 0\n",
      "38:: 9984 episode score is 0.69\n",
      "Expert: 81.64% | Learner: 18.75%\n",
      "score_avg: 0\n",
      "39:: 10240 episode score is 0.69\n",
      "Expert: 90.23% | Learner: 9.77%\n",
      "score_avg: 0\n",
      "40:: 10496 episode score is 0.69\n",
      "Expert: 95.70% | Learner: 4.30%\n",
      "score_avg: 0\n",
      "41:: 10752 episode score is 0.69\n",
      "Expert: 96.88% | Learner: 3.12%\n",
      "score_avg: 0\n",
      "42:: 11008 episode score is 0.69\n",
      "Expert: 98.05% | Learner: 1.56%\n",
      "score_avg: 0\n",
      "43:: 11264 episode score is 0.69\n",
      "Expert: 97.27% | Learner: 2.73%\n",
      "score_avg: 0\n",
      "44:: 11520 episode score is 0.69\n",
      "Expert: 93.75% | Learner: 5.47%\n",
      "score_avg: 0\n",
      "45:: 11776 episode score is 0.69\n",
      "Expert: 90.23% | Learner: 10.16%\n",
      "score_avg: 0\n",
      "46:: 12032 episode score is 0.69\n",
      "Expert: 82.03% | Learner: 18.36%\n",
      "score_avg: 0\n",
      "47:: 12288 episode score is 0.69\n",
      "Expert: 64.84% | Learner: 36.33%\n",
      "score_avg: 0\n",
      "48:: 12544 episode score is 0.69\n",
      "Expert: 41.02% | Learner: 61.72%\n",
      "score_avg: 0\n",
      "49:: 12800 episode score is 0.69\n",
      "Expert: 19.53% | Learner: 79.30%\n",
      "score_avg: 0\n",
      "50:: 13056 episode score is 0.69\n",
      "Expert: 12.50% | Learner: 85.55%\n",
      "score_avg: 0\n",
      "51:: 13312 episode score is 0.69\n",
      "Expert: 7.81% | Learner: 92.58%\n",
      "score_avg: 0\n",
      "52:: 13568 episode score is 0.69\n",
      "Expert: 10.16% | Learner: 90.23%\n",
      "score_avg: 0\n",
      "53:: 13824 episode score is 0.69\n",
      "Expert: 19.92% | Learner: 78.12%\n",
      "score_avg: 0\n",
      "54:: 14080 episode score is 0.69\n",
      "Expert: 39.84% | Learner: 59.77%\n",
      "score_avg: 0\n",
      "55:: 14336 episode score is 0.69\n",
      "Expert: 60.94% | Learner: 38.28%\n",
      "score_avg: 0\n",
      "56:: 14592 episode score is 0.69\n",
      "Expert: 66.80% | Learner: 33.59%\n",
      "score_avg: 0\n",
      "57:: 14848 episode score is 0.69\n",
      "Expert: 70.70% | Learner: 30.08%\n",
      "score_avg: 0\n",
      "58:: 15104 episode score is 0.69\n",
      "Expert: 76.95% | Learner: 23.44%\n",
      "score_avg: 0\n",
      "59:: 15360 episode score is 0.69\n",
      "Expert: 78.91% | Learner: 22.66%\n",
      "score_avg: 0\n",
      "60:: 15616 episode score is 0.69\n",
      "Expert: 77.34% | Learner: 23.05%\n",
      "score_avg: 0\n",
      "61:: 15872 episode score is 0.69\n",
      "Expert: 77.73% | Learner: 22.27%\n",
      "score_avg: 0\n",
      "62:: 16128 episode score is 0.69\n",
      "Expert: 80.47% | Learner: 21.09%\n",
      "score_avg: 0\n",
      "63:: 16384 episode score is 0.69\n",
      "Expert: 76.95% | Learner: 23.05%\n",
      "score_avg: 0\n",
      "64:: 16640 episode score is 0.69\n",
      "Expert: 72.66% | Learner: 26.56%\n",
      "score_avg: 0\n",
      "65:: 16896 episode score is 0.69\n",
      "Expert: 64.45% | Learner: 36.33%\n",
      "score_avg: 0\n",
      "66:: 17152 episode score is 0.69\n",
      "Expert: 69.14% | Learner: 31.64%\n",
      "score_avg: 0\n",
      "67:: 17408 episode score is 0.69\n",
      "Expert: 64.84% | Learner: 34.38%\n",
      "score_avg: 0\n",
      "68:: 17664 episode score is 0.69\n",
      "Expert: 60.16% | Learner: 39.06%\n",
      "score_avg: 0\n",
      "69:: 17920 episode score is 0.69\n",
      "Expert: 60.16% | Learner: 39.84%\n",
      "score_avg: 0\n",
      "70:: 18176 episode score is 0.69\n",
      "Expert: 65.62% | Learner: 33.98%\n",
      "score_avg: 0\n",
      "71:: 18432 episode score is 0.69\n",
      "Expert: 62.11% | Learner: 37.11%\n",
      "score_avg: 0\n",
      "72:: 18688 episode score is 0.69\n",
      "Expert: 67.19% | Learner: 33.59%\n",
      "score_avg: 0\n",
      "73:: 18944 episode score is 0.69\n",
      "Expert: 68.36% | Learner: 32.81%\n",
      "score_avg: 0\n",
      "74:: 19200 episode score is 0.69\n",
      "Expert: 70.31% | Learner: 31.64%\n",
      "score_avg: 0\n",
      "75:: 19456 episode score is 0.69\n",
      "Expert: 70.31% | Learner: 29.69%\n",
      "score_avg: 0\n",
      "76:: 19712 episode score is 0.69\n",
      "Expert: 70.70% | Learner: 30.08%\n",
      "score_avg: 0\n",
      "77:: 19968 episode score is 0.69\n",
      "Expert: 72.27% | Learner: 27.73%\n",
      "score_avg: 0\n",
      "78:: 20224 episode score is 0.69\n",
      "Expert: 71.09% | Learner: 29.30%\n",
      "score_avg: 0\n",
      "79:: 20480 episode score is 0.69\n",
      "Expert: 71.88% | Learner: 29.30%\n",
      "score_avg: 0\n",
      "80:: 20736 episode score is 0.69\n",
      "Expert: 69.92% | Learner: 30.08%\n",
      "score_avg: 0\n",
      "81:: 20992 episode score is 0.69\n",
      "Expert: 68.36% | Learner: 33.59%\n",
      "score_avg: 0\n",
      "82:: 21248 episode score is 0.69\n",
      "Expert: 69.14% | Learner: 32.42%\n",
      "score_avg: 0\n",
      "83:: 21504 episode score is 0.69\n",
      "Expert: 62.11% | Learner: 39.84%\n",
      "score_avg: 0\n",
      "84:: 21760 episode score is 0.69\n",
      "Expert: 62.89% | Learner: 38.67%\n",
      "score_avg: 0\n",
      "85:: 22016 episode score is 0.69\n",
      "Expert: 64.06% | Learner: 35.55%\n",
      "score_avg: 0\n",
      "86:: 22272 episode score is 0.69\n",
      "Expert: 61.33% | Learner: 38.67%\n",
      "score_avg: 0\n",
      "87:: 22528 episode score is 0.69\n",
      "Expert: 61.72% | Learner: 39.45%\n",
      "score_avg: 0\n",
      "88:: 22784 episode score is 0.69\n",
      "Expert: 69.92% | Learner: 32.42%\n",
      "score_avg: 0\n",
      "89:: 23040 episode score is 0.69\n",
      "Expert: 76.56% | Learner: 24.22%\n",
      "score_avg: 0\n",
      "90:: 23296 episode score is 0.69\n",
      "Expert: 76.17% | Learner: 24.22%\n",
      "score_avg: 0\n",
      "91:: 23552 episode score is 0.69\n",
      "Expert: 69.14% | Learner: 30.47%\n",
      "score_avg: 0\n",
      "92:: 23808 episode score is 0.69\n",
      "Expert: 71.48% | Learner: 28.91%\n",
      "score_avg: 0\n",
      "93:: 24064 episode score is 0.69\n",
      "Expert: 64.45% | Learner: 36.72%\n",
      "score_avg: 0\n",
      "94:: 24320 episode score is 0.69\n",
      "Expert: 66.02% | Learner: 34.38%\n",
      "score_avg: 0\n",
      "95:: 24576 episode score is 0.69\n",
      "Expert: 64.84% | Learner: 35.55%\n",
      "score_avg: 0\n",
      "96:: 24832 episode score is 0.69\n",
      "Expert: 66.41% | Learner: 34.77%\n",
      "score_avg: 0\n",
      "97:: 25088 episode score is 0.69\n",
      "Expert: 65.23% | Learner: 37.50%\n",
      "score_avg: 0\n",
      "98:: 25344 episode score is 0.69\n",
      "Expert: 64.84% | Learner: 35.94%\n",
      "score_avg: 0\n",
      "99:: 25600 episode score is 0.69\n",
      "Expert: 60.94% | Learner: 37.50%\n",
      "score_avg: 0\n",
      "100:: 25856 episode score is 0.69\n",
      "Expert: 60.16% | Learner: 40.23%\n",
      "101:: 26112 episode score is 0.69\n",
      "Expert: 62.89% | Learner: 37.11%\n",
      "score_avg: 0\n",
      "102:: 26368 episode score is 0.69\n",
      "Expert: 51.17% | Learner: 44.53%\n",
      "score_avg: 0\n",
      "103:: 26624 episode score is 0.69\n",
      "Expert: 44.92% | Learner: 52.73%\n",
      "score_avg: 0\n",
      "104:: 26880 episode score is 0.69\n",
      "Expert: 48.83% | Learner: 50.00%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score_avg: 0\n",
      "105:: 27136 episode score is 0.69\n",
      "Expert: 50.39% | Learner: 44.14%\n",
      "score_avg: 0\n",
      "106:: 27392 episode score is 0.69\n",
      "Expert: 53.91% | Learner: 44.53%\n",
      "score_avg: 0\n",
      "107:: 27648 episode score is 0.69\n",
      "Expert: 58.98% | Learner: 39.84%\n",
      "score_avg: 0\n",
      "108:: 27904 episode score is 0.69\n",
      "Expert: 68.36% | Learner: 32.81%\n",
      "score_avg: 0\n",
      "109:: 28160 episode score is 0.69\n",
      "Expert: 64.06% | Learner: 37.50%\n",
      "score_avg: 0\n",
      "110:: 28416 episode score is 0.69\n",
      "Expert: 65.62% | Learner: 36.72%\n",
      "score_avg: 0\n",
      "111:: 28672 episode score is 0.69\n",
      "Expert: 73.05% | Learner: 26.95%\n",
      "score_avg: 0\n",
      "112:: 28928 episode score is 0.69\n",
      "Expert: 69.92% | Learner: 30.86%\n",
      "score_avg: 0\n",
      "113:: 29184 episode score is 0.69\n",
      "Expert: 72.27% | Learner: 27.73%\n",
      "score_avg: 0\n",
      "114:: 29440 episode score is 0.69\n",
      "Expert: 72.27% | Learner: 29.30%\n",
      "score_avg: 0\n",
      "115:: 29696 episode score is 0.69\n",
      "Expert: 73.44% | Learner: 27.73%\n",
      "score_avg: 0\n",
      "116:: 29952 episode score is 0.69\n",
      "Expert: 67.97% | Learner: 32.03%\n",
      "score_avg: 0\n",
      "117:: 30208 episode score is 0.69\n",
      "Expert: 65.62% | Learner: 35.16%\n",
      "score_avg: 0\n",
      "118:: 30464 episode score is 0.69\n",
      "Expert: 59.77% | Learner: 40.62%\n",
      "score_avg: 0\n",
      "119:: 30720 episode score is 0.69\n",
      "Expert: 63.28% | Learner: 35.16%\n",
      "score_avg: 0\n",
      "120:: 30976 episode score is 0.69\n",
      "Expert: 59.77% | Learner: 42.58%\n",
      "score_avg: 0\n",
      "121:: 31232 episode score is 0.69\n",
      "Expert: 58.20% | Learner: 42.58%\n",
      "score_avg: 0\n",
      "122:: 31488 episode score is 0.69\n",
      "Expert: 66.80% | Learner: 33.59%\n",
      "score_avg: 0\n",
      "123:: 31744 episode score is 0.69\n",
      "Expert: 59.38% | Learner: 40.62%\n",
      "score_avg: 0\n",
      "124:: 32000 episode score is 0.69\n",
      "Expert: 60.94% | Learner: 38.67%\n",
      "score_avg: 0\n",
      "125:: 32256 episode score is 0.69\n",
      "Expert: 65.62% | Learner: 35.94%\n",
      "score_avg: 0\n",
      "126:: 32512 episode score is 0.69\n",
      "Expert: 64.45% | Learner: 35.55%\n",
      "score_avg: 0\n",
      "127:: 32768 episode score is 0.69\n",
      "Expert: 60.16% | Learner: 39.84%\n",
      "score_avg: 0\n",
      "128:: 33024 episode score is 0.69\n",
      "Expert: 65.62% | Learner: 36.33%\n",
      "score_avg: 0\n",
      "129:: 33280 episode score is 0.69\n",
      "Expert: 68.36% | Learner: 32.03%\n",
      "score_avg: 0\n",
      "130:: 33536 episode score is 0.69\n",
      "Expert: 62.50% | Learner: 36.72%\n",
      "score_avg: 0\n",
      "131:: 33792 episode score is 0.69\n",
      "Expert: 62.89% | Learner: 38.28%\n",
      "score_avg: 0\n",
      "132:: 34048 episode score is 0.69\n",
      "Expert: 68.36% | Learner: 33.98%\n",
      "score_avg: 0\n",
      "133:: 34304 episode score is 0.69\n",
      "Expert: 68.75% | Learner: 33.98%\n",
      "score_avg: 0\n",
      "134:: 34560 episode score is 0.69\n",
      "Expert: 64.84% | Learner: 36.33%\n",
      "score_avg: 0\n",
      "135:: 34816 episode score is 0.69\n",
      "Expert: 77.34% | Learner: 24.61%\n",
      "score_avg: 0\n",
      "136:: 35072 episode score is 0.69\n",
      "Expert: 71.48% | Learner: 30.86%\n",
      "score_avg: 0\n",
      "137:: 35328 episode score is 0.69\n",
      "Expert: 71.09% | Learner: 30.86%\n",
      "score_avg: 0\n",
      "138:: 35584 episode score is 0.69\n",
      "Expert: 72.66% | Learner: 28.52%\n",
      "score_avg: 0\n",
      "139:: 35840 episode score is 0.69\n",
      "Expert: 69.92% | Learner: 31.64%\n",
      "score_avg: 0\n",
      "140:: 36096 episode score is 0.69\n",
      "Expert: 65.62% | Learner: 33.59%\n",
      "score_avg: 0\n",
      "141:: 36352 episode score is 0.69\n",
      "Expert: 60.94% | Learner: 39.45%\n",
      "score_avg: 0\n",
      "142:: 36608 episode score is 0.69\n",
      "Expert: 62.89% | Learner: 37.89%\n",
      "score_avg: 0\n",
      "143:: 36864 episode score is 0.69\n",
      "Expert: 61.33% | Learner: 40.23%\n",
      "score_avg: 0\n",
      "144:: 37120 episode score is 0.69\n",
      "Expert: 71.09% | Learner: 29.30%\n",
      "score_avg: 0\n",
      "145:: 37376 episode score is 0.69\n",
      "Expert: 65.23% | Learner: 36.33%\n",
      "score_avg: 0\n",
      "146:: 37632 episode score is 0.69\n",
      "Expert: 69.92% | Learner: 30.47%\n",
      "score_avg: 0\n",
      "147:: 37888 episode score is 0.69\n",
      "Expert: 71.48% | Learner: 30.08%\n",
      "score_avg: 0\n",
      "148:: 38144 episode score is 0.69\n",
      "Expert: 67.19% | Learner: 33.20%\n",
      "score_avg: 0\n",
      "149:: 38400 episode score is 0.69\n",
      "Expert: 73.83% | Learner: 28.12%\n",
      "score_avg: 0\n",
      "150:: 38656 episode score is 0.69\n",
      "Expert: 65.62% | Learner: 35.55%\n",
      "score_avg: 0\n",
      "151:: 38912 episode score is 0.69\n",
      "Expert: 66.02% | Learner: 35.55%\n",
      "score_avg: 0\n",
      "152:: 39168 episode score is 0.69\n",
      "Expert: 63.28% | Learner: 37.11%\n",
      "score_avg: 0\n",
      "153:: 39424 episode score is 0.69\n",
      "Expert: 64.06% | Learner: 36.72%\n",
      "score_avg: 0\n",
      "154:: 39680 episode score is 0.69\n",
      "Expert: 72.27% | Learner: 28.91%\n",
      "score_avg: 0\n",
      "155:: 39936 episode score is 0.69\n",
      "Expert: 76.95% | Learner: 25.00%\n",
      "score_avg: 0\n",
      "156:: 40192 episode score is 0.69\n",
      "Expert: 67.97% | Learner: 33.98%\n",
      "score_avg: 0\n",
      "157:: 40448 episode score is 0.69\n",
      "Expert: 72.27% | Learner: 32.03%\n",
      "score_avg: 0\n",
      "158:: 40704 episode score is 0.69\n",
      "Expert: 75.00% | Learner: 27.34%\n",
      "score_avg: 0\n",
      "159:: 40960 episode score is 0.69\n",
      "Expert: 67.19% | Learner: 32.81%\n",
      "score_avg: 0\n",
      "160:: 41216 episode score is 0.69\n",
      "Expert: 68.75% | Learner: 32.81%\n",
      "score_avg: 0\n",
      "161:: 41472 episode score is 0.69\n",
      "Expert: 66.80% | Learner: 33.59%\n",
      "score_avg: 0\n",
      "162:: 41728 episode score is 0.69\n",
      "Expert: 71.09% | Learner: 31.25%\n",
      "score_avg: 0\n",
      "163:: 41984 episode score is 0.69\n",
      "Expert: 69.92% | Learner: 33.20%\n",
      "score_avg: 0\n",
      "164:: 42240 episode score is 0.69\n",
      "Expert: 77.73% | Learner: 23.83%\n",
      "score_avg: 0\n",
      "165:: 42496 episode score is 0.69\n",
      "Expert: 71.09% | Learner: 30.86%\n",
      "score_avg: 0\n",
      "166:: 42752 episode score is 0.69\n",
      "Expert: 69.14% | Learner: 33.59%\n",
      "score_avg: 0\n",
      "167:: 43008 episode score is 0.69\n",
      "Expert: 75.78% | Learner: 26.56%\n",
      "score_avg: 0\n",
      "168:: 43264 episode score is 0.69\n",
      "Expert: 71.09% | Learner: 32.81%\n",
      "score_avg: 0\n",
      "169:: 43520 episode score is 0.69\n",
      "Expert: 72.66% | Learner: 30.47%\n",
      "score_avg: 0\n",
      "170:: 43776 episode score is 0.69\n",
      "Expert: 69.53% | Learner: 32.42%\n",
      "score_avg: 0\n",
      "171:: 44032 episode score is 0.69\n",
      "Expert: 71.88% | Learner: 29.30%\n",
      "score_avg: 0\n",
      "172:: 44288 episode score is 0.69\n",
      "Expert: 69.53% | Learner: 32.03%\n",
      "score_avg: 0\n",
      "173:: 44544 episode score is 0.69\n",
      "Expert: 72.27% | Learner: 28.91%\n",
      "score_avg: 0\n",
      "174:: 44800 episode score is 0.69\n",
      "Expert: 71.88% | Learner: 28.91%\n",
      "score_avg: 0\n",
      "175:: 45056 episode score is 0.69\n",
      "Expert: 78.12% | Learner: 24.61%\n",
      "score_avg: 0\n",
      "176:: 45312 episode score is 0.69\n",
      "Expert: 75.78% | Learner: 27.34%\n",
      "score_avg: 0\n",
      "177:: 45568 episode score is 0.69\n",
      "Expert: 71.88% | Learner: 30.47%\n",
      "score_avg: 0\n",
      "178:: 45824 episode score is 0.69\n",
      "Expert: 71.88% | Learner: 30.86%\n",
      "score_avg: 0\n",
      "179:: 46080 episode score is 0.69\n",
      "Expert: 69.92% | Learner: 31.25%\n",
      "score_avg: 0\n",
      "180:: 46336 episode score is 0.69\n",
      "Expert: 72.27% | Learner: 30.47%\n",
      "score_avg: 0\n",
      "181:: 46592 episode score is 0.69\n",
      "Expert: 69.53% | Learner: 33.20%\n",
      "score_avg: 0\n",
      "182:: 46848 episode score is 0.69\n",
      "Expert: 71.09% | Learner: 30.47%\n",
      "score_avg: 0\n",
      "183:: 47104 episode score is 0.69\n",
      "Expert: 69.53% | Learner: 31.25%\n",
      "score_avg: 0\n",
      "184:: 47360 episode score is 0.69\n",
      "Expert: 76.95% | Learner: 23.83%\n",
      "score_avg: 0\n",
      "185:: 47616 episode score is 0.69\n",
      "Expert: 73.44% | Learner: 27.73%\n",
      "score_avg: 0\n",
      "186:: 47872 episode score is 0.69\n",
      "Expert: 71.88% | Learner: 29.69%\n",
      "score_avg: 0\n",
      "187:: 48128 episode score is 0.69\n",
      "Expert: 73.05% | Learner: 28.12%\n",
      "score_avg: 0\n",
      "188:: 48384 episode score is 0.69\n",
      "Expert: 72.66% | Learner: 28.52%\n",
      "score_avg: 0\n",
      "189:: 48640 episode score is 0.69\n",
      "Expert: 76.56% | Learner: 24.22%\n",
      "score_avg: 0\n",
      "190:: 48896 episode score is 0.69\n",
      "Expert: 77.34% | Learner: 25.39%\n",
      "score_avg: 0\n",
      "191:: 49152 episode score is 0.69\n",
      "Expert: 74.61% | Learner: 28.12%\n",
      "score_avg: 0\n",
      "192:: 49408 episode score is 0.69\n",
      "Expert: 71.48% | Learner: 29.69%\n",
      "score_avg: 0\n",
      "193:: 49664 episode score is 0.69\n",
      "Expert: 74.61% | Learner: 26.56%\n",
      "score_avg: 0\n",
      "194:: 49920 episode score is 0.69\n",
      "Expert: 70.70% | Learner: 31.25%\n",
      "score_avg: 0\n",
      "195:: 50176 episode score is 0.69\n",
      "Expert: 73.05% | Learner: 29.30%\n",
      "score_avg: 0\n",
      "196:: 50432 episode score is 0.69\n",
      "Expert: 73.83% | Learner: 28.12%\n",
      "score_avg: 0\n",
      "197:: 50688 episode score is 0.69\n",
      "Expert: 75.39% | Learner: 24.61%\n",
      "score_avg: 0\n",
      "198:: 50944 episode score is 0.69\n",
      "Expert: 66.41% | Learner: 34.77%\n",
      "score_avg: 0\n",
      "199:: 51200 episode score is 0.69\n",
      "Expert: 73.83% | Learner: 27.73%\n",
      "score_avg: 0\n",
      "200:: 51456 episode score is 0.69\n",
      "Expert: 70.31% | Learner: 32.03%\n",
      "201:: 51712 episode score is 0.69\n",
      "Expert: 75.00% | Learner: 25.00%\n",
      "score_avg: 0\n",
      "202:: 51968 episode score is 0.69\n",
      "Expert: 76.17% | Learner: 25.00%\n",
      "score_avg: 0\n",
      "203:: 52224 episode score is 0.69\n",
      "Expert: 80.08% | Learner: 20.70%\n",
      "score_avg: 0\n",
      "204:: 52480 episode score is 0.69\n",
      "Expert: 76.95% | Learner: 24.22%\n",
      "score_avg: 0\n",
      "205:: 52736 episode score is 0.69\n",
      "Expert: 76.17% | Learner: 24.61%\n",
      "score_avg: 0\n",
      "206:: 52992 episode score is 0.69\n",
      "Expert: 79.30% | Learner: 21.88%\n",
      "score_avg: 0\n",
      "207:: 53248 episode score is 0.69\n",
      "Expert: 71.48% | Learner: 29.30%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score_avg: 0\n",
      "208:: 53504 episode score is 0.69\n",
      "Expert: 73.05% | Learner: 28.52%\n",
      "score_avg: 0\n",
      "209:: 53760 episode score is 0.69\n",
      "Expert: 80.47% | Learner: 22.66%\n",
      "score_avg: 0\n",
      "210:: 54016 episode score is 0.69\n",
      "Expert: 73.44% | Learner: 28.52%\n",
      "score_avg: 0\n",
      "211:: 54272 episode score is 0.69\n",
      "Expert: 75.00% | Learner: 28.52%\n",
      "score_avg: 0\n",
      "212:: 54528 episode score is 0.69\n",
      "Expert: 79.30% | Learner: 22.27%\n",
      "score_avg: 0\n",
      "213:: 54784 episode score is 0.69\n",
      "Expert: 82.42% | Learner: 25.00%\n",
      "score_avg: 0\n",
      "214:: 55040 episode score is 0.69\n",
      "Expert: 73.05% | Learner: 29.30%\n",
      "score_avg: 0\n",
      "215:: 55296 episode score is 0.69\n",
      "Expert: 71.48% | Learner: 30.86%\n",
      "score_avg: 0\n",
      "216:: 55552 episode score is 0.69\n",
      "Expert: 77.34% | Learner: 25.00%\n",
      "score_avg: 0\n",
      "217:: 55808 episode score is 0.69\n",
      "Expert: 80.08% | Learner: 22.27%\n",
      "score_avg: 0\n",
      "218:: 56064 episode score is 0.69\n",
      "Expert: 78.91% | Learner: 25.00%\n",
      "score_avg: 0\n",
      "219:: 56320 episode score is 0.69\n",
      "Expert: 79.69% | Learner: 20.70%\n",
      "score_avg: 0\n",
      "220:: 56576 episode score is 0.69\n",
      "Expert: 76.17% | Learner: 25.39%\n",
      "score_avg: 0\n",
      "221:: 56832 episode score is 0.69\n",
      "Expert: 81.64% | Learner: 22.27%\n",
      "score_avg: 0\n",
      "222:: 57088 episode score is 0.69\n",
      "Expert: 76.95% | Learner: 26.17%\n",
      "score_avg: 0\n",
      "223:: 57344 episode score is 0.69\n",
      "Expert: 78.12% | Learner: 24.61%\n",
      "score_avg: 0\n",
      "224:: 57600 episode score is 0.69\n",
      "Expert: 76.17% | Learner: 27.73%\n",
      "score_avg: 0\n",
      "225:: 57856 episode score is 0.69\n",
      "Expert: 82.42% | Learner: 19.53%\n",
      "score_avg: 0\n",
      "226:: 58112 episode score is 0.69\n",
      "Expert: 76.95% | Learner: 25.39%\n",
      "score_avg: 0\n",
      "227:: 58368 episode score is 0.69\n",
      "Expert: 76.17% | Learner: 26.56%\n",
      "score_avg: 0\n",
      "228:: 58624 episode score is 0.69\n",
      "Expert: 82.03% | Learner: 19.92%\n",
      "score_avg: 0\n",
      "229:: 58880 episode score is 0.69\n",
      "Expert: 82.81% | Learner: 21.09%\n",
      "score_avg: 0\n",
      "230:: 59136 episode score is 0.69\n",
      "Expert: 82.42% | Learner: 19.92%\n",
      "score_avg: 0\n",
      "231:: 59392 episode score is 0.69\n",
      "Expert: 80.86% | Learner: 21.88%\n",
      "score_avg: 0\n",
      "232:: 59648 episode score is 0.69\n",
      "Expert: 78.91% | Learner: 25.78%\n",
      "score_avg: 0\n",
      "233:: 59904 episode score is 0.69\n",
      "Expert: 79.30% | Learner: 23.05%\n",
      "score_avg: 0\n",
      "234:: 60160 episode score is 0.69\n",
      "Expert: 78.12% | Learner: 23.05%\n",
      "score_avg: 0\n",
      "235:: 60416 episode score is 0.69\n",
      "Expert: 85.55% | Learner: 18.36%\n",
      "score_avg: 0\n",
      "236:: 60672 episode score is 0.69\n",
      "Expert: 79.30% | Learner: 21.48%\n",
      "score_avg: 0\n",
      "237:: 60928 episode score is 0.69\n",
      "Expert: 78.12% | Learner: 23.05%\n",
      "score_avg: 0\n",
      "238:: 61184 episode score is 0.69\n",
      "Expert: 82.42% | Learner: 18.75%\n",
      "score_avg: 0\n",
      "239:: 61440 episode score is 0.69\n",
      "Expert: 83.98% | Learner: 16.80%\n",
      "score_avg: 0\n",
      "240:: 61696 episode score is 0.69\n",
      "Expert: 83.59% | Learner: 18.36%\n",
      "score_avg: 0\n",
      "241:: 61952 episode score is 0.69\n",
      "Expert: 86.33% | Learner: 17.58%\n",
      "score_avg: 0\n",
      "242:: 62208 episode score is 0.69\n",
      "Expert: 82.42% | Learner: 18.75%\n",
      "score_avg: 0\n",
      "243:: 62464 episode score is 0.69\n",
      "Expert: 80.08% | Learner: 23.44%\n",
      "score_avg: 0\n",
      "244:: 62720 episode score is 0.69\n",
      "Expert: 83.98% | Learner: 17.97%\n",
      "score_avg: 0\n",
      "245:: 62976 episode score is 0.69\n",
      "Expert: 84.77% | Learner: 17.58%\n",
      "score_avg: 0\n",
      "246:: 63232 episode score is 0.69\n",
      "Expert: 85.16% | Learner: 16.41%\n",
      "score_avg: 0\n",
      "247:: 63488 episode score is 0.69\n",
      "Expert: 84.77% | Learner: 18.75%\n",
      "score_avg: 0\n",
      "248:: 63744 episode score is 0.69\n",
      "Expert: 83.59% | Learner: 18.36%\n",
      "score_avg: 0\n",
      "249:: 64000 episode score is 0.69\n",
      "Expert: 82.42% | Learner: 21.48%\n",
      "score_avg: 0\n",
      "250:: 64256 episode score is 0.69\n",
      "Expert: 88.67% | Learner: 15.23%\n",
      "score_avg: 0\n",
      "251:: 64512 episode score is 0.69\n",
      "Expert: 83.98% | Learner: 18.75%\n",
      "score_avg: 0\n",
      "252:: 64768 episode score is 0.69\n",
      "Expert: 89.06% | Learner: 15.23%\n",
      "score_avg: 0\n",
      "253:: 65024 episode score is 0.69\n",
      "Expert: 84.77% | Learner: 16.80%\n",
      "score_avg: 0\n",
      "254:: 65280 episode score is 0.69\n",
      "Expert: 89.06% | Learner: 14.45%\n",
      "score_avg: 0\n",
      "255:: 65536 episode score is 0.69\n",
      "Expert: 85.16% | Learner: 19.14%\n",
      "score_avg: 0\n",
      "256:: 65792 episode score is 0.69\n",
      "Expert: 89.06% | Learner: 16.02%\n",
      "score_avg: 0\n",
      "257:: 66048 episode score is 0.69\n",
      "Expert: 87.50% | Learner: 16.41%\n",
      "score_avg: 0\n",
      "258:: 66304 episode score is 0.69\n",
      "Expert: 83.59% | Learner: 20.31%\n",
      "score_avg: 0\n",
      "259:: 66560 episode score is 0.69\n",
      "Expert: 87.89% | Learner: 15.62%\n",
      "score_avg: 0\n",
      "260:: 66816 episode score is 0.69\n",
      "Expert: 86.33% | Learner: 19.14%\n",
      "score_avg: 0\n",
      "261:: 67072 episode score is 0.69\n",
      "Expert: 91.02% | Learner: 13.28%\n",
      "score_avg: 0\n",
      "262:: 67328 episode score is 0.69\n",
      "Expert: 88.67% | Learner: 15.62%\n",
      "score_avg: 0\n",
      "263:: 67584 episode score is 0.69\n",
      "Expert: 90.23% | Learner: 14.84%\n",
      "score_avg: 0\n",
      "264:: 67840 episode score is 0.69\n",
      "Expert: 89.06% | Learner: 15.23%\n",
      "score_avg: 0\n",
      "265:: 68096 episode score is 0.69\n",
      "Expert: 89.06% | Learner: 17.19%\n",
      "score_avg: 0\n",
      "266:: 68352 episode score is 0.69\n",
      "Expert: 89.45% | Learner: 16.02%\n",
      "score_avg: 0\n",
      "267:: 68608 episode score is 0.69\n",
      "Expert: 89.06% | Learner: 15.62%\n",
      "score_avg: 0\n",
      "268:: 68864 episode score is 0.69\n",
      "Expert: 92.19% | Learner: 12.50%\n",
      "score_avg: 0\n",
      "269:: 69120 episode score is 0.69\n",
      "Expert: 95.31% | Learner: 9.77%\n",
      "score_avg: 0\n",
      "270:: 69376 episode score is 0.69\n",
      "Expert: 90.23% | Learner: 11.72%\n",
      "score_avg: 0\n",
      "271:: 69632 episode score is 0.69\n",
      "Expert: 89.45% | Learner: 14.84%\n",
      "score_avg: 0\n",
      "272:: 69888 episode score is 0.69\n",
      "Expert: 90.62% | Learner: 13.67%\n",
      "score_avg: 0\n",
      "273:: 70144 episode score is 0.69\n",
      "Expert: 93.36% | Learner: 10.16%\n",
      "score_avg: 0\n",
      "274:: 70400 episode score is 0.69\n",
      "Expert: 94.53% | Learner: 8.98%\n",
      "score_avg: 0\n",
      "275:: 70656 episode score is 0.69\n",
      "Expert: 92.19% | Learner: 10.55%\n",
      "score_avg: 0\n",
      "276:: 70912 episode score is 0.69\n",
      "Expert: 92.58% | Learner: 12.11%\n",
      "score_avg: 0\n",
      "277:: 71168 episode score is 0.69\n",
      "Expert: 91.41% | Learner: 14.06%\n",
      "score_avg: 0\n",
      "278:: 71424 episode score is 0.69\n",
      "Expert: 94.14% | Learner: 8.98%\n",
      "score_avg: 0\n",
      "279:: 71680 episode score is 0.69\n",
      "Expert: 95.31% | Learner: 7.03%\n",
      "score_avg: 0\n",
      "280:: 71936 episode score is 0.69\n",
      "Expert: 92.19% | Learner: 10.94%\n",
      "score_avg: 0\n",
      "281:: 72192 episode score is 0.69\n",
      "Expert: 94.14% | Learner: 8.20%\n",
      "score_avg: 0\n"
     ]
    }
   ],
   "source": [
    "# Now what we came for...\n",
    "\n",
    "for iter in range(max_iter_num):\n",
    "    actor.eval(), critic.eval()\n",
    "    memory = deque()\n",
    "\n",
    "    steps = 0\n",
    "    scores = []\n",
    "\n",
    "    while steps < total_sample_size: \n",
    "        state, expert_action, raw_state, raw_expert_action = env.reset()\n",
    "        state = state.to(device)\n",
    "        expert_action = expert_action.to(device)\n",
    "        score = 0\n",
    "            \n",
    "        #print(\"breakpt\")\n",
    "       # break\n",
    "        \n",
    "        \n",
    "        for _ in range(10000): \n",
    "            if render:\n",
    "                print(raw_state, raw_expert_action)\n",
    "            steps += 1\n",
    "\n",
    "            #TODO\n",
    "\n",
    "            mu, std = actor(state.resize(1,60,300))\n",
    "            action = get_action(mu.cpu(), std.cpu())[0]\n",
    "            done= env.step(action)\n",
    "            irl_reward = get_reward(discrim, state, action)\n",
    "            if done:\n",
    "                mask = 0\n",
    "            else:\n",
    "                mask = 1\n",
    "\n",
    "            memory.append([state, torch.from_numpy(action).to(device), irl_reward, mask,expert_action])\n",
    "\n",
    "            sys.exit\n",
    "\n",
    "            score += irl_reward\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        episodes += 1\n",
    "        scores.append(score)\n",
    "\n",
    "    score_avg = np.mean(scores)\n",
    "    print('{}:: {} episode score is {:.2f}'.format(iter, episodes, score_avg))\n",
    "\n",
    "    actor.train(), critic.train(), discrim.train()\n",
    "    if train_discrim_flag:\n",
    "        expert_acc, learner_acc = train_discrim(discrim, memory, discrim_optim, discrim_update_num, clip_param)\n",
    "        print(\"Expert: %.2f%% | Learner: %.2f%%\" % (expert_acc * 100, learner_acc * 100))\n",
    "        if expert_acc > suspend_accu_exp and learner_acc > suspend_accu_gen:\n",
    "            train_discrim_flag = False\n",
    "    train_actor_critic(actor, critic, memory, actor_optim, critic_optim, actor_critic_update_num, batch_size, clip_param)\n",
    "\n",
    "    if iter % 100:\n",
    "        score_avg = int(score_avg)\n",
    "\n",
    "        print(\"score_avg:\",score_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
