{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rough runthrough of GAIL for dialog generation, getting close..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque  \n",
    "#code for training \n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "from models import *\n",
    "from dialog_environment import *\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import torch\n",
    "from torch.distributions import Normal\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = DialogEnvironment()\n",
    "\n",
    "# Normally args but not here :-)\n",
    "seed = 0\n",
    "render = False\n",
    "gamma = 0.99\n",
    "lamda = .98\n",
    "\n",
    "train_discrim_flag = True\n",
    "learning_rate = 3e-4\n",
    "clip_param = .2\n",
    "discrim_update_num = 2\n",
    "actor_critic_update_num = 10\n",
    "l2_rate = 1e-3 # weight decay\n",
    "total_sample_size = 2048 # total num of state-actions to collect before learning\n",
    "batch_size = 256\n",
    "suspend_accu_exp = 1. # do not need to be this high typically, but seems likely it has to be for a simple env like mountain car cont.\n",
    "suspend_accu_gen = 1.\n",
    "max_iter_num = 1000\n",
    "\n",
    "\n",
    "actor = Actor(hidden_size=3,num_layers=3)\n",
    "critic = Critic(hidden_size=1,num_layers=3)\n",
    "discrim = Discriminator(input_size = 300, hidden_size=1,device='cuda',num_layers=3)\n",
    "actor.to(device), critic.to(device), discrim.to(device)\n",
    "actor_optim = optim.Adam(actor.parameters(), lr=learning_rate)\n",
    "critic_optim = optim.Adam(critic.parameters(), lr=learning_rate, \n",
    "                          weight_decay=l2_rate) \n",
    "discrim_optim = optim.Adam(discrim.parameters(), lr=learning_rate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def subsample(data, target, n=15):\n",
    "    return [x[::n] for x in data], [y[::n] for y in target]\n",
    "\n",
    "\n",
    "def get_action(mu, std):\n",
    "    action = torch.normal(mu, std)\n",
    "    action = action.data.numpy()\n",
    "    return action\n",
    "\n",
    "\n",
    "def get_entropy(mu, std):\n",
    "    dist = Normal(mu, std)\n",
    "    entropy = dist.entropy().mean()\n",
    "    return entropy\n",
    "\n",
    "def log_prob_density(x, mu, std):\n",
    "    log_prob_density = -(x - mu).pow(2) / (2 * std.pow(2)) \\\n",
    "                     - 0.5 * math.log(2 * math.pi)\n",
    "    return log_prob_density.sum(1, keepdim=True)\n",
    "\n",
    "def get_reward(discrim, state, action):\n",
    "    \"\"\"\n",
    "    The reward function according to irl. It's log D(s,a). \n",
    "    \n",
    "    Reward is higher the closer this is to 0, because the more similar it is to an expert action. :\n",
    "    Is quite close to imitation learning, but hope here is that with such a large number of expert demonstrations and entropy bonuses etc. it learns more than direct imitation. \n",
    "    \"\"\"\n",
    "\n",
    "    action = torch.Tensor(action).to(device)# turn state into a tensor if not already\n",
    "\n",
    "    with torch.no_grad():\n",
    "        return -math.log(discrim(state.resize(1,60,300),action.resize(1,60,300))[0].item())\n",
    "\n",
    "def save_checkpoint(state, filename):\n",
    "    return\n",
    "\n",
    "\n",
    "def train_discrim(discrim, memory, discrim_optim, discrim_update_num, clip_param):\n",
    "    \"\"\"\n",
    "    Training the discriminator. \n",
    "\n",
    "    Use binary cross entropy to classify whether \n",
    "    or not a sequence was predicted by the expert (real data) or actor. \n",
    "    \"\"\"\n",
    "    states = torch.stack([memory[i][0] for i in range(len(memory))])\n",
    "    actions = torch.stack([memory[i][1] for i in range(len(memory))])\n",
    "    rewards = [memory[i][2] for i in range(len(memory))]\n",
    "\n",
    "    masks = [memory[i][2] for i in range(len(memory))]\n",
    "    expert_actions = torch.stack([memory[i][4] for i in range(len(memory))])\n",
    "\n",
    "    criterion = torch.nn.BCELoss() # classify\n",
    "\n",
    "    for _ in range(discrim_update_num):\n",
    "\n",
    "        learner = discrim(states, actions) #pass (s,a) through discriminator\n",
    "\n",
    "        # TODO\n",
    "       # demonstrations = torch.Tensor([states, expert_actions]) # pass (s,a) of expert through discriminator\n",
    "        expert = discrim(states,expert_actions) #discrimator \"guesses\" whether or not these \n",
    "        # actions came from expert or learner\n",
    "        discrim_loss = criterion(learner, torch.ones((states.shape[0], 1)).to(device)) + \\\n",
    "                        criterion(expert, torch.zeros((states.shape[0], 1)).to(device))\n",
    "                # discrim loss: predict agent is all wrong, get as close to 0, and predict expert is 1, getting as close to 1 as possible. \n",
    "        discrim_optim.zero_grad() # gan loss, it tries to always get it right. \n",
    "        discrim_loss.backward()\n",
    "        discrim_optim.step()\n",
    "            # take these steps, do it however many times specified. \n",
    "        #return discrim(states,expert_actions) , discrim(states,actions)\n",
    "    expert_acc = ((discrim(states,expert_actions) < 0.5).float()).mean() #how often it realized the fake examples were fake\n",
    "    learner_acc = ((discrim(states,actions) > 0.5).float()).mean() #how often if predicted expert correctly. \n",
    "\n",
    "    return expert_acc, learner_acc # accuracy, it's the same kind, but because imbalanced better to look at separately. \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_actor_critic(actor, critic, memory, actor_optim, critic_optim, actor_critic_update_num, batch_size, clip_param):\n",
    "    \"\"\"\n",
    "    Take a PPO step or two to improve the actor critic model,  using GAE to estimate returns. \n",
    "    \n",
    "    In our case each trajectory it most one step, so the value function will have to do. \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    # tuple of a regular old RL problem, but now reward is what the discriminator says. \n",
    "    states = torch.stack([memory[i][0] for i in range(len(memory))])\n",
    "    actions = torch.stack([memory[i][1] for i in range(len(memory))])\n",
    "    rewards = [memory[i][2] for i in range(len(memory))]\n",
    "    masks = [memory[i][2] for i in range(len(memory))]\n",
    "    # compute value of what happened, see if what we can get us better. \n",
    "    old_values = critic(states)\n",
    "\n",
    "    #GAE aka estimate of Value + actual return roughtly \n",
    "    returns, advants = get_gae(rewards, masks, old_values, gamma, lamda)\n",
    "    \n",
    "    # pass states through actor, get corresponding actions\n",
    "    mu, std = actor(states)\n",
    "    # new mus and stds? \n",
    "    old_policy = log_prob_density(actions, mu, std) # sum of log probability\n",
    "    # of old actions\n",
    "\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    n = len(states)\n",
    "    arr = np.arange(n)\n",
    "\n",
    "    for _ in range(actor_critic_update_num):\n",
    "        np.random.shuffle(arr)\n",
    "\n",
    "        for i in range(n // batch_size): \n",
    "            batch_index = arr[batch_size * i : batch_size * (i + 1)]\n",
    "            #batch_index = torch.LongTensor(batch_index)\n",
    "            \n",
    "            inputs = states[batch_index]\n",
    "            actions_samples = actions[batch_index]\n",
    "            returns_samples = returns.unsqueeze(1)[batch_index].to(device)\n",
    "            advants_samples = advants.unsqueeze(1)[batch_index].to(device)\n",
    "            oldvalue_samples = old_values[batch_index].detach()\n",
    "        \n",
    "        \n",
    "            values = critic(inputs) #\n",
    "            clipped_values = oldvalue_samples + \\\n",
    "                             torch.clamp(values - oldvalue_samples,\n",
    "                                         -clip_param, \n",
    "                                         clip_param)\n",
    "            critic_loss1 = criterion(clipped_values, returns_samples)\n",
    "            critic_loss2 = criterion(values, returns_samples)\n",
    "            critic_loss = torch.max(critic_loss1, critic_loss2).mean()\n",
    "\n",
    "            loss, ratio, entropy = surrogate_loss(actor, advants_samples, inputs,\n",
    "                                         old_policy.detach(), actions_samples,\n",
    "                                         batch_index)\n",
    "            clipped_ratio = torch.clamp(ratio,\n",
    "                                        1.0 - clip_param,\n",
    "                                        1.0 + clip_param)\n",
    "            clipped_loss = clipped_ratio * advants_samples\n",
    "            actor_loss = -torch.min(loss, clipped_loss).mean()\n",
    "            #print(actor_loss,critic_loss,entropy)\n",
    "           # return actor_loss, critic_loss, entropy\n",
    "            loss = actor_loss + 0.5 * critic_loss - 0.001 * entropy #entropy bonus to promote exploration.\n",
    "\n",
    "            actor_optim.zero_grad()\n",
    "            loss.backward()\n",
    "            actor_optim.step()\n",
    "\n",
    "           # critic_optim.zero_grad()\n",
    "           # loss.backward() \n",
    "            critic_optim.step()\n",
    "\n",
    "def get_gae(rewards, masks, values, gamma, lamda):\n",
    "    \"\"\"\n",
    "    How much better a particular action is in a particular state. \n",
    "    \n",
    "    Uses reward of current action + value function of that state-action pair, discount factor gamma, and then lamda to compute. \n",
    "    \"\"\"\n",
    "    rewards = torch.Tensor(rewards)\n",
    "    masks = torch.Tensor(masks)\n",
    "    returns = torch.zeros_like(rewards)\n",
    "    advants = torch.zeros_like(rewards)\n",
    "    \n",
    "    running_returns = 0\n",
    "    previous_value = 0\n",
    "    running_advants = 0\n",
    "\n",
    "    for t in reversed(range(0, len(rewards))): #for LL, only ever one step :-)\n",
    "        running_returns = rewards[t] + (gamma * running_returns * masks[t])\n",
    "        returns[t] = running_returns\n",
    "\n",
    "        running_delta = rewards[t] + (gamma * previous_value * masks[t]) - \\\n",
    "                                        values.data[t]\n",
    "        previous_value = values.data[t]\n",
    "        \n",
    "        running_advants = running_delta + (gamma * lamda * \\\n",
    "                                            running_advants * masks[t])\n",
    "        advants[t] = running_advants\n",
    "\n",
    "    advants = (advants - advants.mean()) / advants.std()\n",
    "    return returns, advants\n",
    "\n",
    "def surrogate_loss(actor, advants, states, old_policy, actions, batch_index):\n",
    "    \"\"\"\n",
    "    The loss for PPO. Re-run through network, recomput policy from states\n",
    "    and see if this surrogate ratio is better. If it is, use as proximal policy update. It's very close to prior policy, but def better. \n",
    "    \n",
    "    Not sure this actually works though. Should not the new mu and stds be used to draw,\n",
    "    \n",
    "        When do we use get_action? Only once in main, I think it should be for all? \n",
    "    \"\"\"\n",
    "    mu, std = actor(states)\n",
    "    new_policy = log_prob_density(actions, mu, std)\n",
    "    old_policy = old_policy[batch_index]\n",
    "\n",
    "    ratio = torch.exp(new_policy - old_policy)\n",
    "    surrogate_loss = ratio * advants\n",
    "    entropy = get_entropy(mu, std)\n",
    "\n",
    "    return surrogate_loss, ratio, entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now what we came for...\n",
    "episodes = 0\n",
    "for iter in range(max_iter_num):\n",
    "    actor.eval(), critic.eval()\n",
    "    memory = deque()\n",
    "\n",
    "    steps = 0\n",
    "    scores = []\n",
    "\n",
    "    while steps < total_sample_size: \n",
    "        state, expert_action, raw_state, raw_expert_action = env.reset()\n",
    "        state = state.to(device)\n",
    "        expert_action = expert_action.to(device)\n",
    "        score = 0\n",
    "\n",
    "        \n",
    "        for _ in range(1000): \n",
    "            if render:\n",
    "                print(raw_state, raw_expert_action)\n",
    "            steps += 1\n",
    "\n",
    "            #TODO\n",
    "\n",
    "            mu, std = actor(state.resize(1,60,300))\n",
    "            action = get_action(mu.cpu(), std.cpu())[0]\n",
    "            done= env.step(action)\n",
    "            irl_reward = get_reward(discrim, state, action)\n",
    "            if done:\n",
    "                mask = 0\n",
    "            else:\n",
    "                mask = 1\n",
    "\n",
    "            memory.append([state, torch.from_numpy(action).to(device), irl_reward, mask,expert_action])\n",
    "\n",
    "            score += irl_reward\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        episodes += 1\n",
    "        scores.append(score)\n",
    "\n",
    "    score_avg = np.mean(scores)\n",
    "    print('{}:: {} episode score is {:.2f}'.format(iter, episodes, score_avg))\n",
    "\n",
    "    actor.train(), critic.train(), discrim.train()\n",
    "    if train_discrim_flag:\n",
    "        expert_acc, learner_acc = train_discrim(discrim, memory, discrim_optim, discrim_update_num, clip_param)\n",
    "        print(\"Expert: %.2f%% | Learner: %.2f%%\" % (expert_acc * 100, learner_acc * 100))\n",
    "        if expert_acc > suspend_accu_exp and learner_acc > suspend_accu_gen:\n",
    "            train_discrim_flag = False\n",
    "    train_actor_critic(actor, critic, memory, actor_optim, critic_optim, actor_critic_update_num, batch_size, clip_param)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_action = expert_action.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity = 0\n",
    "for i in range(60):\n",
    "    similarity += np.dot(action[i],expert_action.numpy()[i])/(np.linalg.norm(action[i])*np.linalg.norm(expert_action[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " np.dot(action[i],expert_action.numpy()[i])/(np.linalg.norm(action[i])*np.linalg.norm(expert_action[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action[0].mean(), action[0].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_action[0].mean(), expert_action[0].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
