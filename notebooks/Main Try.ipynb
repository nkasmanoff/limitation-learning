{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append('../src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Main.py file for GAIL implementation on dialog datasets.\n",
    "\n",
    "Uses command line arguments to maximize flexibility, and run many options in parallel\n",
    "\n",
    "\"\"\"\n",
    "import os\n",
    "import pickle\n",
    "import argparse\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from tensorboardX import SummaryWriter \n",
    "\n",
    "from models.actor import Actor\n",
    "from models.critic import Critic\n",
    "from models.discriminator import Discriminator\n",
    "from GAIL import *\n",
    "\n",
    "from dialog_environment import DialogEnvironment\n",
    "\n",
    "device='cuda' # for now\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description='PyTorch GAIL for Dialog')\n",
    "\n",
    "parser.add_argument('--load_model', \n",
    "                    type=str, default=None, \n",
    "                    help='path to load the saved model')\n",
    "\n",
    "parser.add_argument('--render', \n",
    "                    action=\"store_true\", default=False, \n",
    "                    help='if you dont want to render, set this to False')\n",
    "\n",
    "parser.add_argument('--gamma', \n",
    "                    type=float, default=0.99, \n",
    "                    help='discounted factor (default: 0.99)')\n",
    "\n",
    "parser.add_argument('--lamda', \n",
    "                    type=float, default=0.98, \n",
    "                    help='GAE hyper-parameter (default: 0.98)')\n",
    "\n",
    "\n",
    "parser.add_argument('--learning_rate', \n",
    "                    type=float, default=3e-4, \n",
    "                    help='learning rate of models (default: 3e-4)')\n",
    "\n",
    "parser.add_argument('--l2_rate', \n",
    "                    type=float, default=1e-3, \n",
    "                    help='l2 regularizer coefficient (default: 1e-3)')\n",
    "\n",
    "parser.add_argument('--clip_param', \n",
    "                    type=float, default=0.2, \n",
    "                    help='clipping parameter for PPO (default: 0.2)')\n",
    "\n",
    "parser.add_argument('--discrim_update_num', \n",
    "                    type=int, default=2, \n",
    "                    help='update number of discriminator (default: 2)')\n",
    "\n",
    "parser.add_argument('--actor_critic_update_num', \n",
    "                    type=int, default=10, \n",
    "                    help='update number of actor-critic (default: 10)')\n",
    "\n",
    "parser.add_argument('--total_sample_size', \n",
    "                    type=int, default=2048, \n",
    "                    help='total sample size to collect before PPO update (default: 2048)')\n",
    "\n",
    "parser.add_argument('--batch_size', \n",
    "                    type=int, default=128, \n",
    "                    help='batch size to update (default: 128)')\n",
    "\n",
    "parser.add_argument('--suspend_accu_exp', \n",
    "                    type=float, default=0.8,\n",
    "                    help='accuracy for suspending discriminator about expert data (default: 0.8)')\n",
    "\n",
    "parser.add_argument('--suspend_accu_gen', \n",
    "                    type=float, default=0.8,\n",
    "                    help='accuracy for suspending discriminator about generated data (default: 0.8)')\n",
    "\n",
    "parser.add_argument('--max_iter_num', \n",
    "                    type=int, default=4000,\n",
    "                    help='maximal number of main iterations (default: 4000)')\n",
    "\n",
    "parser.add_argument('--seed', \n",
    "                    type=int, default=500,\n",
    "                    help='random seed (default: 500)')\n",
    "\n",
    "parser.add_argument('--logdir', \n",
    "                    type=str, default='logs/EXPERIMENTNAME',\n",
    "                    help='tensorboardx logs directory (default: logs/EXPERIMENTNAME)')\n",
    "\n",
    "parser.add_argument('--hidden_size', \n",
    "                    type=int, default=8,\n",
    "                    help='New sequence length of the representation produced by the encoder/decoder RNNs. (default: 8)')\n",
    "parser.add_argument('--num_layers', \n",
    "                    type=int, default=4,\n",
    "                    help='Number of layers in the respective RNNs (default: 4)')\n",
    "\n",
    "parser.add_argument('--seq_len', \n",
    "                    type=int, default=60,\n",
    "                    help='length of input and response sequences (default: 60, which is also max)')\n",
    "parser.add_argument('--input_size', \n",
    "                    type=int, default=300,\n",
    "                    help='DO NOT CHANGE UNLESS NEW EMBEDDINGS ARE MADE. Dimensionality of embeddings (default: 300)')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    env = DialogEnvironment()\n",
    "\n",
    "    torch.manual_seed(args.seed)\n",
    "\n",
    "    #TODO\n",
    "    actor = Actor(hidden_size=args.hidden_size,num_layers=args.num_layers,device='cuda',input_size=args.input_size,output_size=args.input_size//2)\n",
    "    critic = Critic(hidden_size=args.hidden_size,num_layers=args.num_layers,input_size=args.input_size,seq_len=args.seq_len)\n",
    "    discrim = Discriminator(hidden_size=args.hidden_size,num_layers=args.hidden_size,input_size=args.input_size,seq_len=args.seq_len)\n",
    "    \n",
    "    actor.to(device), critic.to(device), discrim.to(device)\n",
    "    \n",
    "\n",
    "    actor_optim = optim.Adam(actor.parameters(), lr=args.learning_rate)\n",
    "    critic_optim = optim.Adam(critic.parameters(), lr=args.learning_rate, \n",
    "                              weight_decay=args.l2_rate) \n",
    "    discrim_optim = optim.Adam(discrim.parameters(), lr=args.learning_rate)\n",
    "\n",
    "    # load demonstrations\n",
    "\n",
    "    writer = SummaryWriter(args.logdir)\n",
    "\n",
    "    if args.load_model is not None: #TODO\n",
    "        saved_ckpt_path = os.path.join(os.getcwd(), 'save_model', str(args.load_model))\n",
    "        ckpt = torch.load(saved_ckpt_path)\n",
    "\n",
    "        actor.load_state_dict(ckpt['actor'])\n",
    "        critic.load_state_dict(ckpt['critic'])\n",
    "        discrim.load_state_dict(ckpt['discrim'])\n",
    "\n",
    "\n",
    "    \n",
    "    episodes = 0\n",
    "    train_discrim_flag = True\n",
    "\n",
    "    for iter in range(args.max_iter_num):\n",
    "        actor.eval(), critic.eval()\n",
    "        memory = deque()\n",
    "\n",
    "        steps = 0\n",
    "        scores = []\n",
    "\n",
    "        while steps < args.total_sample_size: \n",
    "            state, expert_action, raw_state, raw_expert_action = env.reset()\n",
    "            score = 0\n",
    "\n",
    "            state = state[:args.seq_len,:]\n",
    "            expert_action = expert_action[:args.seq_len,:]\n",
    "            state = state.to(device)\n",
    "            expert_action = expert_action.to(device)\n",
    "            for _ in range(10000): \n",
    "\n",
    "                steps += 1\n",
    "\n",
    "                mu, std = actor(state.resize(1,args.seq_len,args.input_size)) #TODO: gotta be a better way to resize. \n",
    "                action = get_action(mu.cpu(), std.cpu())[0]\n",
    "                done= env.step(action)\n",
    "                irl_reward = get_reward(discrim, state, action, args)\n",
    "                if done:\n",
    "                    mask = 0\n",
    "                else:\n",
    "                    mask = 1\n",
    "\n",
    "\n",
    "                memory.append([state, torch.from_numpy(action).to(device), irl_reward, mask,expert_action])\n",
    "\n",
    "                score += irl_reward\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            \n",
    "            episodes += 1\n",
    "            scores.append(score)\n",
    "        \n",
    "        score_avg = np.mean(scores)\n",
    "        print('{}:: {} episode score is {:.2f}'.format(iter, episodes, score_avg))\n",
    "\n",
    "        actor.train(), critic.train(), discrim.train()\n",
    "        if train_discrim_flag:\n",
    "            expert_acc, learner_acc = train_discrim(discrim, memory, discrim_optim, args) \n",
    "            print(\"Expert: %.2f%% | Learner: %.2f%%\" % (expert_acc * 100, learner_acc * 100))\n",
    "            writer.add_scalar('log/expert_acc', float(expert_acc), iter) #logg\n",
    "            writer.add_scalar('log/learner_acc', float(learner_acc), iter) #logg\n",
    "            writer.add_scalar('log/avg_acc', float(learner_acc + expert_acc)/2, iter) #logg\n",
    "\n",
    "            if expert_acc > args.suspend_accu_exp and learner_acc > args.suspend_accu_gen:\n",
    "                train_discrim_flag = False\n",
    "                \n",
    "        train_actor_critic(actor, critic, memory, actor_optim, critic_optim, args)\n",
    "        writer.add_scalar('log/score', float(score_avg), iter)\n",
    "\n",
    "        writer.add_text('log/raw_state', raw_state[0],iter)\n",
    "        raw_action = get_raw_action(action) #TODO\n",
    "        writer.add_text('log/raw_action', raw_action,iter)\n",
    "        writer.add_text('log/raw_expert_action', raw_expert_action,iter)\n",
    "\n",
    "        if iter % 100:\n",
    "            score_avg = int(score_avg)\n",
    "\n",
    "\n",
    "            model_path = os.path.join(os.getcwd(),'save_model')\n",
    "            if not os.path.isdir(model_path):\n",
    "                os.makedirs(model_path)\n",
    "\n",
    "            ckpt_path = os.path.join(model_path, 'ckpt_'+ str(score_avg)+'.pth.tar')\n",
    "\n",
    "            save_checkpoint({\n",
    "                'actor': actor.state_dict(),\n",
    "                'critic': critic.state_dict(),\n",
    "                'discrim': discrim.state_dict(),\n",
    "                'args': args,\n",
    "                'score': score_avg,\n",
    "            }, filename=ckpt_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from argparse import Namespace\n",
    "args = Namespace(load_model=None,\n",
    "                render=False,\n",
    "                gamma=.99,\n",
    "                lamda=.98,\n",
    "                learning_rate=3e-4,\n",
    "                l2_rate=1e-3,\n",
    "                clip_param=.2,\n",
    "                discrim_update_num=10,\n",
    "                actor_critic_update_num=20,\n",
    "                total_sample_size=2048,\n",
    "                batch_size=128,\n",
    "                suspend_accu_exp=.99,\n",
    "                suspend_accu_gen=.99,\n",
    "                max_iter_num=4000,\n",
    "                seed=500,\n",
    "                logdir='logs/noah',\n",
    "                 hidden_size=8,\n",
    "                 num_layers=4,\n",
    "                 seq_len=5,\n",
    "                 input_size=300\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/nsk367/anaconda3/envs/irl/lib/python3.8/site-packages/torch/tensor.py:447: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:: 2048 episode score is 0.67\n",
      "Expert: 100.00% | Learner: 0.00%\n",
      "1:: 4096 episode score is 0.70\n",
      "Expert: 100.00% | Learner: 0.00%\n",
      "2:: 6144 episode score is 0.70\n",
      "Expert: 0.00% | Learner: 100.00%\n",
      "3:: 8192 episode score is 0.69\n",
      "Expert: 0.00% | Learner: 100.00%\n",
      "4:: 10240 episode score is 0.69\n",
      "Expert: 100.00% | Learner: 0.00%\n",
      "5:: 12288 episode score is 0.69\n",
      "Expert: 0.00% | Learner: 100.00%\n",
      "6:: 14336 episode score is 0.69\n",
      "Expert: 100.00% | Learner: 0.68%\n",
      "7:: 16384 episode score is 0.69\n",
      "Expert: 100.00% | Learner: 40.72%\n",
      "8:: 18432 episode score is 0.69\n",
      "Expert: 99.22% | Learner: 98.58%\n",
      "9:: 20480 episode score is 0.69\n",
      "Expert: 98.78% | Learner: 99.80%\n",
      "10:: 22528 episode score is 0.67\n",
      "Expert: 99.32% | Learner: 100.00%\n",
      "11:: 24576 episode score is 0.63\n",
      "12:: 26624 episode score is 0.63\n",
      "13:: 28672 episode score is 0.63\n",
      "14:: 30720 episode score is 0.63\n",
      "15:: 32768 episode score is 0.63\n",
      "16:: 34816 episode score is 0.63\n",
      "17:: 36864 episode score is 0.64\n",
      "18:: 38912 episode score is 0.63\n",
      "19:: 40960 episode score is 0.64\n",
      "20:: 43008 episode score is 0.64\n",
      "21:: 45056 episode score is 0.64\n",
      "22:: 47104 episode score is 0.64\n",
      "23:: 49152 episode score is 0.65\n",
      "24:: 51200 episode score is 0.65\n",
      "25:: 53248 episode score is 0.65\n",
      "26:: 55296 episode score is 0.65\n",
      "27:: 57344 episode score is 0.65\n",
      "28:: 59392 episode score is 0.66\n",
      "29:: 61440 episode score is 0.66\n",
      "30:: 63488 episode score is 0.66\n",
      "31:: 65536 episode score is 0.66\n",
      "32:: 67584 episode score is 0.67\n",
      "33:: 69632 episode score is 0.67\n",
      "34:: 71680 episode score is 0.67\n",
      "35:: 73728 episode score is 0.67\n",
      "36:: 75776 episode score is 0.68\n",
      "37:: 77824 episode score is 0.68\n",
      "38:: 79872 episode score is 0.69\n",
      "39:: 81920 episode score is 0.69\n",
      "40:: 83968 episode score is 0.69\n",
      "41:: 86016 episode score is 0.70\n",
      "42:: 88064 episode score is 0.70\n",
      "43:: 90112 episode score is 0.71\n",
      "44:: 92160 episode score is 0.71\n",
      "45:: 94208 episode score is 0.71\n",
      "46:: 96256 episode score is 0.71\n",
      "47:: 98304 episode score is 0.72\n",
      "48:: 100352 episode score is 0.72\n",
      "49:: 102400 episode score is 0.72\n",
      "50:: 104448 episode score is 0.72\n",
      "51:: 106496 episode score is 0.73\n",
      "52:: 108544 episode score is 0.73\n",
      "53:: 110592 episode score is 0.73\n",
      "54:: 112640 episode score is 0.74\n",
      "55:: 114688 episode score is 0.74\n",
      "56:: 116736 episode score is 0.74\n",
      "57:: 118784 episode score is 0.74\n",
      "58:: 120832 episode score is 0.74\n",
      "59:: 122880 episode score is 0.74\n",
      "60:: 124928 episode score is 0.75\n",
      "61:: 126976 episode score is 0.75\n",
      "62:: 129024 episode score is 0.75\n",
      "63:: 131072 episode score is 0.75\n",
      "64:: 133120 episode score is 0.75\n",
      "65:: 135168 episode score is 0.76\n",
      "66:: 137216 episode score is 0.76\n",
      "67:: 139264 episode score is 0.76\n",
      "68:: 141312 episode score is 0.76\n",
      "69:: 143360 episode score is 0.76\n",
      "70:: 145408 episode score is 0.76\n",
      "71:: 147456 episode score is 0.76\n",
      "72:: 149504 episode score is 0.76\n",
      "73:: 151552 episode score is 0.77\n",
      "74:: 153600 episode score is 0.77\n",
      "75:: 155648 episode score is 0.77\n",
      "76:: 157696 episode score is 0.77\n",
      "77:: 159744 episode score is 0.77\n",
      "78:: 161792 episode score is 0.77\n",
      "79:: 163840 episode score is 0.77\n",
      "80:: 165888 episode score is 0.78\n",
      "81:: 167936 episode score is 0.78\n",
      "82:: 169984 episode score is 0.78\n",
      "83:: 172032 episode score is 0.78\n",
      "84:: 174080 episode score is 0.78\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
