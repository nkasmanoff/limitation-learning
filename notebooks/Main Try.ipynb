{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append('../src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Main.py file for GAIL implementation on dialog datasets.\n",
    "\n",
    "Uses command line arguments to maximize flexibility, and run many options in parallel\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import sys \n",
    "sys.path.append('../src')\n",
    "import os\n",
    "import pickle\n",
    "import argparse\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from tensorboardX import SummaryWriter \n",
    "\n",
    "from models.actor import Actor\n",
    "from models.critic import Critic\n",
    "from models.discriminator import Discriminator\n",
    "from GAIL import *\n",
    "\n",
    "from dialog_environment import DialogEnvironment\n",
    "\n",
    "device='cuda' # for now\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Limitation Learning')\n",
    "\n",
    "parser.add_argument('--load_model', \n",
    "                    type=str, default=None, \n",
    "                    help='path to load the saved model')\n",
    "\n",
    "parser.add_argument('--gamma', \n",
    "                    type=float, default=0.99, \n",
    "                    help='discounted factor (default: 0.99)')\n",
    "\n",
    "parser.add_argument('--lamda', \n",
    "                    type=float, default=0.98, \n",
    "                    help='GAE hyper-parameter (default: 0.98)')\n",
    "\n",
    "\n",
    "parser.add_argument('--learning_rate', \n",
    "                    type=float, default=3e-4, \n",
    "                    help='learning rate of models (default: 3e-4)')\n",
    "\n",
    "parser.add_argument('--l2_rate', \n",
    "                    type=float, default=1e-3, \n",
    "                    help='l2 regularizer coefficient (default: 1e-3)')\n",
    "\n",
    "parser.add_argument('--clip_param', \n",
    "                    type=float, default=0.2, \n",
    "                    help='clipping parameter for PPO (default: 0.2)')\n",
    "\n",
    "parser.add_argument('--discrim_update_num', \n",
    "                    type=int, default=2, \n",
    "                    help='update number of discriminator (default: 2)')\n",
    "\n",
    "parser.add_argument('--actor_critic_update_num', \n",
    "                    type=int, default=10, \n",
    "                    help='update number of actor-critic (default: 10)')\n",
    "\n",
    "parser.add_argument('--total_sample_size', \n",
    "                    type=int, default=2048, \n",
    "                    help='total sample size to collect before PPO update (default: 2048)')\n",
    "\n",
    "parser.add_argument('--batch_size', \n",
    "                    type=int, default=128, \n",
    "                    help='batch size to update (default: 128)')\n",
    "\n",
    "parser.add_argument('--suspend_accu_exp', \n",
    "                    type=float, default=None,\n",
    "                    help='accuracy for suspending discriminator about expert data (default: None)')\n",
    "\n",
    "parser.add_argument('--suspend_accu_gen', \n",
    "                    type=float, default=None,\n",
    "                    help='accuracy for suspending discriminator about generated data (default: None)')\n",
    "\n",
    "parser.add_argument('--max_iter_num', \n",
    "                    type=int, default=4096,\n",
    "                    help='maximal number of main iterations (default: 4000)')\n",
    "\n",
    "parser.add_argument('--seed', \n",
    "                    type=int, default=42,\n",
    "                    help='random seed (default: 500)')\n",
    "\n",
    "parser.add_argument('--logdir', \n",
    "                    type=str, default='logs/EXPERIMENTNAME',\n",
    "                    help='tensorboardx logs directory (default: logs/EXPERIMENTNAME)')\n",
    "\n",
    "parser.add_argument('--hidden_size', \n",
    "                    type=int, default=128,\n",
    "                    help='New sequence length of the representation produced by the encoder/decoder RNNs. (default: 1024)')\n",
    "parser.add_argument('--num_layers', \n",
    "                    type=int, default=2,\n",
    "                    help='Number of layers in the respective RNNs (default: 2)')\n",
    "\n",
    "parser.add_argument('--seq_len', \n",
    "                    type=int, default=10,\n",
    "                    help='length of input and response sequences (default: 60, which is also max)')\n",
    "parser.add_argument('--input_size', \n",
    "                    type=int, default=300,\n",
    "                    help='DO NOT CHANGE UNLESS NEW EMBEDDINGS ARE MADE. Dimensionality of embeddings (default: 300)')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    env = DialogEnvironment()\n",
    "\n",
    "    torch.manual_seed(args.seed)\n",
    "\n",
    "    #TODO\n",
    "    actor = Actor(hidden_size=args.hidden_size,num_layers=args.num_layers,device='cuda',input_size=args.input_size,output_size=args.input_size//2)\n",
    "    critic = Critic(hidden_size=args.hidden_size,num_layers=args.num_layers,input_size=args.input_size,seq_len=args.seq_len)\n",
    "    discrim = Discriminator(hidden_size=args.hidden_size,num_layers=args.hidden_size,input_size=args.input_size,seq_len=args.seq_len)\n",
    "    \n",
    "    actor.to(device), critic.to(device), discrim.to(device)\n",
    "    \n",
    "    actor_optim = optim.Adam(actor.parameters(), lr=args.learning_rate)\n",
    "    critic_optim = optim.Adam(critic.parameters(), lr=args.learning_rate, \n",
    "                              weight_decay=args.l2_rate) \n",
    "    discrim_optim = optim.Adam(discrim.parameters(), lr=args.learning_rate)\n",
    "\n",
    "    # load demonstrations\n",
    "\n",
    "    writer = SummaryWriter(args.logdir)\n",
    "\n",
    "    if args.load_model is not None: #TODO\n",
    "        saved_ckpt_path = os.path.join(os.getcwd(), 'save_model', str(args.load_model))\n",
    "        ckpt = torch.load(saved_ckpt_path)\n",
    "\n",
    "        actor.load_state_dict(ckpt['actor'])\n",
    "        critic.load_state_dict(ckpt['critic'])\n",
    "        discrim.load_state_dict(ckpt['discrim'])\n",
    "\n",
    "\n",
    "    \n",
    "    episodes = 0\n",
    "    train_discrim_flag = True\n",
    "\n",
    "    for iter in range(args.max_iter_num):\n",
    "        actor.eval(), critic.eval()\n",
    "        memory = deque()\n",
    "\n",
    "        steps = 0\n",
    "        scores = []\n",
    "        similarity_scores = []\n",
    "        while steps < args.total_sample_size: \n",
    "            state, expert_action, raw_state, raw_expert_action = env.reset()\n",
    "            score = 0\n",
    "            similarity_score = 0\n",
    "            state = state[:args.seq_len,:]\n",
    "            expert_action = expert_action[:args.seq_len,:]\n",
    "            state = state.to(device)\n",
    "            expert_action = expert_action.to(device)\n",
    "            for _ in range(10000): \n",
    "\n",
    "                steps += 1\n",
    "\n",
    "                mu, std = actor(state.resize(1,args.seq_len,args.input_size)) #TODO: gotta be a better way to resize. \n",
    "                action = get_action(mu.cpu(), std.cpu())[0]\n",
    "                done= env.step(action)\n",
    "                irl_reward = get_reward(discrim, state, action, args)\n",
    "                if done:\n",
    "                    mask = 0\n",
    "                else:\n",
    "                    mask = 1\n",
    "\n",
    "\n",
    "                memory.append([state, torch.from_numpy(action).to(device), irl_reward, mask,expert_action])\n",
    "                score += irl_reward\n",
    "           #     similarity_score += get_cosine_sim(action,expert_action)\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            episodes += 1\n",
    "            scores.append(score)\n",
    "           # similarity_scores.append(similarity_score)\n",
    "\n",
    "        score_avg = np.mean(scores)\n",
    "      #  similarity_score_avg = np.mean(similarity_scores)\n",
    "        print('{}:: {} episode score is {:.2f}'.format(iter, episodes, score_avg))\n",
    "       # print('{}:: {} episode similarity score is {:.2f}'.format(iter, episodes, similarity_score_avg))\n",
    "\n",
    "        actor.train(), critic.train(), discrim.train()\n",
    "        if train_discrim_flag:\n",
    "            expert_acc, learner_acc = train_discrim(discrim, memory, discrim_optim, args) \n",
    "            print(\"Expert: %.2f%% | Learner: %.2f%%\" % (expert_acc * 100, learner_acc * 100))\n",
    "            writer.add_scalar('log/expert_acc', float(expert_acc), iter) #logg\n",
    "            writer.add_scalar('log/learner_acc', float(learner_acc), iter) #logg\n",
    "            writer.add_scalar('log/avg_acc', float(learner_acc + expert_acc)/2, iter) #logg\n",
    "            if args.suspend_accu_exp is not None: #only if not None do we check.\n",
    "                if expert_acc > args.suspend_accu_exp and learner_acc > args.suspend_accu_gen:\n",
    "                    train_discrim_flag = False\n",
    "                    \n",
    "        train_actor_critic(actor, critic, memory, actor_optim, critic_optim, args)\n",
    "        writer.add_scalar('log/score', float(score_avg), iter)\n",
    "     #   writer.add_scalar('log/similarity_score', float(similarity_score_avg), iter)\n",
    "        writer.add_text('log/raw_state', raw_state[0],iter)\n",
    "        raw_action = get_raw_action(action) #TODO\n",
    "        writer.add_text('log/raw_action', raw_action,iter)\n",
    "        writer.add_text('log/raw_expert_action', raw_expert_action,iter)\n",
    "\n",
    "        if iter % 100:\n",
    "            score_avg = int(score_avg)\n",
    "\n",
    "\n",
    "            model_path = os.path.join(os.getcwd(),'save_model')\n",
    "            if not os.path.isdir(model_path):\n",
    "                os.makedirs(model_path)\n",
    "\n",
    "            ckpt_path = os.path.join(model_path, 'ckpt_'+ str(score_avg)+'.pth.tar')\n",
    "\n",
    "            save_checkpoint({\n",
    "                'actor': actor.state_dict(),\n",
    "                'critic': critic.state_dict(),\n",
    "                'discrim': discrim.state_dict(),\n",
    "                'args': args,\n",
    "                'score': score_avg,\n",
    "            }, filename=ckpt_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from argparse import Namespace\n",
    "args = Namespace(load_model=None,\n",
    "                render=False,\n",
    "                gamma=.99,\n",
    "                lamda=.98,\n",
    "                learning_rate=1e-4,\n",
    "                l2_rate=1e-3,\n",
    "                clip_param=.2,\n",
    "                discrim_update_num=1,\n",
    "                actor_critic_update_num=100,\n",
    "                total_sample_size=100,\n",
    "                batch_size=100,\n",
    "                suspend_accu_exp=None,# won't stop\n",
    "                suspend_accu_gen=None,\n",
    "                max_iter_num=4000,\n",
    "                seed=500,\n",
    "                logdir='logs/noah7',\n",
    "                 hidden_size=1,\n",
    "                 num_layers=1,\n",
    "                 seq_len=10,\n",
    "                 input_size=300\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/nsk367/anaconda3/envs/irl/lib/python3.8/site-packages/torch/tensor.py:447: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:: 100 episode score is 0.76\n",
      "Expert: 100.00% | Learner: 0.00%\n",
      "1:: 200 episode score is 0.76\n",
      "Expert: 100.00% | Learner: 0.00%\n",
      "2:: 300 episode score is 0.76\n",
      "Expert: 100.00% | Learner: 0.00%\n",
      "3:: 400 episode score is 0.76\n",
      "Expert: 100.00% | Learner: 0.00%\n",
      "4:: 500 episode score is 0.76\n",
      "Expert: 100.00% | Learner: 0.00%\n",
      "5:: 600 episode score is 0.75\n",
      "Expert: 100.00% | Learner: 0.00%\n",
      "6:: 700 episode score is 0.75\n",
      "Expert: 100.00% | Learner: 0.00%\n",
      "7:: 800 episode score is 0.75\n",
      "Expert: 100.00% | Learner: 0.00%\n",
      "8:: 900 episode score is 0.75\n",
      "Expert: 100.00% | Learner: 0.00%\n",
      "9:: 1000 episode score is 0.75\n",
      "Expert: 100.00% | Learner: 0.00%\n",
      "10:: 1100 episode score is 0.75\n",
      "Expert: 100.00% | Learner: 0.00%\n",
      "11:: 1200 episode score is 0.75\n",
      "Expert: 100.00% | Learner: 0.00%\n",
      "12:: 1300 episode score is 0.75\n",
      "Expert: 100.00% | Learner: 0.00%\n",
      "13:: 1400 episode score is 0.75\n",
      "Expert: 100.00% | Learner: 0.00%\n",
      "14:: 1500 episode score is 0.75\n",
      "Expert: 100.00% | Learner: 0.00%\n",
      "15:: 1600 episode score is 0.74\n",
      "Expert: 100.00% | Learner: 0.00%\n",
      "16:: 1700 episode score is 0.74\n",
      "Expert: 100.00% | Learner: 0.00%\n",
      "17:: 1800 episode score is 0.74\n",
      "Expert: 100.00% | Learner: 0.00%\n",
      "18:: 1900 episode score is 0.74\n",
      "Expert: 100.00% | Learner: 0.00%\n",
      "19:: 2000 episode score is 0.74\n",
      "Expert: 100.00% | Learner: 0.00%\n",
      "20:: 2100 episode score is 0.74\n",
      "Expert: 100.00% | Learner: 0.00%\n",
      "21:: 2200 episode score is 0.74\n",
      "Expert: 100.00% | Learner: 0.00%\n",
      "22:: 2300 episode score is 0.74\n",
      "Expert: 100.00% | Learner: 0.00%\n",
      "23:: 2400 episode score is 0.74\n",
      "Expert: 100.00% | Learner: 0.00%\n",
      "24:: 2500 episode score is 0.73\n",
      "Expert: 100.00% | Learner: 0.00%\n",
      "25:: 2600 episode score is 0.73\n",
      "Expert: 100.00% | Learner: 0.00%\n",
      "26:: 2700 episode score is 0.73\n",
      "Expert: 100.00% | Learner: 0.00%\n",
      "27:: 2800 episode score is 0.73\n",
      "Expert: 100.00% | Learner: 0.00%\n",
      "28:: 2900 episode score is 0.73\n",
      "Expert: 100.00% | Learner: 0.00%\n",
      "29:: 3000 episode score is 0.73\n",
      "Expert: 100.00% | Learner: 0.00%\n",
      "30:: 3100 episode score is 0.73\n",
      "Expert: 100.00% | Learner: 0.00%\n",
      "31:: 3200 episode score is 0.72\n",
      "Expert: 100.00% | Learner: 0.00%\n",
      "32:: 3300 episode score is 0.72\n",
      "Expert: 100.00% | Learner: 0.00%\n",
      "33:: 3400 episode score is 0.72\n",
      "Expert: 100.00% | Learner: 0.00%\n",
      "34:: 3500 episode score is 0.72\n",
      "Expert: 100.00% | Learner: 0.00%\n",
      "35:: 3600 episode score is 0.72\n",
      "Expert: 100.00% | Learner: 0.00%\n",
      "36:: 3700 episode score is 0.72\n",
      "Expert: 100.00% | Learner: 0.00%\n",
      "37:: 3800 episode score is 0.72\n",
      "Expert: 100.00% | Learner: 1.00%\n",
      "38:: 3900 episode score is 0.71\n",
      "Expert: 100.00% | Learner: 2.00%\n",
      "39:: 4000 episode score is 0.72\n",
      "Expert: 100.00% | Learner: 0.00%\n",
      "40:: 4100 episode score is 0.71\n",
      "Expert: 100.00% | Learner: 0.00%\n",
      "41:: 4200 episode score is 0.71\n",
      "Expert: 100.00% | Learner: 4.00%\n",
      "42:: 4300 episode score is 0.71\n",
      "Expert: 100.00% | Learner: 1.00%\n",
      "43:: 4400 episode score is 0.71\n",
      "Expert: 100.00% | Learner: 5.00%\n",
      "44:: 4500 episode score is 0.71\n",
      "Expert: 100.00% | Learner: 4.00%\n",
      "45:: 4600 episode score is 0.71\n",
      "Expert: 100.00% | Learner: 13.00%\n",
      "46:: 4700 episode score is 0.71\n",
      "Expert: 100.00% | Learner: 11.00%\n",
      "47:: 4800 episode score is 0.70\n",
      "Expert: 100.00% | Learner: 22.00%\n",
      "48:: 4900 episode score is 0.70\n",
      "Expert: 100.00% | Learner: 19.00%\n",
      "49:: 5000 episode score is 0.70\n",
      "Expert: 100.00% | Learner: 40.00%\n",
      "50:: 5100 episode score is 0.70\n",
      "Expert: 100.00% | Learner: 46.00%\n",
      "51:: 5200 episode score is 0.70\n",
      "Expert: 100.00% | Learner: 40.00%\n",
      "52:: 5300 episode score is 0.70\n",
      "Expert: 100.00% | Learner: 45.00%\n",
      "53:: 5400 episode score is 0.70\n",
      "Expert: 100.00% | Learner: 56.00%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-90f009c99aed>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0msimilarity_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0msteps\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_sample_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m             \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpert_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_expert_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m             \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m             \u001b[0msimilarity_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/nsk367/deepRL/limitation-learning/src/dialog_environment.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mexpert_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconversation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mraw_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_conversations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mraw_expert_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_conversations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_conversations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The action space is too obvious. Need to constrain to same, and retry. Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!python ../src/main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
