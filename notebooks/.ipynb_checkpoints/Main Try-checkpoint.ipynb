{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "\n",
    "sys.path.append('../src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Main.py file for GAIL implementation on dialog datasets.\n",
    "\n",
    "Uses command line arguments to maximize flexibility, and run many options in parallel\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import sys \n",
    "sys.path.append('../src')\n",
    "import os\n",
    "import pickle\n",
    "import argparse\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from tensorboardX import SummaryWriter \n",
    "\n",
    "from models.actor import Actor\n",
    "from models.critic import Critic\n",
    "from models.discriminator import Discriminator\n",
    "from GAIL import *\n",
    "\n",
    "from dialog_environment import DialogEnvironment\n",
    "\n",
    "device='cuda' # for now\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Limitation Learning')\n",
    "\n",
    "parser.add_argument('--load_model', \n",
    "                    type=str, default=None, \n",
    "                    help='path to load the saved model')\n",
    "\n",
    "parser.add_argument('--gamma', \n",
    "                    type=float, default=0.99, \n",
    "                    help='discounted factor (default: 0.99)')\n",
    "\n",
    "parser.add_argument('--lamda', \n",
    "                    type=float, default=0.98, \n",
    "                    help='GAE hyper-parameter (default: 0.98)')\n",
    "\n",
    "\n",
    "parser.add_argument('--learning_rate', \n",
    "                    type=float, default=3e-4, \n",
    "                    help='learning rate of models (default: 3e-4)')\n",
    "\n",
    "parser.add_argument('--l2_rate', \n",
    "                    type=float, default=1e-3, \n",
    "                    help='l2 regularizer coefficient (default: 1e-3)')\n",
    "\n",
    "parser.add_argument('--clip_param', \n",
    "                    type=float, default=0.2, \n",
    "                    help='clipping parameter for PPO (default: 0.2)')\n",
    "\n",
    "parser.add_argument('--discrim_update_num', \n",
    "                    type=int, default=2, \n",
    "                    help='update number of discriminator (default: 2)')\n",
    "\n",
    "parser.add_argument('--actor_critic_update_num', \n",
    "                    type=int, default=10, \n",
    "                    help='update number of actor-critic (default: 10)')\n",
    "\n",
    "parser.add_argument('--total_sample_size', \n",
    "                    type=int, default=2048, \n",
    "                    help='total sample size to collect before PPO update (default: 2048)')\n",
    "\n",
    "parser.add_argument('--batch_size', \n",
    "                    type=int, default=128, \n",
    "                    help='batch size to update (default: 128)')\n",
    "\n",
    "parser.add_argument('--suspend_accu_exp', \n",
    "                    type=float, default=None,\n",
    "                    help='accuracy for suspending discriminator about expert data (default: None)')\n",
    "\n",
    "parser.add_argument('--suspend_accu_gen', \n",
    "                    type=float, default=None,\n",
    "                    help='accuracy for suspending discriminator about generated data (default: None)')\n",
    "\n",
    "parser.add_argument('--max_iter_num', \n",
    "                    type=int, default=4096,\n",
    "                    help='maximal number of main iterations (default: 4000)')\n",
    "\n",
    "parser.add_argument('--seed', \n",
    "                    type=int, default=42,\n",
    "                    help='random seed (default: 500)')\n",
    "\n",
    "parser.add_argument('--logdir', \n",
    "                    type=str, default='logs/EXPERIMENTNAME',\n",
    "                    help='tensorboardx logs directory (default: logs/EXPERIMENTNAME)')\n",
    "\n",
    "parser.add_argument('--hidden_size', \n",
    "                    type=int, default=128,\n",
    "                    help='New sequence length of the representation produced by the encoder/decoder RNNs. (default: 1024)')\n",
    "parser.add_argument('--num_layers', \n",
    "                    type=int, default=2,\n",
    "                    help='Number of layers in the respective RNNs (default: 2)')\n",
    "\n",
    "parser.add_argument('--seq_len', \n",
    "                    type=int, default=10,\n",
    "                    help='length of input and response sequences (default: 60, which is also max)')\n",
    "parser.add_argument('--input_size', \n",
    "                    type=int, default=300,\n",
    "                    help='DO NOT CHANGE UNLESS NEW EMBEDDINGS ARE MADE. Dimensionality of embeddings (default: 300)')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    env = DialogEnvironment()\n",
    "\n",
    "    torch.manual_seed(args.seed)\n",
    "\n",
    "    #TODO\n",
    "    actor = Actor(hidden_size=args.hidden_size,num_layers=args.num_layers,device='cuda',input_size=args.input_size,output_size=args.input_size)\n",
    "    critic = Critic(hidden_size=args.hidden_size,num_layers=args.num_layers,input_size=args.input_size,seq_len=args.seq_len)\n",
    "    discrim = Discriminator(hidden_size=args.hidden_size,num_layers=args.hidden_size,input_size=args.input_size,seq_len=args.seq_len)\n",
    "    \n",
    "    actor.to(device), critic.to(device), discrim.to(device)\n",
    "    \n",
    "    actor_optim = optim.Adam(actor.parameters(), lr=args.learning_rate)\n",
    "    critic_optim = optim.Adam(critic.parameters(), lr=args.learning_rate, \n",
    "                              weight_decay=args.l2_rate) \n",
    "    discrim_optim = optim.Adam(discrim.parameters(), lr=args.learning_rate)\n",
    "\n",
    "    # load demonstrations\n",
    "\n",
    "    writer = SummaryWriter(args.logdir)\n",
    "\n",
    "    if args.load_model is not None: #TODO\n",
    "        saved_ckpt_path = os.path.join(os.getcwd(), 'save_model', str(args.load_model))\n",
    "        ckpt = torch.load(saved_ckpt_path)\n",
    "\n",
    "        actor.load_state_dict(ckpt['actor'])\n",
    "        critic.load_state_dict(ckpt['critic'])\n",
    "        discrim.load_state_dict(ckpt['discrim'])\n",
    "\n",
    "\n",
    "    \n",
    "    episodes = 0\n",
    "    train_discrim_flag = True\n",
    "\n",
    "    for iter in range(args.max_iter_num):\n",
    "        actor.eval(), critic.eval()\n",
    "        memory = deque()\n",
    "\n",
    "        steps = 0\n",
    "        scores = []\n",
    "        similarity_scores = []\n",
    "        while steps < args.total_sample_size: \n",
    "            state, expert_action, raw_state, raw_expert_action = env.reset()\n",
    "            score = 0\n",
    "            similarity_score = 0\n",
    "            state = state[:args.seq_len,:]\n",
    "            expert_action = expert_action[:args.seq_len,:]\n",
    "            state = state.to(device)\n",
    "            expert_action = expert_action.to(device)\n",
    "            for _ in range(10000): \n",
    "\n",
    "                steps += 1\n",
    "\n",
    "                mu, std = actor(state.resize(1,args.seq_len,args.input_size)) #TODO: gotta be a better way to resize. \n",
    "                action = get_action(mu.cpu(), std.cpu())[0]\n",
    "                done= env.step(action)\n",
    "                irl_reward = get_reward(discrim, state, action, args)\n",
    "                if done:\n",
    "                    mask = 0\n",
    "                else:\n",
    "                    mask = 1\n",
    "\n",
    "\n",
    "                memory.append([state, torch.from_numpy(action).to(device), irl_reward, mask,expert_action])\n",
    "                score += irl_reward\n",
    "           #     similarity_score += get_cosine_sim(action,expert_action)\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            episodes += 1\n",
    "            scores.append(score)\n",
    "           # similarity_scores.append(similarity_score)\n",
    "\n",
    "        score_avg = np.mean(scores)\n",
    "      #  similarity_score_avg = np.mean(similarity_scores)\n",
    "        print('{}:: {} episode score is {:.2f}'.format(iter, episodes, score_avg))\n",
    "       # print('{}:: {} episode similarity score is {:.2f}'.format(iter, episodes, similarity_score_avg))\n",
    "\n",
    "        actor.train(), critic.train(), discrim.train()\n",
    "        if train_discrim_flag:\n",
    "            expert_acc, learner_acc = train_discrim(discrim, memory, discrim_optim, args) \n",
    "            print(\"Expert: %.2f%% | Learner: %.2f%%\" % (expert_acc * 100, learner_acc * 100))\n",
    "            writer.add_scalar('log/expert_acc', float(expert_acc), iter) #logg\n",
    "            writer.add_scalar('log/learner_acc', float(learner_acc), iter) #logg\n",
    "            writer.add_scalar('log/avg_acc', float(learner_acc + expert_acc)/2, iter) #logg\n",
    "            if args.suspend_accu_exp is not None: #only if not None do we check.\n",
    "                if expert_acc > args.suspend_accu_exp and learner_acc > args.suspend_accu_gen:\n",
    "                    train_discrim_flag = False\n",
    "                    \n",
    "        train_actor_critic(actor, critic, memory, actor_optim, critic_optim, args)\n",
    "        writer.add_scalar('log/score', float(score_avg), iter)\n",
    "     #   writer.add_scalar('log/similarity_score', float(similarity_score_avg), iter)\n",
    "        writer.add_text('log/raw_state', raw_state[0],iter)\n",
    "        raw_action = get_raw_action(action) #TODO\n",
    "        writer.add_text('log/raw_action', raw_action,iter)\n",
    "        writer.add_text('log/raw_expert_action', raw_expert_action,iter)\n",
    "\n",
    "        if iter % 100:\n",
    "            score_avg = int(score_avg)\n",
    "\n",
    "            print(similarity_score)\n",
    "            model_path = os.path.join(os.getcwd(),'save_model')\n",
    "            if not os.path.isdir(model_path):\n",
    "                os.makedirs(model_path)\n",
    "\n",
    "            ckpt_path = os.path.join(model_path, 'ckpt_'+ str(score_avg)+'.pth.tar')\n",
    "\n",
    "            save_checkpoint({\n",
    "                'actor': actor.state_dict(),\n",
    "                'critic': critic.state_dict(),\n",
    "                'discrim': discrim.state_dict(),\n",
    "                'args': args,\n",
    "                'score': score_avg,\n",
    "            }, filename=ckpt_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from argparse import Namespace\n",
    "args = Namespace(load_model=None,\n",
    "                render=False,\n",
    "                gamma=.99,\n",
    "                lamda=.98,\n",
    "                learning_rate=1e-4,\n",
    "                l2_rate=1e-3,\n",
    "                clip_param=.2,\n",
    "                discrim_update_num=2,\n",
    "                actor_critic_update_num=10,\n",
    "                total_sample_size=100,\n",
    "                batch_size=100,\n",
    "                suspend_accu_exp=None,# won't stop\n",
    "                suspend_accu_gen=None,\n",
    "                max_iter_num=4000,\n",
    "                seed=500,\n",
    "                logdir='logs/noah321',\n",
    "                 hidden_size=1,\n",
    "                 num_layers=1,\n",
    "                 seq_len=5,\n",
    "                 input_size=50\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/nsk367/anaconda3/envs/irl/lib/python3.8/site-packages/torch/tensor.py:447: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:: 100 episode score is 0.79\n",
      "Expert: 100.00% | Learner: 0.00%\n",
      "1:: 200 episode score is 0.79\n",
      "Expert: 100.00% | Learner: 0.00%\n",
      "0\n",
      "2:: 300 episode score is 0.78\n",
      "Expert: 100.00% | Learner: 0.00%\n",
      "0\n",
      "3:: 400 episode score is 0.78\n",
      "Expert: 100.00% | Learner: 0.00%\n",
      "0\n",
      "4:: 500 episode score is 0.78\n",
      "Expert: 100.00% | Learner: 0.00%\n",
      "0\n",
      "5:: 600 episode score is 0.78\n",
      "Expert: 100.00% | Learner: 0.00%\n",
      "0\n",
      "6:: 700 episode score is 0.77\n",
      "Expert: 100.00% | Learner: 0.00%\n",
      "0\n",
      "7:: 800 episode score is 0.77\n",
      "Expert: 100.00% | Learner: 0.00%\n",
      "0\n",
      "8:: 900 episode score is 0.77\n",
      "Expert: 100.00% | Learner: 0.00%\n",
      "0\n",
      "9:: 1000 episode score is 0.77\n",
      "Expert: 100.00% | Learner: 0.00%\n",
      "0\n",
      "10:: 1100 episode score is 0.77\n",
      "Expert: 100.00% | Learner: 0.00%\n",
      "0\n",
      "11:: 1200 episode score is 0.76\n",
      "Expert: 100.00% | Learner: 0.00%\n",
      "0\n",
      "12:: 1300 episode score is 0.76\n",
      "Expert: 100.00% | Learner: 0.00%\n",
      "0\n",
      "13:: 1400 episode score is 0.76\n",
      "Expert: 100.00% | Learner: 0.00%\n",
      "0\n",
      "14:: 1500 episode score is 0.76\n",
      "Expert: 100.00% | Learner: 0.00%\n",
      "0\n",
      "15:: 1600 episode score is 0.76\n",
      "Expert: 100.00% | Learner: 0.00%\n",
      "0\n",
      "16:: 1700 episode score is 0.75\n",
      "Expert: 100.00% | Learner: 0.00%\n",
      "0\n",
      "17:: 1800 episode score is 0.75\n",
      "Expert: 100.00% | Learner: 0.00%\n",
      "0\n",
      "18:: 1900 episode score is 0.75\n",
      "Expert: 100.00% | Learner: 0.00%\n",
      "0\n",
      "19:: 2000 episode score is 0.75\n",
      "Expert: 100.00% | Learner: 0.00%\n",
      "0\n",
      "20:: 2100 episode score is 0.75\n",
      "Expert: 100.00% | Learner: 0.00%\n",
      "0\n",
      "21:: 2200 episode score is 0.74\n",
      "Expert: 100.00% | Learner: 0.00%\n",
      "0\n",
      "22:: 2300 episode score is 0.74\n",
      "Expert: 100.00% | Learner: 0.00%\n",
      "0\n",
      "23:: 2400 episode score is 0.74\n",
      "Expert: 100.00% | Learner: 0.00%\n",
      "0\n",
      "24:: 2500 episode score is 0.74\n",
      "Expert: 100.00% | Learner: 0.00%\n",
      "0\n",
      "25:: 2600 episode score is 0.74\n",
      "Expert: 100.00% | Learner: 0.00%\n",
      "0\n",
      "26:: 2700 episode score is 0.74\n",
      "Expert: 100.00% | Learner: 0.00%\n",
      "0\n",
      "27:: 2800 episode score is 0.73\n",
      "Expert: 100.00% | Learner: 0.00%\n",
      "0\n",
      "28:: 2900 episode score is 0.73\n",
      "Expert: 100.00% | Learner: 0.00%\n",
      "0\n",
      "29:: 3000 episode score is 0.73\n",
      "Expert: 100.00% | Learner: 0.00%\n",
      "0\n",
      "30:: 3100 episode score is 0.73\n",
      "Expert: 100.00% | Learner: 0.00%\n",
      "0\n",
      "31:: 3200 episode score is 0.73\n",
      "Expert: 100.00% | Learner: 0.00%\n",
      "0\n",
      "32:: 3300 episode score is 0.72\n",
      "Expert: 100.00% | Learner: 0.00%\n",
      "0\n",
      "33:: 3400 episode score is 0.72\n",
      "Expert: 100.00% | Learner: 0.00%\n",
      "0\n",
      "34:: 3500 episode score is 0.72\n",
      "Expert: 100.00% | Learner: 0.00%\n",
      "0\n",
      "35:: 3600 episode score is 0.72\n",
      "Expert: 100.00% | Learner: 0.00%\n",
      "0\n",
      "36:: 3700 episode score is 0.72\n",
      "Expert: 100.00% | Learner: 0.00%\n",
      "0\n",
      "37:: 3800 episode score is 0.71\n",
      "Expert: 100.00% | Learner: 0.00%\n",
      "0\n",
      "38:: 3900 episode score is 0.71\n",
      "Expert: 100.00% | Learner: 0.00%\n",
      "0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-7ba81edd5cf1>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m                 \u001b[0mdone\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m                 \u001b[0mirl_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_reward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiscrim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m                     \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/nsk367/deepRL/limitation-learning/src/GAIL.py\u001b[0m in \u001b[0;36mget_reward\u001b[0;34m(discrim, state, action, args)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;31m#TODO: better resize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiscrim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msave_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/nsk367/anaconda3/envs/irl/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/nsk367/deepRL/limitation-learning/src/models/discriminator.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, state, action)\u001b[0m\n\u001b[1;32m     56\u001b[0m        \u001b[0;31m# return state_action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mstate_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mprob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/scratch/nsk367/deepRL/limitation-learning/src/models/discriminator.py\u001b[0m(58)\u001b[0;36mforward\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     56 \u001b[0;31m       \u001b[0;31m# return state_action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     57 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 58 \u001b[0;31m        \u001b[0mstate_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     59 \u001b[0;31m        \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     60 \u001b[0;31m        \u001b[0;32mreturn\u001b[0m \u001b[0mprob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> u\n",
      "> \u001b[0;32m/scratch/nsk367/anaconda3/envs/irl/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m(727)\u001b[0;36m_call_impl\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    725 \u001b[0;31m            \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    726 \u001b[0;31m        \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 727 \u001b[0;31m            \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    728 \u001b[0;31m        for hook in itertools.chain(\n",
      "\u001b[0m\u001b[0;32m    729 \u001b[0;31m                \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> u\n",
      "> \u001b[0;32m/scratch/nsk367/deepRL/limitation-learning/src/GAIL.py\u001b[0m(123)\u001b[0;36mget_reward\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    121 \u001b[0;31m    \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    122 \u001b[0;31m        \u001b[0;31m#TODO: better resize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 123 \u001b[0;31m        \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiscrim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    124 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    125 \u001b[0;31m\u001b[0;32mdef\u001b[0m \u001b[0msave_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> action\n",
      "tensor([[-0.2223, -0.1774, -0.0813, -0.0752,  0.0374, -0.0214, -0.2397,  0.1165,\n",
      "          0.1273, -0.0245, -0.1026, -0.1032, -0.0895,  0.2321, -0.0410,  0.0671,\n",
      "          0.0577,  0.0891,  0.1207, -0.1397,  0.1459, -0.1090,  0.2070,  0.1441,\n",
      "         -0.1886,  0.1311, -0.2692,  0.0566,  0.1879, -0.0547, -0.0728, -0.2095,\n",
      "          0.0230,  0.1618,  0.1467,  0.1699, -0.2070,  0.0510, -0.2235, -0.0056,\n",
      "          0.0776,  0.0893, -0.2173, -0.1800,  0.2211,  0.1493,  0.0317, -0.1487,\n",
      "          0.1440,  0.0064],\n",
      "        [-0.2196, -0.1887, -0.0767, -0.0717,  0.0274, -0.0177, -0.2299,  0.1197,\n",
      "          0.1359, -0.0263, -0.0903, -0.1213, -0.0851,  0.2299, -0.0484,  0.0785,\n",
      "          0.0579,  0.0875,  0.1335, -0.1457,  0.1452, -0.1175,  0.2037,  0.1396,\n",
      "         -0.1865,  0.1327, -0.2712,  0.0599,  0.1812, -0.0498, -0.0665, -0.1947,\n",
      "          0.0288,  0.1633,  0.1552,  0.1778, -0.2056,  0.0451, -0.2204, -0.0011,\n",
      "          0.0819,  0.1043, -0.2146, -0.1749,  0.2246,  0.1478,  0.0348, -0.1472,\n",
      "          0.1423,  0.0081],\n",
      "        [-0.2146, -0.1850, -0.0748, -0.0719,  0.0406, -0.0265, -0.2246,  0.1154,\n",
      "          0.1353, -0.0222, -0.0907, -0.1052, -0.0908,  0.2271, -0.0406,  0.0654,\n",
      "          0.0552,  0.0887,  0.1211, -0.1372,  0.1449, -0.1243,  0.2029,  0.1298,\n",
      "         -0.1900,  0.1368, -0.2682,  0.0548,  0.1867, -0.0539, -0.0748, -0.2018,\n",
      "          0.0243,  0.1560,  0.1425,  0.1696, -0.2057,  0.0280, -0.2217, -0.0031,\n",
      "          0.0877,  0.0967, -0.2162, -0.1834,  0.2151,  0.1531,  0.0362, -0.1562,\n",
      "          0.1365,  0.0073],\n",
      "        [-0.2104, -0.1841, -0.0842, -0.0841,  0.0355, -0.0195, -0.2320,  0.1215,\n",
      "          0.1302, -0.0230, -0.0821, -0.1171, -0.0937,  0.2253, -0.0279,  0.0670,\n",
      "          0.0562,  0.0879,  0.1128, -0.1352,  0.1508, -0.1141,  0.2010,  0.1394,\n",
      "         -0.1966,  0.1297, -0.2681,  0.0632,  0.1791, -0.0538, -0.0585, -0.2054,\n",
      "          0.0231,  0.1540,  0.1458,  0.1722, -0.2010,  0.0419, -0.2188, -0.0023,\n",
      "          0.0822,  0.0901, -0.2233, -0.1690,  0.2299,  0.1436,  0.0201, -0.1520,\n",
      "          0.1345, -0.0032],\n",
      "        [-0.2218, -0.1869, -0.0794, -0.0644,  0.0462, -0.0131, -0.2247,  0.1186,\n",
      "          0.1358, -0.0206, -0.0858, -0.1119, -0.0861,  0.2335, -0.0336,  0.0723,\n",
      "          0.0527,  0.0868,  0.1144, -0.1383,  0.1451, -0.1080,  0.2011,  0.1326,\n",
      "         -0.1888,  0.1307, -0.2621,  0.0619,  0.1904, -0.0552, -0.0648, -0.2084,\n",
      "          0.0275,  0.1642,  0.1518,  0.1663, -0.2021,  0.0465, -0.2143,  0.0012,\n",
      "          0.0860,  0.1083, -0.2121, -0.1763,  0.2259,  0.1501,  0.0350, -0.1522,\n",
      "          0.1319,  0.0038]], device='cuda:0')\n",
      "ipdb> expert_action\n",
      "*** NameError: name 'expert_action' is not defined\n",
      "ipdb> u\n",
      "> \u001b[0;32m<ipython-input-5-7ba81edd5cf1>\u001b[0m(166)\u001b[0;36mmain\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    164 \u001b[0;31m                \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    165 \u001b[0;31m                \u001b[0mdone\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 166 \u001b[0;31m                \u001b[0mirl_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_reward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiscrim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    167 \u001b[0;31m                \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    168 \u001b[0;31m                    \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> action.mean(\n",
      "*** SyntaxError: unexpected EOF while parsing\n",
      "ipdb> action.mean()\n",
      "-0.003497705\n",
      "ipdb> expert_action.mean()\n",
      "tensor(0.0068, device='cuda:0')\n",
      "ipdb> action.std()\n",
      "0.14131393\n",
      "ipdb> expert_action.std()\n",
      "tensor(0.1266, device='cuda:0')\n",
      "ipdb> state.mean()\n",
      "tensor(0.0023, device='cuda:0')\n",
      "ipdb> state.std()\n",
      "tensor(0.1097, device='cuda:0')\n",
      "ipdb> exit\n"
     ]
    }
   ],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The action space is huge. PPO with these small updates will take forever, and this in the end harms GAIL training. This demands pre-training, which we can do with BC. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!python ../src/main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = torch.ones(100,5,1)\n",
    "advants = torch.randn(100,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new = advants.unsqueeze(dim=1) * ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
