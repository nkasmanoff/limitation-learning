{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from models.utils import get_model\n",
    "from models.config import TOKENS_RAW_CUTOFF\n",
    "from models.seq2seqattn import init_weights, EncRnn, DecRnn, Seq2SeqAttn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1931, 300)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model = get_model()\n",
    "# w2ind from w2v\n",
    "w2ind = {token: token_index for token_index, token in enumerate(w2v_model.wv.index2word)} \n",
    "# sorted vocab words\n",
    "assert w2v_model.vocabulary.sorted_vocab == True\n",
    "word_counts = {word: vocab_obj.count for word, vocab_obj in w2v_model.wv.vocab.items()}\n",
    "word_counts = sorted(word_counts.items(), key=lambda x:-x[1])\n",
    "words = [t[0] for t in word_counts]\n",
    "# sentence marker token inds\n",
    "sos_ind = w2ind['<sos>']\n",
    "eos_ind = w2ind['<eos>']\n",
    "# adjusted sequence length\n",
    "SEQ_LEN = 5 + 2 # sos, eos tokens\n",
    "# padding token for now\n",
    "TRG_PAD_IDX = w2ind[\".\"] # this is 0\n",
    "# vocab, embed dims\n",
    "VOCAB_SIZE, EMBED_DIM = w2v_model.wv.vectors.shape\n",
    "VOCAB_SIZE, EMBED_DIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "enc = EncRnn(hidden_size=64, num_layers=2, embed_size=EMBED_DIM)\n",
    "dec = DecRnn(hidden_size=64, num_layers=2, embed_size=EMBED_DIM, output_size=VOCAB_SIZE)\n",
    "model = Seq2SeqAttn(enc, dec, TRG_PAD_IDX, VOCAB_SIZE, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.load_state_dict(torch.load(\n",
    "    '/scratch/nsk367/deepRL/limitation-learning/src/pretrained_generators/model-epoch10.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = torch.load('../dat/processed/padded_vectorized_states_v3.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(words, input_state, next_state, model, eos_ind, max_len, device):\n",
    "    \n",
    "    model.eval()\n",
    "    src_tensor = input_state.unsqueeze(0).to(device)\n",
    "    src_len = torch.Tensor([int(max_len)])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        encoder_outputs, hidden = model.encoder(src_tensor, src_len)\n",
    "\n",
    "    mask = model.create_mask(src_tensor.transpose(1,0)).to(device)\n",
    "    # get first decoder input (<sos>)'s one hot\n",
    "    trg_indexes = [next_state[0]]\n",
    "    # create a array to store attetnion\n",
    "    attentions = torch.zeros(max_len, 1, len(input_state))\n",
    "    #print(attentions.shape)\n",
    "\n",
    "\n",
    "    for i in range(max_len):\n",
    "        trg_tensor = torch.LongTensor([trg_indexes[-1]]).to(device)\n",
    "        #print(trg_tensor.shape)\n",
    "        with torch.no_grad():\n",
    "            output, hidden, attention = model.decoder(trg_tensor, hidden, encoder_outputs, mask)\n",
    "        #print(F.softmax(output))\n",
    "        attentions[i] = attention\n",
    "        pred_token = output.argmax(1).item()\n",
    "        if pred_token == eos_ind: # end of sentence.\n",
    "            break\n",
    "        trg_indexes.append(pred_token)\n",
    "        \n",
    "    trg_tokens = [words[int(ind)] for ind in trg_indexes]\n",
    "    #  remove <sos>\n",
    "    return trg_tokens[1:], attentions[:len(trg_tokens)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action_probs(model, input_state, sos_ind, eos_ind, SEQ_LEN, device):\n",
    "    \"\"\"\n",
    "    Given an input sequence and policy, produce a distribution over tokens the predicted token for each step in the sequence. \n",
    "    \"\"\"\n",
    "    src_tensor = input_state.unsqueeze(0).to(device)\n",
    "    src_len = torch.Tensor([int(SEQ_LEN)])\n",
    "    encoder_outputs, hidden = model.encoder(src_tensor, src_len)\n",
    "    print('encoutshape',encoder_outputs.shape)\n",
    "    mask = model.create_mask(src_tensor.transpose(1,0)).to(device)\n",
    "    trg_indexes = [sos_ind]\n",
    "    attentions = torch.zeros(SEQ_LEN, 1, len(input_state))\n",
    "    outputs = []\n",
    "    for i in range(SEQ_LEN):\n",
    "        trg_tensor = torch.LongTensor([trg_indexes[-1]]).to(device)\n",
    "        output, hidden, attention = model.decoder(trg_tensor, hidden, encoder_outputs, mask)\n",
    "        attentions[i] = attention\n",
    "        pred_token = output.argmax(1).item()\n",
    "        trg_indexes.append(pred_token)\n",
    "       # print(pred_token)\n",
    "        if pred_token == eos_ind: # end of sentence.\n",
    "        \n",
    "            break\n",
    "        outputs.append(output)\n",
    "        \n",
    "  #  trg_tokens = [words[int(ind)] for ind in trg_indexes]\n",
    "    #  remove <sos>\n",
    "    return F.softmax(torch.stack(outputs)).to(device)\n",
    "    #return torch.stack(outputs).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for toke in range(output_dist.shape[0]):\n",
    "    m = Categorical(output_dist[toke])\n",
    "    action = m.sample()\n",
    "    action_log_prob = m.log_prob(action)# * reward\n",
    "    print(action_log_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.argmax(dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "J = -action_log_prob * irl_reward\n",
    "\n",
    "# irl_reward = what discriminator says\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator(action,state) -> # between 0 and 1 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-37-27fd5da83057>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-37-27fd5da83057>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    for toke in range(output_dist.shape[0]):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "def get_action(action_probs):\n",
    "\n",
    "for toke in range(output_dist.shape[0]):\n",
    "    m = Categorical(output_dist[toke])\n",
    "    action = m.sample()\n",
    "    action_log_prob = m.log_prob(action)# * reward\n",
    "    print(action_log_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = []\n",
    "action_log_probs = []\n",
    "for i in action_probs:\n",
    "    m = Categorical(i)\n",
    "    action_ = m.sample()\n",
    "    action_log_prob = m.log_prob(action_)# * reward\n",
    "    action.append(action_.item())\n",
    "    action_log_probs.append(action_log_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_loss = torch.stack(action_log_probs).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-34.5850, device='cuda:0', grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "            policy_loss = -log_probs[batch_index] * rewards[batch_index]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAIL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-19.3099,  -4.5089,   1.9169,  ...,  -8.5969, -19.4253, -10.1636]],\n",
       "\n",
       "        [[-16.5315,   3.7468,   1.5240,  ...,  -7.6410, -16.6408,  -3.1556]],\n",
       "\n",
       "        [[-10.6652,   0.8810,   2.2188,  ...,  -2.5769, -10.7382,   1.9986]],\n",
       "\n",
       "        [[-17.1563,   3.1028,   0.3516,  ...,  -7.3466, -17.2156,  -4.6980]],\n",
       "\n",
       "        [[-23.7547,   7.6000,  -2.6602,  ..., -12.2492, -23.8158,   0.2901]]],\n",
       "       device='cuda:0', grad_fn=<StackBackward>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoutshape torch.Size([7, 1, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-62-1e5749893323>:27: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(torch.stack(outputs)).to(device)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "get_action() missing 5 required positional arguments: 'input_state', 'sos_ind', 'eos_ind', 'SEQ_LEN', and 'device'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-cd05b8a746af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0maction_probs\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mget_action_probs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msos_ind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meos_ind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSEQ_LEN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# get action from this using torch distributions!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: get_action() missing 5 required positional arguments: 'input_state', 'sos_ind', 'eos_ind', 'SEQ_LEN', and 'device'"
     ]
    }
   ],
   "source": [
    "# Main loop... \n",
    "# iterate through episodes\n",
    "for idx, (index, vects) in enumerate(d.items()):\n",
    "        input_state, expert_action = vects\n",
    "\n",
    "        input_state = torch.cat((torch.LongTensor([sos_ind]), \n",
    "                                 input_state,\n",
    "                                 torch.LongTensor([eos_ind])), \n",
    "                                 dim=0).to(device)\n",
    "        expert_action = torch.cat((torch.LongTensor([sos_ind]), \n",
    "                                expert_action, \n",
    "                                torch.LongTensor([eos_ind])), \n",
    "                               dim=0).to(device)\n",
    "        \n",
    "        \n",
    "        \n",
    "        action_probs =  get_action_probs(model, input_state, sos_ind, eos_ind, SEQ_LEN, device)\n",
    "        action = get_action(action_probs)\n",
    "        break\n",
    "        # get action from this using torch distributions! \n",
    "        \n",
    "        state_tokens = [words[int(ind)] for ind in input_state.cpu().detach().numpy()][1:-1]\n",
    "        #expert_act = [words[int(ind)] for ind in next_state.cpu().detach().numpy()][1:-1]\n",
    "        print(state_tokens,'\\n',target_tokens)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = input_state.unsqueeze(0).to(device)\n",
    "expert_action = expert_action.unsqueeze(0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self,model,SEQ_LEN):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.state_encoder = model.encoder\n",
    "        self.action_encoder = model.encoder\n",
    "        self.src_len = torch.Tensor([int(SEQ_LEN)])\n",
    "    def forward(self,x1,x2):\n",
    "        state_z, _ = self.state_encoder(x1, self.src_len)\n",
    "        action_z, _ = self.action_encoder(x2, self.src_len)\n",
    "\n",
    "        state_action = torch.cat([state_z.flatten().unsqueeze(0), action_z.flatten().unsqueeze(0)],dim=1)\n",
    "        return state_encoder_outputs, action_encoder_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "discrim = Discriminator(model,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_z,action_z = discrim(state,expert_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1, 128])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1, 128])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 640])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_z.flatten().unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 640])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_z.flatten().unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1312,  0.1229,  0.2266,  ..., -0.0913,  0.1660, -0.3223]],\n",
       "       device='cuda:0', grad_fn=<CatBackward>)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([state_z.flatten().unsqueeze(0), action_z.flatten().unsqueeze(0)],dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
