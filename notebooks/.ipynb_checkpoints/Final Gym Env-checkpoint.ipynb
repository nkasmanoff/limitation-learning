{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully last setup necessary for preparing the environment for Inverse RL procedure on conversational AI. In this version we use pre-trained word embeddings and combine them with the raw states to provide a clear flow of logic as a part of the convo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "OpenAI Gym inspired environment for this NLP task. \n",
    "\n",
    "Upon resetting environment, returns the state and expert action in raw and embedding form. \n",
    "\n",
    "In our case conversations are only pairs, although this is a scalable approach, and as a\n",
    "starting point for that we include a .step(action) function which simply returns done=True. This also allows us\n",
    "to make our framework as similar as possible to previously successful approaches using GAIL. \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import torch\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import torch\n",
    "from torch.jit import script, trace\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import csv\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import unicodedata\n",
    "import codecs\n",
    "import numpy as np\n",
    "from io import open\n",
    "import itertools\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class DialogEnvironment(object):\n",
    "    \"\"\"\n",
    "    \n",
    "    Gym environment for dialog.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, mode='train'):\n",
    "        \n",
    "\n",
    "        self.conversations = torch.load('../apps/dat/preprocess/padded_vectorized_states.pt')\n",
    "        self.raw_conversations = torch.load('../apps/dat/preprocess/raw_states.pt')\n",
    "        \n",
    "        \n",
    "        self.conversations_visited = []\n",
    "        \n",
    "    def clear(self):\n",
    "        self.conversations_visited = [] #\n",
    "    def current_state(self):\n",
    "        return i  # i for current conversation index, j for current word (these should be odd? )\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Start a new trajectory, aka a new conversation. Environment does this by \n",
    "        picking a random i in the length of the total conversations. \n",
    "\n",
    "        Using random with replacement, so it is possible to revisit environments.\n",
    "\n",
    "        I will leave this as a TODO in case without replacement is preferred. \n",
    "        \"\"\"\n",
    "        while True: #some indices have been removed, for various resions. This while loop \n",
    "                    # allows us to keep trying until a viable conversation is selected. \n",
    "            try:\n",
    "                self.i = random.randint(a=0,b=len(self.conversations))\n",
    "                self.conversations_visited.append(self.i)\n",
    "                self.conversation = self.conversations[self.i]\n",
    "                break\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "\n",
    "        state = self.conversation[0]\n",
    "        expert_action = self.conversation[0]\n",
    "        \n",
    "        raw_state = list(self.raw_conversations.keys())[self.i], \n",
    "        \n",
    "        raw_expert_action = self.raw_conversations[list(self.raw_conversations.keys())[self.i]]\n",
    "\n",
    "        return state, expert_action, raw_state, raw_expert_action\n",
    "    \n",
    "    def step(self,action):\n",
    "        done = True\n",
    "\n",
    "        return done\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = DialogEnvironment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "state, expert_action, raw_state, raw_expert_action = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0532,  0.1359,  0.0235,  ..., -0.2295, -0.2234, -0.0766],\n",
       "        [ 0.0985,  0.2500, -0.2702,  ..., -0.0626,  0.2442,  0.1778],\n",
       "        [ 0.0014,  0.3565, -0.0555,  ..., -0.1124,  0.0783,  0.2240],\n",
       "        ...,\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Actor,self).__init__()\n",
    "        \n",
    "        self.encoder = nn.RNN(input_size = 300,hidden_size=300)\n",
    "        \n",
    "    def forward(self,x)\n",
    "    \n",
    "        m\n",
    "        \n",
    "        \n",
    "ninp, nhid, nlayers, nonlinearity=nonlinearity, dropout=dropout\n",
    "class RNNModel(nn.Module):\n",
    "    \"\"\"Container module with an encoder, a recurrent module, and a decoder.\"\"\"\n",
    "\n",
    "    def __init__(self, rnn_type, ntoken, ninp, nhid, nlayers, dropout=0.5, tie_weights=False):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.ntoken = ntoken\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        if rnn_type in ['LSTM', 'GRU']:\n",
    "            self.rnn = getattr(nn, rnn_type)(ninp, nhid, nlayers, dropout=dropout)\n",
    "        else:\n",
    "            try:\n",
    "                nonlinearity = {'RNN_TANH': 'tanh', 'RNN_RELU': 'relu'}[rnn_type]\n",
    "            except KeyError:\n",
    "                raise ValueError( \"\"\"An invalid option for `--model` was supplied,\n",
    "                                 options are ['LSTM', 'GRU', 'RNN_TANH' or 'RNN_RELU']\"\"\")\n",
    "            self.rnn = nn.RNN(ninp, nhid, nlayers, nonlinearity=nonlinearity, dropout=dropout)\n",
    "        self.decoder = nn.Linear(nhid, ntoken)\n",
    "\n",
    "        # Optionally tie weights as in:\n",
    "        # \"Using the Output Embedding to Improve Language Models\" (Press & Wolf 2016)\n",
    "        # https://arxiv.org/abs/1608.05859\n",
    "        # and\n",
    "        # \"Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling\" (Inan et al. 2016)\n",
    "        # https://arxiv.org/abs/1611.01462\n",
    "        if tie_weights:\n",
    "            if nhid != ninp:\n",
    "                raise ValueError('When using the tied flag, nhid must be equal to emsize')\n",
    "            self.decoder.weight = self.encoder.weight\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "        self.rnn_type = rnn_type\n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayers\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        nn.init.uniform_(self.encoder.weight, -initrange, initrange)\n",
    "        nn.init.zeros_(self.decoder.weight)\n",
    "        nn.init.uniform_(self.decoder.weight, -initrange, initrange)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        emb = self.drop(self.encoder(input))\n",
    "        output, hidden = self.rnn(emb, hidden)\n",
    "        output = self.drop(output)\n",
    "        decoded = self.decoder(output)\n",
    "        decoded = decoded.view(-1, self.ntoken)\n",
    "        return F.log_softmax(decoded, dim=1), hidden\n",
    "\n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters())\n",
    "        if self.rnn_type == 'LSTM':\n",
    "            return (weight.new_zeros(self.nlayers, bsz, self.nhid),\n",
    "                    weight.new_zeros(self.nlayers, bsz, self.nhid))\n",
    "        else:\n",
    "            return weight.new_zeros(self.nlayers, bsz, self.nhid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = nn.RNN(input_size = 300,hidden_size=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = test(torch.randn(1,60,300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 60, 300])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
