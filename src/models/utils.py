import torch
import gensim
import numpy as np

def get_model():
    model = gensim.models.Word2Vec.load("../models/custom_w2v_intersect_GoogleNews") # ("./models/custom_w2v")
    model.init_sims(replace=True) #precomputed l2 normed vectors in-place â€“ saving the extra RAM
    EMBED_DIM = model.wv.vectors.shape[1]
    model.wv["<sos>"] = np.random.rand(EMBED_DIM)
    model.wv["<eos>"] = np.random.rand(EMBED_DIM)
    return model

def get_vectors():
    model = get_model()
    return torch.FloatTensor(model.wv.vectors)

def from_pretrained(embeddings=None, freeze=False):
    if not embeddings:
        embeddings = get_vectors() # 2 D embeddings param
    rows, cols = embeddings.shape
    # A simple lookup table that stores embeddings of a fixed dictionary and size.
    embedding = torch.nn.Embedding(num_embeddings=rows, embedding_dim=cols)
    embedding.weight = torch.nn.Parameter(embeddings)
    # no update if freeze=True (default is False)
    embedding.weight.requires_grad = not freeze
    return embedding
