{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-implementing the GAIL algorithm on the expert data acquired to run mountain car"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import os\n",
    "import gym\n",
    "import pickle\n",
    "import argparse\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from tensorboardX import SummaryWriter \n",
    "from utils.utils import *\n",
    "from utils.zfilter import *\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x103812ed0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs, hidden_size):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(num_inputs, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, num_outputs)\n",
    "        \n",
    "        self.fc3.weight.data.mul_(0.1)\n",
    "        self.fc3.bias.data.mul_(0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(self.fc1(x))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        actions = self.fc3(x)\n",
    "\n",
    "        return actions\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, num_inputs, hidden_size):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(num_inputs, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "        self.fc3.weight.data.mul_(0.1)\n",
    "        self.fc3.bias.data.mul_(0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(self.fc1(x))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        v = self.fc3(x)\n",
    "        return v\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, num_inputs, hidden_size):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.fc1 = nn.Linear(num_inputs, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "      #  self.fc3.weight.data.mul_(0.1)\n",
    "     #   self.fc3.bias.data.mul_(0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(self.fc1(x))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        prob = torch.sigmoid(self.fc3(x))\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37508, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I ran a DQN and after it beat the MountainCar env, I told it \n",
    "\n",
    "demonstrations = np.load('/Users/noahkasmanoff/Desktop/Projects/lets-do-irl/noah/gail/MCExpert.npy')\n",
    "                         \n",
    "demonstrations.shape                  \n",
    "                         \n",
    "                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected character after line continuation character (<ipython-input-5-44ecaf0cebbe>, line 34)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-44ecaf0cebbe>\"\u001b[0;36m, line \u001b[0;32m34\u001b[0m\n\u001b[0;31m    discrim_loss = criterion(learner, torch.ones((states.shape[0], 1))) + \\ # ones for learner\u001b[0m\n\u001b[0m                                                                                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected character after line continuation character\n"
     ]
    }
   ],
   "source": [
    "#code for training \n",
    "import torch\n",
    "import numpy as np\n",
    "from utils.utils import get_entropy, log_prob_density\n",
    "\n",
    "def train_discrim(discrim, memory, discrim_optim, demonstrations, discrim_update_num, batch_size, clip_param):\n",
    "    \"\"\"\n",
    "    Training the discriminator. \n",
    "\n",
    "    Why? \n",
    "    Goal of discrim is to do bce loss on learner and expert.\n",
    "\n",
    "    Should go through step by step, but the idea is to make learner look bad? That way it improves  \n",
    "\n",
    "    \"\"\"\n",
    "    memory = np.array(memory)  # s a r s' tuple\n",
    "    states = np.vstack(memory[:, 0]) \n",
    "    actions = list(memory[:, 1]) #actions taken by actor/policy\n",
    "    # TODO: Fix discrim \n",
    "    states = torch.Tensor(states)\n",
    "    actions = torch.Tensor(actions)\n",
    "        \n",
    "    criterion = torch.nn.BCELoss() # classify\n",
    " \n",
    "    for _ in range(discrim_update_num):\n",
    "        \n",
    "        learner = discrim(torch.cat([states, actions], dim=1)) #pass (s,a) through discriminator\n",
    "        demonstrations = torch.Tensor(demonstrations) # pass (s,a) of expert through discriminator\n",
    "        index = torch.randperm(demonstrations.shape[0])\n",
    "        demonstrations_batch = demonstrations[index,:][:batch_size]\n",
    "        expert = discrim(demonstrations_batch) #discrimator \"guesses\" whether or not these \n",
    "        # actions came from expert or learner\n",
    "\n",
    "        discrim_loss = criterion(learner, torch.ones((states.shape[0], 1))) + \\ # ones for learner\n",
    "                        criterion(expert, torch.zeros((demonstrations_batch.shape[0], 1))) # zeros for expert # TODO\n",
    "                # discrim loss: predict agent is all wrong, get as close to 0, and predict expert is 1, getting as close to 1 as possible. \n",
    "        discrim_optim.zero_grad() # gan loss, it tries to always get it right. \n",
    "        discrim_loss.backward()\n",
    "        discrim_optim.step()\n",
    "\n",
    "        # take these steps, do it however many times specified. \n",
    "\n",
    "    expert_acc = ((discrim(demonstrations_batch) < 0.5).float()).mean() #how often it realized the fake examples were fake\n",
    "    learner_acc = ((discrim(torch.cat([states, actions], dim=1)) > 0.5).float()).mean() #how often if predicted expert correctly. \n",
    "\n",
    "    return expert_acc, learner_acc # accuracy, it's the same kind, but because imbalanced better to look at separately. \n",
    " \n",
    "\n",
    "def train_actor_critic(actor, critic, memory, actor_optim, critic_optim, actor_critic_update_num, batch_size, clip_param):\n",
    "    \"\"\"\n",
    "    Using get gae, this is basically ppo . \n",
    "\n",
    "    It's somewhat straightforward, and trained with the irl reward which is \n",
    "    from that memory versus what would usually be the real rewa\n",
    "    \n",
    "    Transform this into discrete PPO more or less\n",
    "    \"\"\"\n",
    "    memory = np.array(memory) \n",
    "    # tuple of a regular old RL problem, but now reward is what the discriminator says. \n",
    "    states = np.vstack(memory[:, 0]) \n",
    "    actions = list(memory[:, 1]) \n",
    "    rewards = list(memory[:, 2])  #IRL Rewards? yes. \n",
    "    masks = list(memory[:, 3]) \n",
    "\n",
    "    # compute value of what happened, see if what we can get ius better. \n",
    "    old_values = critic(torch.Tensor(states))\n",
    "    #GAE aka estimate of Value + actual return roughtly \n",
    "    returns, advants = get_gae(rewards, masks, old_values, gamma, lamda) #TODO\n",
    "    \n",
    "    # pass states through actor, get corresponding actions\n",
    "    mu, std = actor(torch.Tensor(states)) #TODO\n",
    "    # new mus and stds? \n",
    "    old_policy = log_prob_density(torch.Tensor(actions), mu, std) # sum of log probability\n",
    "    # of old actions  \n",
    "\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    n = len(states)\n",
    "    arr = np.arange(n)\n",
    "\n",
    "    for _ in range(actor_critic_update_num):\n",
    "        np.random.shuffle(arr)\n",
    "\n",
    "        for i in range(n // batch_size): \n",
    "            batch_index = arr[batch_size * i : batch_size * (i + 1)]\n",
    "            #batch_index = torch.LongTensor(batch_index)\n",
    "            \n",
    "            inputs = torch.Tensor(states)[batch_index]\n",
    "            actions_samples = torch.Tensor(actions)[batch_index]\n",
    "            returns_samples = returns.unsqueeze(1)[batch_index]\n",
    "            advants_samples = advants.unsqueeze(1)[batch_index]\n",
    "            oldvalue_samples = old_values[batch_index].detach()\n",
    "        \n",
    "        \n",
    "            values = critic(inputs) #\n",
    "            clipped_values = oldvalue_samples + \\\n",
    "                             torch.clamp(values - oldvalue_samples,\n",
    "                                         -clip_param, \n",
    "                                         clip_param)\n",
    "            critic_loss1 = criterion(clipped_values, returns_samples)\n",
    "            critic_loss2 = criterion(values, returns_samples)\n",
    "            critic_loss = torch.max(critic_loss1, critic_loss2).mean()\n",
    "\n",
    "            loss, ratio, entropy = surrogate_loss(actor, advants_samples, inputs,\n",
    "                                         old_policy.detach(), actions_samples,\n",
    "                                         batch_index)\n",
    "            clipped_ratio = torch.clamp(ratio,\n",
    "                                        1.0 - clip_param,\n",
    "                                        1.0 + clip_param)\n",
    "            clipped_loss = clipped_ratio * advants_samples\n",
    "            actor_loss = -torch.min(loss, clipped_loss).mean()\n",
    "            #print(actor_loss,critic_loss,entropy)\n",
    "           # return actor_loss, critic_loss, entropy\n",
    "            loss = actor_loss + 0.5 * critic_loss - 0.001 * entropy\n",
    "           # asdf\n",
    "            #loss = loss.mean() #TODO\n",
    "            actor_optim.zero_grad()\n",
    "            loss.backward()\n",
    "            actor_optim.step()\n",
    "\n",
    "           # critic_optim.zero_grad()\n",
    "           # loss.backward() \n",
    "            critic_optim.step()\n",
    "\n",
    "           # loss.zero_grad()\n",
    "\n",
    "def get_gae(rewards, masks, values, gamma, lamda):\n",
    "    \"\"\"\n",
    "    How much better a particular action is in a particular state. \n",
    "    \n",
    "    Uses reward of current action + value function of that state-action pair, discount factor gamma, and then lamda to compute. \n",
    "    \"\"\"\n",
    "    rewards = torch.Tensor(rewards)\n",
    "    masks = torch.Tensor(masks)\n",
    "    returns = torch.zeros_like(rewards)\n",
    "    advants = torch.zeros_like(rewards)\n",
    "    \n",
    "    running_returns = 0\n",
    "    previous_value = 0\n",
    "    running_advants = 0\n",
    "\n",
    "    for t in reversed(range(0, len(rewards))):\n",
    "        running_returns = rewards[t] + (gamma * running_returns * masks[t])\n",
    "        returns[t] = running_returns\n",
    "\n",
    "        running_delta = rewards[t] + (gamma * previous_value * masks[t]) - \\\n",
    "                                        values.data[t]\n",
    "        previous_value = values.data[t]\n",
    "        \n",
    "        running_advants = running_delta + (gamma * lamda * \\\n",
    "                                            running_advants * masks[t])\n",
    "        advants[t] = running_advants\n",
    "\n",
    "    advants = (advants - advants.mean()) / advants.std()\n",
    "    return returns, advants\n",
    "\n",
    "def surrogate_loss(actor, advants, states, old_policy, actions, batch_index):\n",
    "    \"\"\"\n",
    "    The loss for PPO. Re-run through network, recomput policy from states\n",
    "    and see if this surrogate ratio is better. If it is, use as proximal policy update. It's very close to prior policy, but def better. \n",
    "    \n",
    "    Not sure this actually works though. Should not the new mu and stds be used to draw,\n",
    "    \n",
    "        When do we use get_action? Only once in main, I think it should be for all? \n",
    "    \"\"\"\n",
    "    mu, std = actor(states)\n",
    "    new_policy = log_prob_density(actions, mu, std) # TODO\n",
    "    old_policy = old_policy[batch_index]\n",
    "\n",
    "    ratio = torch.exp(new_policy - old_policy)\n",
    "    surrogate_loss = ratio * advants\n",
    "    entropy = get_entropy(mu, std)\n",
    "\n",
    "    return surrogate_loss, ratio, entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state size: 2\n",
      "action size: 1\n"
     ]
    }
   ],
   "source": [
    "# Normally args but not here :-)\n",
    "env_name = 'MountainCar-v0' # TODO\n",
    "load_model = None\n",
    "seed = 0\n",
    "render = False\n",
    "gamma = 0.99\n",
    "lamda = .98\n",
    "hidden_size = 128\n",
    "train_discrim_flag = True\n",
    "learning_rate = 3e-4\n",
    "clip_param = .2\n",
    "discrim_update_num = 2\n",
    "actor_critic_update_num = 10\n",
    "l2_rate = 1e-3 # weight decay\n",
    "total_sample_size = 256 # total num of state-actions to collect before learning\n",
    "batch_size = 32\n",
    "suspend_accu_exp = 1. # do not need to be this high typically, but seems likely it has to be for a simple env like mountain car cont.\n",
    "suspend_accu_gen = 1.\n",
    "max_iter_num = 500\n",
    "seed = 42\n",
    "logdir = 'logs'\n",
    "\n",
    "env = gym.make(env_name)\n",
    "    \n",
    "env.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "num_inputs = env.observation_space.shape[0]\n",
    "num_actions = 1#env.action_space.n#.shape[0]\n",
    "running_state = ZFilter((num_inputs,), clip=5) # huh? \n",
    "# oh wow. ZFilter is exactly what I do in capstone project, removing \"badtimes\"\n",
    "\n",
    "print('state size:', num_inputs) \n",
    "print('action size:', num_actions)\n",
    "\n",
    "#load agent stuff \n",
    "actor = Actor(num_inputs, num_actions, hidden_size)\n",
    "critic = Critic(num_inputs, hidden_size)\n",
    "discrim = Discriminator(num_inputs + num_actions, hidden_size)\n",
    "\n",
    "actor_optim = optim.Adam(actor.parameters(), lr=learning_rate)\n",
    "critic_optim = optim.Adam(critic.parameters(), lr=learning_rate, \n",
    "                          weight_decay=l2_rate) \n",
    "discrim_optim = optim.Adam(discrim.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation 0 is car position, and observation 1 is car velocity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "demonstrations.shape (37508, 3)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch, m1: [1 x 2], m2: [3 x 128] at ../aten/src/TH/generic/THTensorMath.cpp:41",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-ab6f3f628a57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;31m#action = get_action(mu, std)[0] #compute random action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#take a step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0mirl_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_reward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiscrim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#infer what the reward of this action is based on discriminator's get reward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m             \u001b[0mstop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Projects/lets-do-irl/noah/gail/utils/utils.py\u001b[0m in \u001b[0;36mget_reward\u001b[0;34m(discrim, state, action)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mstate_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiscrim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msave_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-8fc1d1e2baa3>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1674\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1675\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1676\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1677\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1678\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: size mismatch, m1: [1 x 2], m2: [3 x 128] at ../aten/src/TH/generic/THTensorMath.cpp:41"
     ]
    }
   ],
   "source": [
    "# load demonstrations\n",
    "#expert_demo, _ = pickle.load(open('./expert_demo/expert_demo.p', \"rb\"))\n",
    "#demonstrations = np.load('/Users/noahkasmanoff/Desktop/Projects/lets-do-irl/mountaincar/app/expert_demo/expert_demo.npy')\n",
    "print(\"demonstrations.shape\", demonstrations.shape)\n",
    "\n",
    "writer = SummaryWriter(logdir)\n",
    "\n",
    "#if you aren't starting from scratch, load in this \n",
    "if load_model is not None:\n",
    "    saved_ckpt_path = os.path.join(os.getcwd(), 'save_model', str(load_model))\n",
    "    ckpt = torch.load(saved_ckpt_path)\n",
    "\n",
    "    # initialize everything\n",
    "    actor.load_state_dict(ckpt['actor'])\n",
    "    critic.load_state_dict(ckpt['critic'])\n",
    "    discrim.load_state_dict(ckpt['discrim'])\n",
    "\n",
    "    running_state.rs.n = ckpt['z_filter_n']\n",
    "    running_state.rs.mean = ckpt['z_filter_m']\n",
    "    running_state.rs.sum_square = ckpt['z_filter_s']\n",
    "\n",
    "    print(\"Loaded OK ex. Zfilter N {}\".format(running_state.rs.n))\n",
    "\n",
    "# if no old model no worries, start training. \n",
    "episodes = 0\n",
    "for iter in range(max_iter_num):\n",
    "    # for i total trajectories \n",
    "    actor.eval(), critic.eval()\n",
    "    memory = deque()\n",
    "\n",
    "    steps = 0\n",
    "    scores = []\n",
    "\n",
    "    while steps < total_sample_size: \n",
    "        # sample trajectories  (batch size)\n",
    "        state = env.reset()\n",
    "        score = 0\n",
    "\n",
    "        state = running_state(state) #uh.. again ZFilter related, cleans the state \n",
    "        epsteps = 0\n",
    "        done = False\n",
    "        while not done: \n",
    "            epsteps += 1\n",
    "            #run through environment\n",
    "            if render: \n",
    "                env.render()\n",
    "\n",
    "            steps += 1\n",
    "\n",
    "            action_probs = torch.softmax(actor(torch.Tensor(state).unsqueeze(0)),-1) #pass state through actor network $ TODO\n",
    "            action, action_log_prob = get_discrete_action(action_probs)\n",
    "            action = action.numpy()[0]\n",
    "            action_log_prob = action_log_prob.detach().numpy()[0]\n",
    "            \n",
    "\n",
    "            #action = get_action(mu, std)[0] #compute random action\n",
    "            next_state, reward, done, _ = env.step(action.item()) #take a step\n",
    "            irl_reward = get_reward(discrim, state, action) #infer what the reward of this action is based on discriminator's get reward \n",
    "            stop\n",
    "            if done:\n",
    "                mask = 0\n",
    "            else:\n",
    "                mask = 1 #if done, save this, \n",
    "\n",
    "            memory.append([state, action, irl_reward, mask])\n",
    "\n",
    "            next_state = running_state(next_state) #save cleaned next state\n",
    "            state = next_state #and set to current state, \n",
    "\n",
    "            score += irl_reward #add total reward\n",
    "           # print(\"IRL Reward=\",irl_reward)\n",
    "            if done:\n",
    "                break\n",
    "            #actual sampling done here \n",
    "\n",
    "            \n",
    "\n",
    "        \n",
    "        episodes += 1\n",
    "        scores.append(score)\n",
    "        print(\"steps in ep:\", epsteps)\n",
    "    score_avg = np.mean(scores) #how this model did, \n",
    "    print('{}:: {} episode score is {:.2f}'.format(iter, episodes, score_avg))\n",
    "    writer.add_scalar('log/score', float(score_avg), iter) #logg\n",
    "\n",
    "    actor.train(), critic.train(), discrim.train() #now train \n",
    "    if train_discrim_flag: #if this batch optimizes discrim/reward, \n",
    "        # for training the discriminator, classify where state-action pair came from. \n",
    "        expert_acc, learner_acc = train_discrim(discrim, memory, discrim_optim, demonstrations, discrim_update_num, batch_size, clip_param) # see comments in train_model. \n",
    "        print(\"Expert: %.2f%% | Learner: %.2f%%\" % (expert_acc * 100, learner_acc * 100))\n",
    "        if expert_acc > suspend_accu_exp and learner_acc > suspend_accu_gen:\n",
    "            print(\"Now it will only train the policy, seeing as it is good enough at finding the differences between learner and expert trajectories.\")\n",
    "            train_discrim_flag = False #now train only policy, vanilla RL\n",
    "            render = True\n",
    "    #for training actor critic \n",
    "    \n",
    "    # PPO operation, \n",
    "    train_actor_critic(actor, critic, memory, actor_optim, critic_optim, actor_critic_update_num, batch_size, clip_param) # no output, see comments in train_model \n",
    "\n",
    "    if iter % 100:\n",
    "        score_avg = int(score_avg)\n",
    "\n",
    "        model_path = os.path.join(os.getcwd(),'save_model')\n",
    "        if not os.path.isdir(model_path):\n",
    "            os.makedirs(model_path)\n",
    "\n",
    "        ckpt_path = os.path.join(model_path, 'ckpt_'+ str(score_avg)+'.pth.tar')\n",
    "\n",
    "        save_checkpoint({\n",
    "            'actor': actor.state_dict(),\n",
    "            'critic': critic.state_dict(),\n",
    "            'discrim': discrim.state_dict(),\n",
    "            'z_filter_n':running_state.rs.n,\n",
    "            'z_filter_m': running_state.rs.mean,\n",
    "            'z_filter_s': running_state.rs.sum_square,\n",
    "           # 'args': args,\n",
    "            'score': score_avg\n",
    "        }, filename=ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/Users/noahkasmanoff/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m(1676)\u001b[0;36mlinear\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m   1674 \u001b[0;31m        \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m   1675 \u001b[0;31m    \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m-> 1676 \u001b[0;31m        \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m   1677 \u001b[0;31m        \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m   1678 \u001b[0;31m            \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> u\n",
      "> \u001b[0;32m/Users/noahkasmanoff/anaconda3/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m(91)\u001b[0;36mforward\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     89 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     90 \u001b[0;31m    \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 91 \u001b[0;31m        \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     92 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     93 \u001b[0;31m    \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> u\n",
      "> \u001b[0;32m/Users/noahkasmanoff/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m(722)\u001b[0;36m_call_impl\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    720 \u001b[0;31m            \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    721 \u001b[0;31m        \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 722 \u001b[0;31m            \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    723 \u001b[0;31m        for hook in itertools.chain(\n",
      "\u001b[0m\u001b[0;32m    724 \u001b[0;31m                \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> u\n",
      "> \u001b[0;32m<ipython-input-3-8fc1d1e2baa3>\u001b[0m(47)\u001b[0;36mforward\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     45 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     46 \u001b[0;31m    \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 47 \u001b[0;31m        \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     48 \u001b[0;31m        \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     49 \u001b[0;31m        \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> x.shape\n",
      "torch.Size([2])\n",
      "ipdb> u\n",
      "> \u001b[0;32m/Users/noahkasmanoff/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m(722)\u001b[0;36m_call_impl\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    720 \u001b[0;31m            \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    721 \u001b[0;31m        \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 722 \u001b[0;31m            \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    723 \u001b[0;31m        for hook in itertools.chain(\n",
      "\u001b[0m\u001b[0;32m    724 \u001b[0;31m                \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> x\n",
      "*** NameError: name 'x' is not defined\n",
      "ipdb> d\n",
      "> \u001b[0;32m<ipython-input-3-8fc1d1e2baa3>\u001b[0m(47)\u001b[0;36mforward\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     45 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     46 \u001b[0;31m    \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 47 \u001b[0;31m        \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     48 \u001b[0;31m        \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     49 \u001b[0;31m        \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> x\n",
      "tensor([0., 0.])\n",
      "ipdb> u\n",
      "> \u001b[0;32m/Users/noahkasmanoff/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m(722)\u001b[0;36m_call_impl\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    720 \u001b[0;31m            \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    721 \u001b[0;31m        \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 722 \u001b[0;31m            \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    723 \u001b[0;31m        for hook in itertools.chain(\n",
      "\u001b[0m\u001b[0;32m    724 \u001b[0;31m                \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> u\n",
      "> \u001b[0;32m/Users/noahkasmanoff/Desktop/Projects/lets-do-irl/noah/gail/utils/utils.py\u001b[0m(45)\u001b[0;36mget_reward\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     43 \u001b[0;31m    \u001b[0mstate_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     44 \u001b[0;31m    \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 45 \u001b[0;31m        \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiscrim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     46 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     47 \u001b[0;31m\u001b[0;32mdef\u001b[0m \u001b[0msave_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> u\n",
      "> \u001b[0;32m<ipython-input-17-ab6f3f628a57>\u001b[0m(58)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     56 \u001b[0;31m            \u001b[0;31m#action = get_action(mu, std)[0] #compute random action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     57 \u001b[0;31m            \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#take a step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 58 \u001b[0;31m            \u001b[0mirl_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_reward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiscrim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#infer what the reward of this action is based on discriminator's get reward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     59 \u001b[0;31m            \u001b[0mstop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     60 \u001b[0;31m            \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ipdb> d\n",
      "> \u001b[0;32m/Users/noahkasmanoff/Desktop/Projects/lets-do-irl/noah/gail/utils/utils.py\u001b[0m(45)\u001b[0;36mget_reward\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     43 \u001b[0;31m    \u001b[0mstate_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     44 \u001b[0;31m    \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 45 \u001b[0;31m        \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiscrim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     46 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     47 \u001b[0;31m\u001b[0;32mdef\u001b[0m \u001b[0msave_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> state\n",
      "tensor([0., 0.])\n",
      "ipdb> state_action\n",
      "tensor([0., 0.])\n",
      "ipdb> action\n",
      "tensor([])\n"
     ]
    }
   ],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "High reward signal for getting stuck might seem bad, but the discriminator is never going to provide a negative reward value, but instead finds a way to penalize this activity by the fact the the loss is an average over states. By this we know the networks's J (loss) is going to be much lower, and therefore less learning signal via gradient magnitude is sent back. This is a good thing and means more info is given backward for shorter episodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the discriminator has finished training, let's compare how it's reward function looks next to the actual one. \n",
    "\n",
    "How should I do this? \n",
    "\n",
    "Run trajectories below, print out IRL reward next to reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "env = gym.make('MountainCarContinuous-v0')\n",
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "        \n",
    "    mu, std = actor(torch.Tensor(state))\n",
    "    a = np.tanh(get_action(mu,std))\n",
    "    state, reward, done, _ = env.step(a)\n",
    "    print(reward,a)\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
