{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-implementing the GAIL algorithm on the expert data acquired to run mountain car"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import os\n",
    "import gym\n",
    "import pickle\n",
    "import argparse\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from tensorboardX import SummaryWriter \n",
    "from utils.utils import *\n",
    "from utils.zfilter import *\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x1031b3950>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs, hidden_size):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(num_inputs, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, num_outputs)\n",
    "        \n",
    "        self.fc3.weight.data.mul_(0.1)\n",
    "        self.fc3.bias.data.mul_(0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(self.fc1(x))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        actions = self.fc3(x)\n",
    "\n",
    "        return actions\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, num_inputs, hidden_size):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(num_inputs, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "        self.fc3.weight.data.mul_(0.1)\n",
    "        self.fc3.bias.data.mul_(0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(self.fc1(x))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        v = self.fc3(x)\n",
    "        return v\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, num_inputs, hidden_size):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.fc1 = nn.Linear(num_inputs, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "      #  self.fc3.weight.data.mul_(0.1)\n",
    "     #   self.fc3.bias.data.mul_(0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(self.fc1(x))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        prob = torch.sigmoid(self.fc3(x))\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37508, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I ran a DQN and after it beat the MountainCar env, I told it \n",
    "\n",
    "demonstrations = np.load('/Users/noahkasmanoff/Desktop/Projects/lets-do-irl/noah/gail/MCExpert.npy')\n",
    "                         \n",
    "demonstrations.shape                  \n",
    "                         \n",
    "                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code for training \n",
    "import torch\n",
    "import numpy as np\n",
    "from utils.utils import get_entropy, log_prob_density\n",
    "\n",
    "def train_discrim(discrim, memory, discrim_optim, demonstrations, discrim_update_num, batch_size, clip_param):\n",
    "    \"\"\"\n",
    "    Training the discriminator. \n",
    "\n",
    "    Why? \n",
    "    Goal of discrim is to do bce loss on learner and expert.\n",
    "\n",
    "    Should go through step by step, but the idea is to make learner look bad? That way it improves  \n",
    "\n",
    "    \"\"\"\n",
    "    memory = np.array(memory)  # s a r s' tuple\n",
    "    states = np.vstack(memory[:, 0]) \n",
    "    actions = list(memory[:, 1]) #actions taken by actor/policy\n",
    "    # TODO: Fix discrim \n",
    "    states = torch.Tensor(states)\n",
    "    actions = torch.Tensor(actions)\n",
    "        \n",
    "    criterion = torch.nn.BCELoss() # classify\n",
    " \n",
    "    for _ in range(discrim_update_num):\n",
    "        \n",
    "        learner = discrim(torch.cat([states, actions], dim=1)) #pass (s,a) through discriminator\n",
    "        demonstrations = torch.Tensor(demonstrations) # pass (s,a) of expert through discriminator\n",
    "        index = torch.randperm(demonstrations.shape[0])\n",
    "        demonstrations_batch = demonstrations[index,:][:batch_size]\n",
    "        expert = discrim(demonstrations_batch) #discrimator \"guesses\" whether or not these \n",
    "        # actions came from expert or learner\n",
    "\n",
    "        discrim_loss = criterion(learner, torch.ones((states.shape[0], 1))) + \\ # ones for learner\n",
    "                        criterion(expert, torch.zeros((demonstrations_batch.shape[0], 1))) # zeros for expert # TODO\n",
    "                # discrim loss: predict agent is all wrong, get as close to 0, and predict expert is 1, getting as close to 1 as possible. \n",
    "        discrim_optim.zero_grad() # gan loss, it tries to always get it right. \n",
    "        discrim_loss.backward()\n",
    "        discrim_optim.step()\n",
    "\n",
    "        # take these steps, do it however many times specified. \n",
    "\n",
    "    expert_acc = ((discrim(demonstrations_batch) < 0.5).float()).mean() #how often it realized the fake examples were fake\n",
    "    learner_acc = ((discrim(torch.cat([states, actions], dim=1)) > 0.5).float()).mean() #how often if predicted expert correctly. \n",
    "\n",
    "    return expert_acc, learner_acc # accuracy, it's the same kind, but because imbalanced better to look at separately. \n",
    " \n",
    "\n",
    "def train_actor_critic(actor, critic, memory, actor_optim, critic_optim, actor_critic_update_num, batch_size, clip_param):\n",
    "    \"\"\"\n",
    "    Using get gae, this is basically ppo . \n",
    "\n",
    "    It's somewhat straightforward, and trained with the irl reward which is \n",
    "    from that memory versus what would usually be the real rewa\n",
    "    \n",
    "    Transform this into discrete PPO more or less\n",
    "    \"\"\"\n",
    "    memory = np.array(memory) \n",
    "    # tuple of a regular old RL problem, but now reward is what the discriminator says. \n",
    "    states = np.vstack(memory[:, 0]) \n",
    "    actions = list(memory[:, 1]) \n",
    "    rewards = list(memory[:, 2])  #IRL Rewards? yes. \n",
    "    masks = list(memory[:, 3]) \n",
    "\n",
    "    # compute value of what happened, see if what we can get ius better. \n",
    "    old_values = critic(torch.Tensor(states))\n",
    "    #GAE aka estimate of Value + actual return roughtly \n",
    "    returns, advants = get_gae(rewards, masks, old_values, gamma, lamda) #TODO\n",
    "    \n",
    "    # pass states through actor, get corresponding actions\n",
    "    mu, std = actor(torch.Tensor(states)) #TODO\n",
    "    # new mus and stds? \n",
    "    old_policy = log_prob_density(torch.Tensor(actions), mu, std) # sum of log probability\n",
    "    # of old actions  \n",
    "\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    n = len(states)\n",
    "    arr = np.arange(n)\n",
    "\n",
    "    for _ in range(actor_critic_update_num):\n",
    "        np.random.shuffle(arr)\n",
    "\n",
    "        for i in range(n // batch_size): \n",
    "            batch_index = arr[batch_size * i : batch_size * (i + 1)]\n",
    "            #batch_index = torch.LongTensor(batch_index)\n",
    "            \n",
    "            inputs = torch.Tensor(states)[batch_index]\n",
    "            actions_samples = torch.Tensor(actions)[batch_index]\n",
    "            returns_samples = returns.unsqueeze(1)[batch_index]\n",
    "            advants_samples = advants.unsqueeze(1)[batch_index]\n",
    "            oldvalue_samples = old_values[batch_index].detach()\n",
    "        \n",
    "        \n",
    "            values = critic(inputs) #\n",
    "            clipped_values = oldvalue_samples + \\\n",
    "                             torch.clamp(values - oldvalue_samples,\n",
    "                                         -clip_param, \n",
    "                                         clip_param)\n",
    "            critic_loss1 = criterion(clipped_values, returns_samples)\n",
    "            critic_loss2 = criterion(values, returns_samples)\n",
    "            critic_loss = torch.max(critic_loss1, critic_loss2).mean()\n",
    "\n",
    "            loss, ratio, entropy = surrogate_loss(actor, advants_samples, inputs,\n",
    "                                         old_policy.detach(), actions_samples,\n",
    "                                         batch_index)\n",
    "            clipped_ratio = torch.clamp(ratio,\n",
    "                                        1.0 - clip_param,\n",
    "                                        1.0 + clip_param)\n",
    "            clipped_loss = clipped_ratio * advants_samples\n",
    "            actor_loss = -torch.min(loss, clipped_loss).mean()\n",
    "            #print(actor_loss,critic_loss,entropy)\n",
    "           # return actor_loss, critic_loss, entropy\n",
    "            loss = actor_loss + 0.5 * critic_loss - 0.001 * entropy\n",
    "           # asdf\n",
    "            #loss = loss.mean() #TODO\n",
    "            actor_optim.zero_grad()\n",
    "            loss.backward()\n",
    "            actor_optim.step()\n",
    "\n",
    "           # critic_optim.zero_grad()\n",
    "           # loss.backward() \n",
    "            critic_optim.step()\n",
    "\n",
    "           # loss.zero_grad()\n",
    "\n",
    "def get_gae(rewards, masks, values, gamma, lamda):\n",
    "    \"\"\"\n",
    "    How much better a particular action is in a particular state. \n",
    "    \n",
    "    Uses reward of current action + value function of that state-action pair, discount factor gamma, and then lamda to compute. \n",
    "    \"\"\"\n",
    "    rewards = torch.Tensor(rewards)\n",
    "    masks = torch.Tensor(masks)\n",
    "    returns = torch.zeros_like(rewards)\n",
    "    advants = torch.zeros_like(rewards)\n",
    "    \n",
    "    running_returns = 0\n",
    "    previous_value = 0\n",
    "    running_advants = 0\n",
    "\n",
    "    for t in reversed(range(0, len(rewards))):\n",
    "        running_returns = rewards[t] + (gamma * running_returns * masks[t])\n",
    "        returns[t] = running_returns\n",
    "\n",
    "        running_delta = rewards[t] + (gamma * previous_value * masks[t]) - \\\n",
    "                                        values.data[t]\n",
    "        previous_value = values.data[t]\n",
    "        \n",
    "        running_advants = running_delta + (gamma * lamda * \\\n",
    "                                            running_advants * masks[t])\n",
    "        advants[t] = running_advants\n",
    "\n",
    "    advants = (advants - advants.mean()) / advants.std()\n",
    "    return returns, advants\n",
    "\n",
    "def surrogate_loss(actor, advants, states, old_policy, actions, batch_index):\n",
    "    \"\"\"\n",
    "    The loss for PPO. Re-run through network, recomput policy from states\n",
    "    and see if this surrogate ratio is better. If it is, use as proximal policy update. It's very close to prior policy, but def better. \n",
    "    \n",
    "    Not sure this actually works though. Should not the new mu and stds be used to draw,\n",
    "    \n",
    "        When do we use get_action? Only once in main, I think it should be for all? \n",
    "    \"\"\"\n",
    "    mu, std = actor(states)\n",
    "    new_policy = log_prob_density(actions, mu, std) # TODO\n",
    "    old_policy = old_policy[batch_index]\n",
    "\n",
    "    ratio = torch.exp(new_policy - old_policy)\n",
    "    surrogate_loss = ratio * advants\n",
    "    entropy = get_entropy(mu, std)\n",
    "\n",
    "    return surrogate_loss, ratio, entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state size: 2\n",
      "action size: 1\n"
     ]
    }
   ],
   "source": [
    "# Normally args but not here :-)\n",
    "env_name = 'MountainCar-v0' # TODO\n",
    "load_model = None\n",
    "seed = 0\n",
    "render = False\n",
    "gamma = 0.99\n",
    "lamda = .98\n",
    "hidden_size = 128\n",
    "train_discrim_flag = True\n",
    "learning_rate = 3e-4\n",
    "clip_param = .2\n",
    "discrim_update_num = 2\n",
    "actor_critic_update_num = 10\n",
    "l2_rate = 1e-3 # weight decay\n",
    "total_sample_size = 256 # total num of state-actions to collect before learning\n",
    "batch_size = 32\n",
    "suspend_accu_exp = 1. # do not need to be this high typically, but seems likely it has to be for a simple env like mountain car cont.\n",
    "suspend_accu_gen = 1.\n",
    "max_iter_num = 500\n",
    "seed = 42\n",
    "logdir = 'logs'\n",
    "\n",
    "env = gym.make(env_name)\n",
    "    \n",
    "env.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "num_inputs = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.shape[0]\n",
    "running_state = ZFilter((num_inputs,), clip=5) # huh? \n",
    "# oh wow. ZFilter is exactly what I do in capstone project, removing \"badtimes\"\n",
    "\n",
    "print('state size:', num_inputs) \n",
    "print('action size:', num_actions)\n",
    "\n",
    "#load agent stuff \n",
    "actor = Actor(num_inputs, num_actions, hidden_size)\n",
    "critic = Critic(num_inputs, hidden_size)\n",
    "discrim = Discriminator(num_inputs + num_actions, hidden_size)\n",
    "\n",
    "actor_optim = optim.Adam(actor.parameters(), lr=learning_rate)\n",
    "critic_optim = optim.Adam(critic.parameters(), lr=learning_rate, \n",
    "                          weight_decay=l2_rate) \n",
    "discrim_optim = optim.Adam(discrim.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "demonstrations.shape (37508, 3)\n",
      "steps in ep: 999\n",
      "0:: 1 episode score is 640.65\n",
      "Expert: 53.12% | Learner: 64.46%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/noahkasmanoff/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:16: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  app.launch_new_instance()\n",
      "/Users/noahkasmanoff/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:56: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps in ep: 999\n",
      "1:: 2 episode score is 645.50\n",
      "Expert: 60.94% | Learner: 63.06%\n",
      "steps in ep: 999\n",
      "2:: 3 episode score is 649.56\n",
      "Expert: 64.06% | Learner: 61.76%\n",
      "steps in ep: 999\n",
      "3:: 4 episode score is 661.91\n",
      "Expert: 87.50% | Learner: 58.96%\n",
      "steps in ep: 999\n",
      "4:: 5 episode score is 670.44\n",
      "Expert: 79.69% | Learner: 55.86%\n",
      "steps in ep: 505\n",
      "5:: 6 episode score is 344.94\n",
      "Expert: 82.81% | Learner: 58.81%\n",
      "steps in ep: 923\n",
      "6:: 7 episode score is 628.07\n",
      "Expert: 90.62% | Learner: 57.20%\n",
      "steps in ep: 999\n",
      "7:: 8 episode score is 720.10\n",
      "Expert: 81.25% | Learner: 51.95%\n",
      "steps in ep: 999\n",
      "8:: 9 episode score is 704.85\n",
      "Expert: 82.81% | Learner: 55.76%\n",
      "steps in ep: 999\n",
      "9:: 10 episode score is 690.72\n",
      "Expert: 87.50% | Learner: 58.16%\n",
      "steps in ep: 999\n",
      "10:: 11 episode score is 713.99\n",
      "Expert: 82.81% | Learner: 55.86%\n",
      "steps in ep: 999\n",
      "11:: 12 episode score is 725.76\n",
      "Expert: 78.12% | Learner: 54.95%\n",
      "steps in ep: 999\n",
      "12:: 13 episode score is 746.29\n",
      "Expert: 92.19% | Learner: 54.05%\n",
      "steps in ep: 999\n",
      "13:: 14 episode score is 725.62\n",
      "Expert: 82.81% | Learner: 55.26%\n",
      "steps in ep: 999\n",
      "14:: 15 episode score is 748.81\n",
      "Expert: 90.62% | Learner: 55.06%\n",
      "steps in ep: 999\n",
      "15:: 16 episode score is 765.72\n",
      "Expert: 81.25% | Learner: 52.45%\n",
      "steps in ep: 999\n",
      "16:: 17 episode score is 745.86\n",
      "Expert: 78.12% | Learner: 54.65%\n",
      "steps in ep: 999\n",
      "17:: 18 episode score is 712.74\n",
      "Expert: 84.38% | Learner: 58.06%\n",
      "steps in ep: 999\n",
      "18:: 19 episode score is 679.10\n",
      "Expert: 79.69% | Learner: 62.16%\n",
      "steps in ep: 999\n",
      "19:: 20 episode score is 636.76\n",
      "Expert: 84.38% | Learner: 67.67%\n",
      "steps in ep: 999\n",
      "20:: 21 episode score is 605.98\n",
      "Expert: 82.81% | Learner: 67.87%\n",
      "steps in ep: 999\n",
      "21:: 22 episode score is 575.14\n",
      "Expert: 81.25% | Learner: 72.77%\n",
      "steps in ep: 999\n",
      "22:: 23 episode score is 526.80\n",
      "Expert: 79.69% | Learner: 78.78%\n",
      "steps in ep: 999\n",
      "23:: 24 episode score is 566.99\n",
      "Expert: 76.56% | Learner: 69.87%\n",
      "steps in ep: 999\n",
      "24:: 25 episode score is 575.30\n",
      "Expert: 79.69% | Learner: 73.57%\n",
      "steps in ep: 999\n",
      "25:: 26 episode score is 548.69\n",
      "Expert: 81.25% | Learner: 69.17%\n",
      "steps in ep: 999\n",
      "26:: 27 episode score is 557.70\n",
      "Expert: 76.56% | Learner: 70.97%\n",
      "steps in ep: 999\n",
      "27:: 28 episode score is 685.85\n",
      "Expert: 82.81% | Learner: 66.77%\n",
      "steps in ep: 999\n",
      "28:: 29 episode score is 730.20\n",
      "Expert: 85.94% | Learner: 61.46%\n",
      "steps in ep: 999\n",
      "29:: 30 episode score is 782.08\n",
      "Expert: 79.69% | Learner: 58.86%\n",
      "steps in ep: 999\n",
      "30:: 31 episode score is 734.85\n",
      "Expert: 75.00% | Learner: 59.86%\n",
      "steps in ep: 999\n",
      "31:: 32 episode score is 706.88\n",
      "Expert: 81.25% | Learner: 60.26%\n",
      "steps in ep: 999\n",
      "32:: 33 episode score is 672.46\n",
      "Expert: 82.81% | Learner: 60.06%\n",
      "steps in ep: 999\n",
      "33:: 34 episode score is 699.83\n",
      "Expert: 78.12% | Learner: 61.46%\n",
      "steps in ep: 999\n",
      "34:: 35 episode score is 806.11\n",
      "Expert: 81.25% | Learner: 56.16%\n",
      "steps in ep: 999\n",
      "35:: 36 episode score is 825.75\n",
      "Expert: 85.94% | Learner: 56.76%\n",
      "steps in ep: 999\n",
      "36:: 37 episode score is 724.33\n",
      "Expert: 85.94% | Learner: 58.86%\n",
      "steps in ep: 999\n",
      "37:: 38 episode score is 719.87\n",
      "Expert: 79.69% | Learner: 59.86%\n",
      "steps in ep: 999\n",
      "38:: 39 episode score is 718.21\n",
      "Expert: 82.81% | Learner: 58.76%\n",
      "steps in ep: 999\n",
      "39:: 40 episode score is 731.15\n",
      "Expert: 78.12% | Learner: 56.56%\n",
      "steps in ep: 999\n",
      "40:: 41 episode score is 680.06\n",
      "Expert: 76.56% | Learner: 62.56%\n",
      "steps in ep: 999\n",
      "41:: 42 episode score is 667.78\n",
      "Expert: 73.44% | Learner: 63.66%\n",
      "steps in ep: 999\n",
      "42:: 43 episode score is 637.70\n",
      "Expert: 73.44% | Learner: 66.37%\n",
      "steps in ep: 999\n",
      "43:: 44 episode score is 616.58\n",
      "Expert: 68.75% | Learner: 67.47%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-5acf05f8d0da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;31m# PPO operation,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m     \u001b[0mtrain_actor_critic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcritic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactor_optim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcritic_optim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactor_critic_update_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip_param\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# no output, see comments in train_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0miter\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-d0dbd925f58d>\u001b[0m in \u001b[0;36mtrain_actor_critic\u001b[0;34m(actor, critic, memory, actor_optim, critic_optim, actor_critic_update_num, batch_size, clip_param)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;31m# compute value of what happened, see if what we can get ius better.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0mold_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcritic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;31m#GAE aka estimate of Value + actual return roughtly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0mreturns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madvants\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_gae\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlamda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-87edf6ebbbbb>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1672\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1673\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1674\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1675\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1676\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/traceback.py\u001b[0m in \u001b[0;36mformat_stack\u001b[0;34m(f, limit)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_back\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mformat_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextract_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/traceback.py\u001b[0m in \u001b[0;36mextract_stack\u001b[0;34m(f, limit)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_back\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m     \u001b[0mstack\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStackSummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwalk_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m     \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreverse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/traceback.py\u001b[0m in \u001b[0;36mextract\u001b[0;34m(klass, frame_gen, limit, lookup_lines, capture_locals)\u001b[0m\n\u001b[1;32m    357\u001b[0m                 filename, lineno, name, lookup_line=False, locals=f_locals))\n\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfnames\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m             \u001b[0mlinecache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckcache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m         \u001b[0;31m# If immediate lookup was desired, trigger lookups now.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlookup_lines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/IPython/core/compilerop.py\u001b[0m in \u001b[0;36mcheck_linecache_ipython\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    155\u001b[0m     \"\"\"\n\u001b[1;32m    156\u001b[0m     \u001b[0;31m# First call the original checkcache as intended\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m     \u001b[0mlinecache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_checkcache_ori\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m     \u001b[0;31m# Then, update back the cache with our data, so that tracebacks related\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;31m# to our compiled codes can be produced.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/linecache.py\u001b[0m in \u001b[0;36mcheckcache\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mcontinue\u001b[0m   \u001b[0;31m# no-op for files loaded via a __loader__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mstat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfullname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# load demonstrations\n",
    "#expert_demo, _ = pickle.load(open('./expert_demo/expert_demo.p', \"rb\"))\n",
    "#demonstrations = np.load('/Users/noahkasmanoff/Desktop/Projects/lets-do-irl/mountaincar/app/expert_demo/expert_demo.npy')\n",
    "print(\"demonstrations.shape\", demonstrations.shape)\n",
    "\n",
    "writer = SummaryWriter(logdir)\n",
    "\n",
    "#if you aren't starting from scratch, load in this \n",
    "if load_model is not None:\n",
    "    saved_ckpt_path = os.path.join(os.getcwd(), 'save_model', str(load_model))\n",
    "    ckpt = torch.load(saved_ckpt_path)\n",
    "\n",
    "    # initialize everything\n",
    "    actor.load_state_dict(ckpt['actor'])\n",
    "    critic.load_state_dict(ckpt['critic'])\n",
    "    discrim.load_state_dict(ckpt['discrim'])\n",
    "\n",
    "    running_state.rs.n = ckpt['z_filter_n']\n",
    "    running_state.rs.mean = ckpt['z_filter_m']\n",
    "    running_state.rs.sum_square = ckpt['z_filter_s']\n",
    "\n",
    "    print(\"Loaded OK ex. Zfilter N {}\".format(running_state.rs.n))\n",
    "\n",
    "# if no old model no worries, start training. \n",
    "episodes = 0\n",
    "for iter in range(max_iter_num):\n",
    "    # for i total trajectories \n",
    "    actor.eval(), critic.eval()\n",
    "    memory = deque()\n",
    "\n",
    "    steps = 0\n",
    "    scores = []\n",
    "\n",
    "    while steps < total_sample_size: \n",
    "        # sample trajectories  (batch size)\n",
    "        state = env.reset()\n",
    "        score = 0\n",
    "\n",
    "        state = running_state(state) #uh.. again ZFilter related, cleans the state \n",
    "        epsteps = 0\n",
    "        done = False\n",
    "        while not done: \n",
    "            epsteps += 1\n",
    "            #run through environment\n",
    "            if render: \n",
    "                env.render()\n",
    "\n",
    "            steps += 1\n",
    "\n",
    "             = actor(torch.Tensor(state).unsqueeze(0)) #pass state through actor network $ TODO\n",
    "            action = get_action(mu, std)[0] #compute random action\n",
    "            next_state, reward, done, _ = env.step(action) #take a step\n",
    "            irl_reward = get_reward(discrim, state, action) #infer what the reward of this action is based on discriminator's get reward \n",
    "\n",
    "            if done:\n",
    "                mask = 0\n",
    "            else:\n",
    "                mask = 1 #if done, save this, \n",
    "\n",
    "            memory.append([state, action, irl_reward, mask])\n",
    "\n",
    "            next_state = running_state(next_state) #save cleaned next state\n",
    "            state = next_state #and set to current state, \n",
    "\n",
    "            score += irl_reward #add total reward\n",
    "           # print(\"IRL Reward=\",irl_reward)\n",
    "            if done:\n",
    "                break\n",
    "            #actual sampling done here \n",
    "\n",
    "            \n",
    "\n",
    "        \n",
    "        episodes += 1\n",
    "        scores.append(score)\n",
    "        print(\"steps in ep:\", epsteps)\n",
    "    score_avg = np.mean(scores) #how this model did, \n",
    "    print('{}:: {} episode score is {:.2f}'.format(iter, episodes, score_avg))\n",
    "    writer.add_scalar('log/score', float(score_avg), iter) #logg\n",
    "\n",
    "    actor.train(), critic.train(), discrim.train() #now train \n",
    "    if train_discrim_flag: #if this batch optimizes discrim/reward, \n",
    "        # for training the discriminator, classify where state-action pair came from. \n",
    "        expert_acc, learner_acc = train_discrim(discrim, memory, discrim_optim, demonstrations, discrim_update_num, batch_size, clip_param) # see comments in train_model. \n",
    "        print(\"Expert: %.2f%% | Learner: %.2f%%\" % (expert_acc * 100, learner_acc * 100))\n",
    "        if expert_acc > suspend_accu_exp and learner_acc > suspend_accu_gen:\n",
    "            print(\"Now it will only train the policy, seeing as it is good enough at finding the differences between learner and expert trajectories.\")\n",
    "            train_discrim_flag = False #now train only policy, vanilla RL\n",
    "            render = True\n",
    "    #for training actor critic \n",
    "    \n",
    "    # PPO operation, \n",
    "    train_actor_critic(actor, critic, memory, actor_optim, critic_optim, actor_critic_update_num, batch_size, clip_param) # no output, see comments in train_model \n",
    "\n",
    "    if iter % 100:\n",
    "        score_avg = int(score_avg)\n",
    "\n",
    "        model_path = os.path.join(os.getcwd(),'save_model')\n",
    "        if not os.path.isdir(model_path):\n",
    "            os.makedirs(model_path)\n",
    "\n",
    "        ckpt_path = os.path.join(model_path, 'ckpt_'+ str(score_avg)+'.pth.tar')\n",
    "\n",
    "        save_checkpoint({\n",
    "            'actor': actor.state_dict(),\n",
    "            'critic': critic.state_dict(),\n",
    "            'discrim': discrim.state_dict(),\n",
    "            'z_filter_n':running_state.rs.n,\n",
    "            'z_filter_m': running_state.rs.mean,\n",
    "            'z_filter_s': running_state.rs.sum_square,\n",
    "           # 'args': args,\n",
    "            'score': score_avg\n",
    "        }, filename=ckpt_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "High reward signal for getting stuck might seem bad, but the discriminator is never going to provide a negative reward value, but instead finds a way to penalize this activity by the fact the the loss is an average over states. By this we know the networks's J (loss) is going to be much lower, and therefore less learning signal via gradient magnitude is sent back. This is a good thing and means more info is given backward for shorter episodes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the discriminator has finished training, let's compare how it's reward function looks next to the actual one. \n",
    "\n",
    "How should I do this? \n",
    "\n",
    "Run trajectories below, print out IRL reward next to reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "env = gym.make('MountainCarContinuous-v0')\n",
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "        \n",
    "    mu, std = actor(torch.Tensor(state))\n",
    "    a = np.tanh(get_action(mu,std))\n",
    "    state, reward, done, _ = env.step(a)\n",
    "    print(reward,a)\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
